[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2026-01-07 16:30:21] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 16:30:21] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2026-01-07 16:30:21] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2026-01-07 16:30:21] logging.py:143 >> Loading dataset aaaaaaaaaaaaaaaaaaaaa-t.json...
[INFO|2026-01-07 16:30:48] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 16:30:48] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 16:30:48] logging.py:143 >> KV cache is disabled during training.
[WARNING|2026-01-07 16:30:48] logging.py:328 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|2026-01-07 16:30:48] modeling_utils.py:1169 >> loading weights file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\model.safetensors
[INFO|2026-01-07 16:30:48] modeling_utils.py:1243 >> Will use dtype=torch.bfloat16 as defined in model's config object
[INFO|2026-01-07 16:30:48] modeling_utils.py:2341 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|2026-01-07 16:30:48] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|2026-01-07 16:30:49] configuration_utils.py:939 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\generation_config.json
[INFO|2026-01-07 16:30:49] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "top_p": 0.8
}

[INFO|2026-01-07 16:30:49] dynamic_module_utils.py:423 >> Could not locate the custom_generate/generate.py inside C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat.
[INFO|2026-01-07 16:30:49] logging.py:143 >> Gradient checkpointing enabled.
[INFO|2026-01-07 16:30:49] logging.py:143 >> Using torch SDPA for faster training and inference.
[INFO|2026-01-07 16:30:49] logging.py:143 >> Upcasting trainable params to float32.
[INFO|2026-01-07 16:30:49] logging.py:143 >> Fine-tuning method: LoRA
[INFO|2026-01-07 16:30:49] logging.py:143 >> Found linear modules: down_proj,v_proj,gate_proj,up_proj,k_proj,o_proj,q_proj
[INFO|2026-01-07 16:30:49] logging.py:143 >> trainable params: 3,784,704 || all params: 467,772,416 || trainable%: 0.8091
[WARNING|2026-01-07 16:30:49] trainer.py:906 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|2026-01-07 16:30:49] trainer.py:749 >> Using auto half precision backend
[WARNING|2026-01-07 16:30:49] trainer.py:982 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|2026-01-07 16:30:49] trainer.py:2519 >> ***** Running training *****
[INFO|2026-01-07 16:30:49] trainer.py:2520 >>   Num examples = 20,699
[INFO|2026-01-07 16:30:49] trainer.py:2521 >>   Num Epochs = 3
[INFO|2026-01-07 16:30:49] trainer.py:2522 >>   Instantaneous batch size per device = 2
[INFO|2026-01-07 16:30:49] trainer.py:2525 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|2026-01-07 16:30:49] trainer.py:2526 >>   Gradient Accumulation steps = 8
[INFO|2026-01-07 16:30:49] trainer.py:2527 >>   Total optimization steps = 3,882
[INFO|2026-01-07 16:30:49] trainer.py:2528 >>   Number of trainable parameters = 3,784,704
[INFO|2026-01-07 16:31:06] logging.py:143 >> {'loss': 2.6074, 'learning_rate': 5.0000e-05, 'epoch': 0.00, 'throughput': 1522.70}
[INFO|2026-01-07 16:31:23] logging.py:143 >> {'loss': 2.4270, 'learning_rate': 4.9999e-05, 'epoch': 0.01, 'throughput': 1531.43}
[INFO|2026-01-07 16:31:41] logging.py:143 >> {'loss': 2.2248, 'learning_rate': 4.9998e-05, 'epoch': 0.01, 'throughput': 1555.69}
[INFO|2026-01-07 16:31:57] logging.py:143 >> {'loss': 2.4657, 'learning_rate': 4.9997e-05, 'epoch': 0.02, 'throughput': 1545.78}
[INFO|2026-01-07 16:32:12] logging.py:143 >> {'loss': 2.3999, 'learning_rate': 4.9995e-05, 'epoch': 0.02, 'throughput': 1515.30}
[INFO|2026-01-07 16:32:29] logging.py:143 >> {'loss': 2.5051, 'learning_rate': 4.9993e-05, 'epoch': 0.02, 'throughput': 1513.68}
[INFO|2026-01-07 16:32:46] logging.py:143 >> {'loss': 2.0395, 'learning_rate': 4.9991e-05, 'epoch': 0.03, 'throughput': 1513.23}
[INFO|2026-01-07 16:33:02] logging.py:143 >> {'loss': 2.1609, 'learning_rate': 4.9988e-05, 'epoch': 0.03, 'throughput': 1515.54}
[INFO|2026-01-07 16:33:21] logging.py:143 >> {'loss': 2.0806, 'learning_rate': 4.9984e-05, 'epoch': 0.03, 'throughput': 1518.94}
[INFO|2026-01-07 16:33:39] logging.py:143 >> {'loss': 1.7158, 'learning_rate': 4.9980e-05, 'epoch': 0.04, 'throughput': 1525.11}
[INFO|2026-01-07 16:33:58] logging.py:143 >> {'loss': 2.1779, 'learning_rate': 4.9976e-05, 'epoch': 0.04, 'throughput': 1524.57}
[INFO|2026-01-07 16:34:13] logging.py:143 >> {'loss': 2.1040, 'learning_rate': 4.9972e-05, 'epoch': 0.05, 'throughput': 1515.57}
[INFO|2026-01-07 16:34:32] logging.py:143 >> {'loss': 1.9757, 'learning_rate': 4.9966e-05, 'epoch': 0.05, 'throughput': 1514.49}
[INFO|2026-01-07 16:34:48] logging.py:143 >> {'loss': 1.9460, 'learning_rate': 4.9961e-05, 'epoch': 0.05, 'throughput': 1511.03}
[INFO|2026-01-07 16:35:07] logging.py:143 >> {'loss': 1.5455, 'learning_rate': 4.9955e-05, 'epoch': 0.06, 'throughput': 1510.30}
[INFO|2026-01-07 16:35:26] logging.py:143 >> {'loss': 1.7514, 'learning_rate': 4.9949e-05, 'epoch': 0.06, 'throughput': 1514.11}
[INFO|2026-01-07 16:35:44] logging.py:143 >> {'loss': 1.9383, 'learning_rate': 4.9942e-05, 'epoch': 0.07, 'throughput': 1511.45}
[INFO|2026-01-07 16:36:01] logging.py:143 >> {'loss': 1.8850, 'learning_rate': 4.9935e-05, 'epoch': 0.07, 'throughput': 1508.84}
[INFO|2026-01-07 16:36:16] logging.py:143 >> {'loss': 2.2096, 'learning_rate': 4.9928e-05, 'epoch': 0.07, 'throughput': 1501.80}
[INFO|2026-01-07 16:36:35] logging.py:143 >> {'loss': 2.1001, 'learning_rate': 4.9920e-05, 'epoch': 0.08, 'throughput': 1503.88}
[INFO|2026-01-07 16:36:35] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-100
[INFO|2026-01-07 16:36:35] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 16:36:35] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 16:36:35] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-100\chat_template.jinja
[INFO|2026-01-07 16:36:35] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-100\tokenizer_config.json
[INFO|2026-01-07 16:36:35] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-100\special_tokens_map.json
[INFO|2026-01-07 16:36:51] logging.py:143 >> {'loss': 1.6895, 'learning_rate': 4.9912e-05, 'epoch': 0.08, 'throughput': 1496.78}
[INFO|2026-01-07 16:37:09] logging.py:143 >> {'loss': 1.7858, 'learning_rate': 4.9903e-05, 'epoch': 0.09, 'throughput': 1495.02}
[INFO|2026-01-07 16:37:26] logging.py:143 >> {'loss': 1.8247, 'learning_rate': 4.9894e-05, 'epoch': 0.09, 'throughput': 1493.48}
[INFO|2026-01-07 16:37:43] logging.py:143 >> {'loss': 2.1066, 'learning_rate': 4.9884e-05, 'epoch': 0.09, 'throughput': 1490.99}
[INFO|2026-01-07 16:38:03] logging.py:143 >> {'loss': 1.9646, 'learning_rate': 4.9874e-05, 'epoch': 0.10, 'throughput': 1492.88}
[INFO|2026-01-07 16:38:20] logging.py:143 >> {'loss': 1.8094, 'learning_rate': 4.9864e-05, 'epoch': 0.10, 'throughput': 1489.98}
[INFO|2026-01-07 16:38:35] logging.py:143 >> {'loss': 1.9958, 'learning_rate': 4.9853e-05, 'epoch': 0.10, 'throughput': 1483.97}
[INFO|2026-01-07 16:38:52] logging.py:143 >> {'loss': 1.7228, 'learning_rate': 4.9842e-05, 'epoch': 0.11, 'throughput': 1483.60}
[INFO|2026-01-07 16:39:07] logging.py:143 >> {'loss': 1.9234, 'learning_rate': 4.9830e-05, 'epoch': 0.11, 'throughput': 1480.15}
[INFO|2026-01-07 16:39:28] logging.py:143 >> {'loss': 1.6267, 'learning_rate': 4.9818e-05, 'epoch': 0.12, 'throughput': 1482.04}
[INFO|2026-01-07 16:39:44] logging.py:143 >> {'loss': 1.7910, 'learning_rate': 4.9806e-05, 'epoch': 0.12, 'throughput': 1479.91}
[INFO|2026-01-07 16:40:02] logging.py:143 >> {'loss': 1.7532, 'learning_rate': 4.9793e-05, 'epoch': 0.12, 'throughput': 1479.05}
[INFO|2026-01-07 16:40:17] logging.py:143 >> {'loss': 1.9917, 'learning_rate': 4.9780e-05, 'epoch': 0.13, 'throughput': 1476.77}
[INFO|2026-01-07 16:40:36] logging.py:143 >> {'loss': 1.4709, 'learning_rate': 4.9767e-05, 'epoch': 0.13, 'throughput': 1478.42}
[INFO|2026-01-07 16:40:53] logging.py:143 >> {'loss': 1.6237, 'learning_rate': 4.9753e-05, 'epoch': 0.14, 'throughput': 1477.56}
[INFO|2026-01-07 16:41:09] logging.py:143 >> {'loss': 1.5752, 'learning_rate': 4.9738e-05, 'epoch': 0.14, 'throughput': 1476.39}
[INFO|2026-01-07 16:41:27] logging.py:143 >> {'loss': 1.6035, 'learning_rate': 4.9723e-05, 'epoch': 0.14, 'throughput': 1476.08}
[INFO|2026-01-07 16:41:44] logging.py:143 >> {'loss': 1.7119, 'learning_rate': 4.9708e-05, 'epoch': 0.15, 'throughput': 1475.59}
[INFO|2026-01-07 16:42:00] logging.py:143 >> {'loss': 1.8642, 'learning_rate': 4.9693e-05, 'epoch': 0.15, 'throughput': 1473.39}
[INFO|2026-01-07 16:42:18] logging.py:143 >> {'loss': 1.4742, 'learning_rate': 4.9677e-05, 'epoch': 0.15, 'throughput': 1473.38}
[INFO|2026-01-07 16:42:18] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-200
[INFO|2026-01-07 16:42:18] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 16:42:18] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 16:42:18] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-200\chat_template.jinja
[INFO|2026-01-07 16:42:18] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-200\tokenizer_config.json
[INFO|2026-01-07 16:42:18] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-200\special_tokens_map.json
[INFO|2026-01-07 16:42:35] logging.py:143 >> {'loss': 2.0491, 'learning_rate': 4.9660e-05, 'epoch': 0.16, 'throughput': 1471.57}
[INFO|2026-01-07 16:42:52] logging.py:143 >> {'loss': 1.9960, 'learning_rate': 4.9643e-05, 'epoch': 0.16, 'throughput': 1471.02}
[INFO|2026-01-07 16:43:11] logging.py:143 >> {'loss': 1.6837, 'learning_rate': 4.9626e-05, 'epoch': 0.17, 'throughput': 1472.02}
[INFO|2026-01-07 16:43:30] logging.py:143 >> {'loss': 1.5468, 'learning_rate': 4.9608e-05, 'epoch': 0.17, 'throughput': 1471.91}
[INFO|2026-01-07 16:43:47] logging.py:143 >> {'loss': 1.9058, 'learning_rate': 4.9590e-05, 'epoch': 0.17, 'throughput': 1471.48}
[INFO|2026-01-07 16:44:05] logging.py:143 >> {'loss': 1.8959, 'learning_rate': 4.9572e-05, 'epoch': 0.18, 'throughput': 1471.34}
[INFO|2026-01-07 16:44:23] logging.py:143 >> {'loss': 1.6354, 'learning_rate': 4.9553e-05, 'epoch': 0.18, 'throughput': 1470.75}
[INFO|2026-01-07 16:44:39] logging.py:143 >> {'loss': 1.7558, 'learning_rate': 4.9534e-05, 'epoch': 0.19, 'throughput': 1469.73}
[INFO|2026-01-07 16:44:58] logging.py:143 >> {'loss': 1.7939, 'learning_rate': 4.9514e-05, 'epoch': 0.19, 'throughput': 1469.26}
[INFO|2026-01-07 16:45:15] logging.py:143 >> {'loss': 1.8311, 'learning_rate': 4.9494e-05, 'epoch': 0.19, 'throughput': 1468.89}
[INFO|2026-01-07 16:45:33] logging.py:143 >> {'loss': 1.5927, 'learning_rate': 4.9474e-05, 'epoch': 0.20, 'throughput': 1469.35}
[INFO|2026-01-07 16:45:51] logging.py:143 >> {'loss': 1.5323, 'learning_rate': 4.9453e-05, 'epoch': 0.20, 'throughput': 1468.85}
[INFO|2026-01-07 16:46:08] logging.py:143 >> {'loss': 1.8399, 'learning_rate': 4.9432e-05, 'epoch': 0.20, 'throughput': 1468.60}
[INFO|2026-01-07 16:46:26] logging.py:143 >> {'loss': 1.6921, 'learning_rate': 4.9410e-05, 'epoch': 0.21, 'throughput': 1468.35}
[INFO|2026-01-07 16:46:44] logging.py:143 >> {'loss': 1.7681, 'learning_rate': 4.9388e-05, 'epoch': 0.21, 'throughput': 1467.86}
[INFO|2026-01-07 16:47:03] logging.py:143 >> {'loss': 1.4287, 'learning_rate': 4.9365e-05, 'epoch': 0.22, 'throughput': 1468.53}
[INFO|2026-01-07 16:47:23] logging.py:143 >> {'loss': 1.6306, 'learning_rate': 4.9343e-05, 'epoch': 0.22, 'throughput': 1468.96}
[INFO|2026-01-07 16:47:38] logging.py:143 >> {'loss': 2.0344, 'learning_rate': 4.9319e-05, 'epoch': 0.22, 'throughput': 1466.48}
[INFO|2026-01-07 16:47:57] logging.py:143 >> {'loss': 1.7694, 'learning_rate': 4.9296e-05, 'epoch': 0.23, 'throughput': 1467.39}
[INFO|2026-01-07 16:48:13] logging.py:143 >> {'loss': 1.6465, 'learning_rate': 4.9272e-05, 'epoch': 0.23, 'throughput': 1465.93}
[INFO|2026-01-07 16:48:13] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-300
[INFO|2026-01-07 16:48:13] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 16:48:13] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 16:48:13] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-300\chat_template.jinja
[INFO|2026-01-07 16:48:13] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-300\tokenizer_config.json
[INFO|2026-01-07 16:48:13] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-300\special_tokens_map.json
[INFO|2026-01-07 16:48:31] logging.py:143 >> {'loss': 2.1047, 'learning_rate': 4.9247e-05, 'epoch': 0.24, 'throughput': 1465.42}
[INFO|2026-01-07 16:48:47] logging.py:143 >> {'loss': 1.7219, 'learning_rate': 4.9222e-05, 'epoch': 0.24, 'throughput': 1464.11}
[INFO|2026-01-07 16:49:04] logging.py:143 >> {'loss': 1.7493, 'learning_rate': 4.9197e-05, 'epoch': 0.24, 'throughput': 1463.54}
[INFO|2026-01-07 16:49:21] logging.py:143 >> {'loss': 1.4871, 'learning_rate': 4.9172e-05, 'epoch': 0.25, 'throughput': 1463.78}
[INFO|2026-01-07 16:49:38] logging.py:143 >> {'loss': 1.6790, 'learning_rate': 4.9146e-05, 'epoch': 0.25, 'throughput': 1463.00}
[INFO|2026-01-07 16:49:56] logging.py:143 >> {'loss': 1.6195, 'learning_rate': 4.9119e-05, 'epoch': 0.26, 'throughput': 1462.76}
[INFO|2026-01-07 16:50:11] logging.py:143 >> {'loss': 1.5078, 'learning_rate': 4.9092e-05, 'epoch': 0.26, 'throughput': 1461.85}
[INFO|2026-01-07 16:50:27] logging.py:143 >> {'loss': 1.7342, 'learning_rate': 4.9065e-05, 'epoch': 0.26, 'throughput': 1461.01}
[INFO|2026-01-07 16:50:44] logging.py:143 >> {'loss': 2.0593, 'learning_rate': 4.9037e-05, 'epoch': 0.27, 'throughput': 1460.21}
[INFO|2026-01-07 16:51:01] logging.py:143 >> {'loss': 1.4419, 'learning_rate': 4.9009e-05, 'epoch': 0.27, 'throughput': 1459.92}
[INFO|2026-01-07 16:51:20] logging.py:143 >> {'loss': 1.6392, 'learning_rate': 4.8981e-05, 'epoch': 0.27, 'throughput': 1460.83}
[INFO|2026-01-07 16:51:37] logging.py:143 >> {'loss': 1.6408, 'learning_rate': 4.8952e-05, 'epoch': 0.28, 'throughput': 1460.29}
[INFO|2026-01-07 16:51:57] logging.py:143 >> {'loss': 1.5985, 'learning_rate': 4.8923e-05, 'epoch': 0.28, 'throughput': 1461.75}
[INFO|2026-01-07 16:52:14] logging.py:143 >> {'loss': 1.7182, 'learning_rate': 4.8894e-05, 'epoch': 0.29, 'throughput': 1461.35}
[INFO|2026-01-07 16:52:32] logging.py:143 >> {'loss': 1.7712, 'learning_rate': 4.8864e-05, 'epoch': 0.29, 'throughput': 1461.39}
[INFO|2026-01-07 16:52:50] logging.py:143 >> {'loss': 1.4844, 'learning_rate': 4.8833e-05, 'epoch': 0.29, 'throughput': 1460.86}
[INFO|2026-01-07 16:53:08] logging.py:143 >> {'loss': 1.8397, 'learning_rate': 4.8803e-05, 'epoch': 0.30, 'throughput': 1460.41}
[INFO|2026-01-07 16:53:26] logging.py:143 >> {'loss': 1.9067, 'learning_rate': 4.8771e-05, 'epoch': 0.30, 'throughput': 1460.17}
[INFO|2026-01-07 16:53:44] logging.py:143 >> {'loss': 1.5078, 'learning_rate': 4.8740e-05, 'epoch': 0.31, 'throughput': 1460.34}
[INFO|2026-01-07 16:53:59] logging.py:143 >> {'loss': 1.8769, 'learning_rate': 4.8708e-05, 'epoch': 0.31, 'throughput': 1459.21}
[INFO|2026-01-07 16:53:59] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-400
[INFO|2026-01-07 16:53:59] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 16:53:59] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 16:54:00] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-400\chat_template.jinja
[INFO|2026-01-07 16:54:00] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-400\tokenizer_config.json
[INFO|2026-01-07 16:54:00] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-400\special_tokens_map.json
[INFO|2026-01-07 16:54:18] logging.py:143 >> {'loss': 1.3969, 'learning_rate': 4.8676e-05, 'epoch': 0.31, 'throughput': 1459.53}
[INFO|2026-01-07 16:54:35] logging.py:143 >> {'loss': 1.5483, 'learning_rate': 4.8643e-05, 'epoch': 0.32, 'throughput': 1459.17}
[INFO|2026-01-07 16:54:54] logging.py:143 >> {'loss': 1.5186, 'learning_rate': 4.8610e-05, 'epoch': 0.32, 'throughput': 1459.56}
[INFO|2026-01-07 16:55:12] logging.py:143 >> {'loss': 1.4508, 'learning_rate': 4.8576e-05, 'epoch': 0.32, 'throughput': 1460.00}
[INFO|2026-01-07 16:55:29] logging.py:143 >> {'loss': 1.5427, 'learning_rate': 4.8543e-05, 'epoch': 0.33, 'throughput': 1459.70}
[INFO|2026-01-07 16:55:51] logging.py:143 >> {'loss': 1.3394, 'learning_rate': 4.8508e-05, 'epoch': 0.33, 'throughput': 1461.16}
[INFO|2026-01-07 16:56:09] logging.py:143 >> {'loss': 1.1452, 'learning_rate': 4.8474e-05, 'epoch': 0.34, 'throughput': 1461.24}
[INFO|2026-01-07 16:56:26] logging.py:143 >> {'loss': 1.8626, 'learning_rate': 4.8439e-05, 'epoch': 0.34, 'throughput': 1461.13}
[INFO|2026-01-07 16:56:43] logging.py:143 >> {'loss': 1.7978, 'learning_rate': 4.8403e-05, 'epoch': 0.34, 'throughput': 1460.58}
[INFO|2026-01-07 16:57:03] logging.py:143 >> {'loss': 1.3782, 'learning_rate': 4.8368e-05, 'epoch': 0.35, 'throughput': 1461.61}
[INFO|2026-01-07 16:57:22] logging.py:143 >> {'loss': 1.7073, 'learning_rate': 4.8332e-05, 'epoch': 0.35, 'throughput': 1461.77}
[INFO|2026-01-07 16:57:41] logging.py:143 >> {'loss': 1.5115, 'learning_rate': 4.8295e-05, 'epoch': 0.36, 'throughput': 1462.27}
[INFO|2026-01-07 16:57:57] logging.py:143 >> {'loss': 1.6964, 'learning_rate': 4.8258e-05, 'epoch': 0.36, 'throughput': 1461.63}
[INFO|2026-01-07 16:58:12] logging.py:143 >> {'loss': 1.7301, 'learning_rate': 4.8221e-05, 'epoch': 0.36, 'throughput': 1460.70}
[INFO|2026-01-07 16:58:31] logging.py:143 >> {'loss': 1.5337, 'learning_rate': 4.8183e-05, 'epoch': 0.37, 'throughput': 1461.20}
[INFO|2026-01-07 16:58:47] logging.py:143 >> {'loss': 1.5954, 'learning_rate': 4.8145e-05, 'epoch': 0.37, 'throughput': 1460.64}
[INFO|2026-01-07 16:59:05] logging.py:143 >> {'loss': 1.3825, 'learning_rate': 4.8107e-05, 'epoch': 0.37, 'throughput': 1460.72}
[INFO|2026-01-07 16:59:23] logging.py:143 >> {'loss': 1.3837, 'learning_rate': 4.8068e-05, 'epoch': 0.38, 'throughput': 1461.38}
[INFO|2026-01-07 16:59:41] logging.py:143 >> {'loss': 1.3528, 'learning_rate': 4.8029e-05, 'epoch': 0.38, 'throughput': 1461.66}
[INFO|2026-01-07 16:59:58] logging.py:143 >> {'loss': 1.7386, 'learning_rate': 4.7989e-05, 'epoch': 0.39, 'throughput': 1461.55}
[INFO|2026-01-07 16:59:58] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-500
[INFO|2026-01-07 16:59:59] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 16:59:59] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 16:59:59] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-500\chat_template.jinja
[INFO|2026-01-07 16:59:59] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-500\tokenizer_config.json
[INFO|2026-01-07 16:59:59] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-500\special_tokens_map.json
[INFO|2026-01-07 17:00:17] logging.py:143 >> {'loss': 1.6453, 'learning_rate': 4.7949e-05, 'epoch': 0.39, 'throughput': 1461.07}
[INFO|2026-01-07 17:00:35] logging.py:143 >> {'loss': 1.9343, 'learning_rate': 4.7909e-05, 'epoch': 0.39, 'throughput': 1461.13}
[INFO|2026-01-07 17:00:54] logging.py:143 >> {'loss': 1.4994, 'learning_rate': 4.7868e-05, 'epoch': 0.40, 'throughput': 1461.49}
[INFO|2026-01-07 17:01:12] logging.py:143 >> {'loss': 1.6767, 'learning_rate': 4.7827e-05, 'epoch': 0.40, 'throughput': 1461.08}
[INFO|2026-01-07 17:01:30] logging.py:143 >> {'loss': 1.6184, 'learning_rate': 4.7786e-05, 'epoch': 0.41, 'throughput': 1460.90}
[INFO|2026-01-07 17:01:49] logging.py:143 >> {'loss': 1.5209, 'learning_rate': 4.7744e-05, 'epoch': 0.41, 'throughput': 1461.38}
[INFO|2026-01-07 17:02:07] logging.py:143 >> {'loss': 1.6116, 'learning_rate': 4.7702e-05, 'epoch': 0.41, 'throughput': 1461.29}
[INFO|2026-01-07 17:02:25] logging.py:143 >> {'loss': 1.6554, 'learning_rate': 4.7659e-05, 'epoch': 0.42, 'throughput': 1461.46}
[INFO|2026-01-07 17:02:47] logging.py:143 >> {'loss': 1.0694, 'learning_rate': 4.7616e-05, 'epoch': 0.42, 'throughput': 1462.45}
[INFO|2026-01-07 17:03:09] logging.py:143 >> {'loss': 1.4463, 'learning_rate': 4.7573e-05, 'epoch': 0.43, 'throughput': 1463.78}
[INFO|2026-01-07 17:03:28] logging.py:143 >> {'loss': 1.2995, 'learning_rate': 4.7529e-05, 'epoch': 0.43, 'throughput': 1464.25}
[INFO|2026-01-07 17:03:45] logging.py:143 >> {'loss': 1.8469, 'learning_rate': 4.7485e-05, 'epoch': 0.43, 'throughput': 1463.82}
[INFO|2026-01-07 17:04:02] logging.py:143 >> {'loss': 1.7679, 'learning_rate': 4.7441e-05, 'epoch': 0.44, 'throughput': 1463.44}
[INFO|2026-01-07 17:04:17] logging.py:143 >> {'loss': 1.5461, 'learning_rate': 4.7396e-05, 'epoch': 0.44, 'throughput': 1462.60}
[INFO|2026-01-07 17:04:35] logging.py:143 >> {'loss': 1.8573, 'learning_rate': 4.7351e-05, 'epoch': 0.44, 'throughput': 1462.17}
[INFO|2026-01-07 17:04:54] logging.py:143 >> {'loss': 1.4282, 'learning_rate': 4.7305e-05, 'epoch': 0.45, 'throughput': 1462.08}
[INFO|2026-01-07 17:05:12] logging.py:143 >> {'loss': 1.6687, 'learning_rate': 4.7260e-05, 'epoch': 0.45, 'throughput': 1461.42}
[INFO|2026-01-07 17:05:30] logging.py:143 >> {'loss': 1.7393, 'learning_rate': 4.7213e-05, 'epoch': 0.46, 'throughput': 1461.08}
[INFO|2026-01-07 17:05:51] logging.py:143 >> {'loss': 1.6126, 'learning_rate': 4.7167e-05, 'epoch': 0.46, 'throughput': 1461.33}
[INFO|2026-01-07 17:06:08] logging.py:143 >> {'loss': 1.6233, 'learning_rate': 4.7120e-05, 'epoch': 0.46, 'throughput': 1460.41}
[INFO|2026-01-07 17:06:08] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-600
[INFO|2026-01-07 17:06:08] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:06:08] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:06:08] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-600\chat_template.jinja
[INFO|2026-01-07 17:06:08] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-600\tokenizer_config.json
[INFO|2026-01-07 17:06:08] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-600\special_tokens_map.json
[INFO|2026-01-07 17:06:27] logging.py:143 >> {'loss': 1.4474, 'learning_rate': 4.7072e-05, 'epoch': 0.47, 'throughput': 1458.97}
[INFO|2026-01-07 17:06:47] logging.py:143 >> {'loss': 1.3311, 'learning_rate': 4.7025e-05, 'epoch': 0.47, 'throughput': 1458.76}
[INFO|2026-01-07 17:07:12] logging.py:143 >> {'loss': 1.2148, 'learning_rate': 4.6977e-05, 'epoch': 0.48, 'throughput': 1453.90}
[INFO|2026-01-07 17:07:36] logging.py:143 >> {'loss': 1.5333, 'learning_rate': 4.6928e-05, 'epoch': 0.48, 'throughput': 1449.58}
[INFO|2026-01-07 17:07:53] logging.py:143 >> {'loss': 1.4907, 'learning_rate': 4.6880e-05, 'epoch': 0.48, 'throughput': 1449.66}
[INFO|2026-01-07 17:08:12] logging.py:143 >> {'loss': 1.1153, 'learning_rate': 4.6830e-05, 'epoch': 0.49, 'throughput': 1450.36}
[INFO|2026-01-07 17:08:30] logging.py:143 >> {'loss': 1.8125, 'learning_rate': 4.6781e-05, 'epoch': 0.49, 'throughput': 1450.40}
[INFO|2026-01-07 17:08:47] logging.py:143 >> {'loss': 1.7564, 'learning_rate': 4.6731e-05, 'epoch': 0.49, 'throughput': 1450.38}
[INFO|2026-01-07 17:09:06] logging.py:143 >> {'loss': 1.7468, 'learning_rate': 4.6681e-05, 'epoch': 0.50, 'throughput': 1450.56}
[INFO|2026-01-07 17:09:20] logging.py:143 >> {'loss': 2.0722, 'learning_rate': 4.6630e-05, 'epoch': 0.50, 'throughput': 1449.94}
[INFO|2026-01-07 17:09:38] logging.py:143 >> {'loss': 1.4161, 'learning_rate': 4.6579e-05, 'epoch': 0.51, 'throughput': 1449.98}
[INFO|2026-01-07 17:09:55] logging.py:143 >> {'loss': 1.3642, 'learning_rate': 4.6528e-05, 'epoch': 0.51, 'throughput': 1449.76}
[INFO|2026-01-07 17:10:10] logging.py:143 >> {'loss': 1.9097, 'learning_rate': 4.6477e-05, 'epoch': 0.51, 'throughput': 1448.97}
[INFO|2026-01-07 17:10:31] logging.py:143 >> {'loss': 1.8367, 'learning_rate': 4.6425e-05, 'epoch': 0.52, 'throughput': 1449.92}
[INFO|2026-01-07 17:10:51] logging.py:143 >> {'loss': 1.6710, 'learning_rate': 4.6372e-05, 'epoch': 0.52, 'throughput': 1450.23}
[INFO|2026-01-07 17:11:08] logging.py:143 >> {'loss': 1.9120, 'learning_rate': 4.6320e-05, 'epoch': 0.53, 'throughput': 1450.01}
[INFO|2026-01-07 17:11:24] logging.py:143 >> {'loss': 1.5996, 'learning_rate': 4.6267e-05, 'epoch': 0.53, 'throughput': 1449.62}
[INFO|2026-01-07 17:11:41] logging.py:143 >> {'loss': 1.7360, 'learning_rate': 4.6213e-05, 'epoch': 0.53, 'throughput': 1447.62}
[INFO|2026-01-07 17:11:59] logging.py:143 >> {'loss': 1.6821, 'learning_rate': 4.6160e-05, 'epoch': 0.54, 'throughput': 1446.53}
[INFO|2026-01-07 17:12:19] logging.py:143 >> {'loss': 1.5540, 'learning_rate': 4.6106e-05, 'epoch': 0.54, 'throughput': 1445.86}
[INFO|2026-01-07 17:12:19] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-700
[INFO|2026-01-07 17:12:19] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:12:19] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:12:19] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-700\chat_template.jinja
[INFO|2026-01-07 17:12:19] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-700\tokenizer_config.json
[INFO|2026-01-07 17:12:19] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-700\special_tokens_map.json
[INFO|2026-01-07 17:12:38] logging.py:143 >> {'loss': 1.6636, 'learning_rate': 4.6051e-05, 'epoch': 0.54, 'throughput': 1445.81}
[INFO|2026-01-07 17:12:57] logging.py:143 >> {'loss': 1.5502, 'learning_rate': 4.5996e-05, 'epoch': 0.55, 'throughput': 1446.22}
[INFO|2026-01-07 17:13:14] logging.py:143 >> {'loss': 1.4389, 'learning_rate': 4.5941e-05, 'epoch': 0.55, 'throughput': 1446.22}
[INFO|2026-01-07 17:13:31] logging.py:143 >> {'loss': 1.3894, 'learning_rate': 4.5886e-05, 'epoch': 0.56, 'throughput': 1446.28}
[INFO|2026-01-07 17:13:47] logging.py:143 >> {'loss': 1.8154, 'learning_rate': 4.5830e-05, 'epoch': 0.56, 'throughput': 1445.99}
[INFO|2026-01-07 17:14:05] logging.py:143 >> {'loss': 1.2456, 'learning_rate': 4.5774e-05, 'epoch': 0.56, 'throughput': 1446.50}
[INFO|2026-01-07 17:14:23] logging.py:143 >> {'loss': 1.4994, 'learning_rate': 4.5718e-05, 'epoch': 0.57, 'throughput': 1446.49}
[INFO|2026-01-07 17:14:38] logging.py:143 >> {'loss': 1.6176, 'learning_rate': 4.5661e-05, 'epoch': 0.57, 'throughput': 1446.36}
[INFO|2026-01-07 17:14:56] logging.py:143 >> {'loss': 1.3931, 'learning_rate': 4.5604e-05, 'epoch': 0.58, 'throughput': 1446.38}
[INFO|2026-01-07 17:15:14] logging.py:143 >> {'loss': 1.4656, 'learning_rate': 4.5546e-05, 'epoch': 0.58, 'throughput': 1446.79}
[INFO|2026-01-07 17:15:32] logging.py:143 >> {'loss': 1.5694, 'learning_rate': 4.5488e-05, 'epoch': 0.58, 'throughput': 1446.99}
[INFO|2026-01-07 17:15:50] logging.py:143 >> {'loss': 1.5769, 'learning_rate': 4.5430e-05, 'epoch': 0.59, 'throughput': 1447.31}
[INFO|2026-01-07 17:16:07] logging.py:143 >> {'loss': 1.9009, 'learning_rate': 4.5372e-05, 'epoch': 0.59, 'throughput': 1447.18}
[INFO|2026-01-07 17:16:25] logging.py:143 >> {'loss': 1.4383, 'learning_rate': 4.5313e-05, 'epoch': 0.60, 'throughput': 1447.28}
[INFO|2026-01-07 17:16:43] logging.py:143 >> {'loss': 1.2766, 'learning_rate': 4.5254e-05, 'epoch': 0.60, 'throughput': 1447.67}
[INFO|2026-01-07 17:17:00] logging.py:143 >> {'loss': 1.5826, 'learning_rate': 4.5194e-05, 'epoch': 0.60, 'throughput': 1447.64}
[INFO|2026-01-07 17:17:17] logging.py:143 >> {'loss': 1.3160, 'learning_rate': 4.5135e-05, 'epoch': 0.61, 'throughput': 1447.67}
[INFO|2026-01-07 17:17:32] logging.py:143 >> {'loss': 1.6125, 'learning_rate': 4.5075e-05, 'epoch': 0.61, 'throughput': 1447.33}
[INFO|2026-01-07 17:17:49] logging.py:143 >> {'loss': 1.5317, 'learning_rate': 4.5014e-05, 'epoch': 0.61, 'throughput': 1447.62}
[INFO|2026-01-07 17:18:03] logging.py:143 >> {'loss': 1.7109, 'learning_rate': 4.4953e-05, 'epoch': 0.62, 'throughput': 1446.91}
[INFO|2026-01-07 17:18:03] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-800
[INFO|2026-01-07 17:18:03] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:18:03] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:18:03] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-800\chat_template.jinja
[INFO|2026-01-07 17:18:03] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-800\tokenizer_config.json
[INFO|2026-01-07 17:18:03] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-800\special_tokens_map.json
[INFO|2026-01-07 17:18:23] logging.py:143 >> {'loss': 1.4410, 'learning_rate': 4.4892e-05, 'epoch': 0.62, 'throughput': 1447.15}
[INFO|2026-01-07 17:18:38] logging.py:143 >> {'loss': 1.3226, 'learning_rate': 4.4831e-05, 'epoch': 0.63, 'throughput': 1446.87}
[INFO|2026-01-07 17:18:54] logging.py:143 >> {'loss': 1.7669, 'learning_rate': 4.4769e-05, 'epoch': 0.63, 'throughput': 1446.67}
[INFO|2026-01-07 17:19:13] logging.py:143 >> {'loss': 1.7791, 'learning_rate': 4.4707e-05, 'epoch': 0.63, 'throughput': 1446.94}
[INFO|2026-01-07 17:19:29] logging.py:143 >> {'loss': 1.6083, 'learning_rate': 4.4645e-05, 'epoch': 0.64, 'throughput': 1446.73}
[INFO|2026-01-07 17:19:46] logging.py:143 >> {'loss': 1.3810, 'learning_rate': 4.4582e-05, 'epoch': 0.64, 'throughput': 1446.83}
[INFO|2026-01-07 17:20:02] logging.py:143 >> {'loss': 1.4217, 'learning_rate': 4.4519e-05, 'epoch': 0.65, 'throughput': 1446.72}
[INFO|2026-01-07 17:20:20] logging.py:143 >> {'loss': 1.4638, 'learning_rate': 4.4455e-05, 'epoch': 0.65, 'throughput': 1446.72}
[INFO|2026-01-07 17:20:37] logging.py:143 >> {'loss': 1.3993, 'learning_rate': 4.4392e-05, 'epoch': 0.65, 'throughput': 1446.79}
[INFO|2026-01-07 17:20:55] logging.py:143 >> {'loss': 1.8240, 'learning_rate': 4.4328e-05, 'epoch': 0.66, 'throughput': 1446.94}
[INFO|2026-01-07 17:21:12] logging.py:143 >> {'loss': 1.9334, 'learning_rate': 4.4263e-05, 'epoch': 0.66, 'throughput': 1446.93}
[INFO|2026-01-07 17:21:30] logging.py:143 >> {'loss': 1.3226, 'learning_rate': 4.4199e-05, 'epoch': 0.66, 'throughput': 1447.32}
[INFO|2026-01-07 17:21:47] logging.py:143 >> {'loss': 1.6347, 'learning_rate': 4.4134e-05, 'epoch': 0.67, 'throughput': 1447.16}
[INFO|2026-01-07 17:22:02] logging.py:143 >> {'loss': 1.8404, 'learning_rate': 4.4068e-05, 'epoch': 0.67, 'throughput': 1446.33}
[INFO|2026-01-07 17:22:18] logging.py:143 >> {'loss': 1.5877, 'learning_rate': 4.4003e-05, 'epoch': 0.68, 'throughput': 1446.19}
[INFO|2026-01-07 17:22:37] logging.py:143 >> {'loss': 1.3713, 'learning_rate': 4.3937e-05, 'epoch': 0.68, 'throughput': 1446.29}
[INFO|2026-01-07 17:22:54] logging.py:143 >> {'loss': 1.5153, 'learning_rate': 4.3871e-05, 'epoch': 0.68, 'throughput': 1446.17}
[INFO|2026-01-07 17:23:12] logging.py:143 >> {'loss': 1.9655, 'learning_rate': 4.3804e-05, 'epoch': 0.69, 'throughput': 1446.26}
[INFO|2026-01-07 17:23:29] logging.py:143 >> {'loss': 1.6522, 'learning_rate': 4.3738e-05, 'epoch': 0.69, 'throughput': 1446.20}
[INFO|2026-01-07 17:23:49] logging.py:143 >> {'loss': 1.2513, 'learning_rate': 4.3670e-05, 'epoch': 0.70, 'throughput': 1446.59}
[INFO|2026-01-07 17:23:49] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-900
[INFO|2026-01-07 17:23:49] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:23:49] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:23:49] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-900\chat_template.jinja
[INFO|2026-01-07 17:23:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-900\tokenizer_config.json
[INFO|2026-01-07 17:23:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-900\special_tokens_map.json
[INFO|2026-01-07 17:24:07] logging.py:143 >> {'loss': 1.5750, 'learning_rate': 4.3603e-05, 'epoch': 0.70, 'throughput': 1446.57}
[INFO|2026-01-07 17:24:27] logging.py:143 >> {'loss': 1.5339, 'learning_rate': 4.3535e-05, 'epoch': 0.70, 'throughput': 1447.00}
[INFO|2026-01-07 17:24:44] logging.py:143 >> {'loss': 1.4408, 'learning_rate': 4.3467e-05, 'epoch': 0.71, 'throughput': 1447.05}
[INFO|2026-01-07 17:25:02] logging.py:143 >> {'loss': 1.2585, 'learning_rate': 4.3399e-05, 'epoch': 0.71, 'throughput': 1447.29}
[INFO|2026-01-07 17:25:19] logging.py:143 >> {'loss': 1.4254, 'learning_rate': 4.3330e-05, 'epoch': 0.71, 'throughput': 1447.29}
[INFO|2026-01-07 17:25:36] logging.py:143 >> {'loss': 1.3599, 'learning_rate': 4.3261e-05, 'epoch': 0.72, 'throughput': 1447.05}
[INFO|2026-01-07 17:25:53] logging.py:143 >> {'loss': 1.6105, 'learning_rate': 4.3192e-05, 'epoch': 0.72, 'throughput': 1446.87}
[INFO|2026-01-07 17:26:12] logging.py:143 >> {'loss': 1.5684, 'learning_rate': 4.3123e-05, 'epoch': 0.73, 'throughput': 1447.29}
[INFO|2026-01-07 17:26:32] logging.py:143 >> {'loss': 1.6671, 'learning_rate': 4.3053e-05, 'epoch': 0.73, 'throughput': 1447.70}
[INFO|2026-01-07 17:26:48] logging.py:143 >> {'loss': 1.5427, 'learning_rate': 4.2983e-05, 'epoch': 0.73, 'throughput': 1447.49}
[INFO|2026-01-07 17:27:05] logging.py:143 >> {'loss': 1.5009, 'learning_rate': 4.2912e-05, 'epoch': 0.74, 'throughput': 1447.49}
[INFO|2026-01-07 17:27:23] logging.py:143 >> {'loss': 1.3797, 'learning_rate': 4.2841e-05, 'epoch': 0.74, 'throughput': 1447.83}
[INFO|2026-01-07 17:27:41] logging.py:143 >> {'loss': 1.3231, 'learning_rate': 4.2770e-05, 'epoch': 0.75, 'throughput': 1447.91}
[INFO|2026-01-07 17:28:00] logging.py:143 >> {'loss': 1.4670, 'learning_rate': 4.2699e-05, 'epoch': 0.75, 'throughput': 1448.19}
[INFO|2026-01-07 17:28:16] logging.py:143 >> {'loss': 1.5811, 'learning_rate': 4.2628e-05, 'epoch': 0.75, 'throughput': 1447.91}
[INFO|2026-01-07 17:28:38] logging.py:143 >> {'loss': 1.4168, 'learning_rate': 4.2556e-05, 'epoch': 0.76, 'throughput': 1448.68}
[INFO|2026-01-07 17:28:54] logging.py:143 >> {'loss': 1.8599, 'learning_rate': 4.2483e-05, 'epoch': 0.76, 'throughput': 1448.46}
[INFO|2026-01-07 17:29:10] logging.py:143 >> {'loss': 1.2478, 'learning_rate': 4.2411e-05, 'epoch': 0.77, 'throughput': 1448.25}
[INFO|2026-01-07 17:29:30] logging.py:143 >> {'loss': 1.7238, 'learning_rate': 4.2338e-05, 'epoch': 0.77, 'throughput': 1448.20}
[INFO|2026-01-07 17:29:45] logging.py:143 >> {'loss': 1.6268, 'learning_rate': 4.2265e-05, 'epoch': 0.77, 'throughput': 1448.05}
[INFO|2026-01-07 17:29:45] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1000
[INFO|2026-01-07 17:29:45] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:29:45] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:29:45] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1000\chat_template.jinja
[INFO|2026-01-07 17:29:45] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1000\tokenizer_config.json
[INFO|2026-01-07 17:29:45] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1000\special_tokens_map.json
[INFO|2026-01-07 17:30:04] logging.py:143 >> {'loss': 1.7340, 'learning_rate': 4.2192e-05, 'epoch': 0.78, 'throughput': 1448.24}
[INFO|2026-01-07 17:30:22] logging.py:143 >> {'loss': 1.5039, 'learning_rate': 4.2118e-05, 'epoch': 0.78, 'throughput': 1448.25}
[INFO|2026-01-07 17:30:38] logging.py:143 >> {'loss': 1.2953, 'learning_rate': 4.2045e-05, 'epoch': 0.78, 'throughput': 1448.12}
[INFO|2026-01-07 17:30:58] logging.py:143 >> {'loss': 1.1203, 'learning_rate': 4.1970e-05, 'epoch': 0.79, 'throughput': 1448.64}
[INFO|2026-01-07 17:31:15] logging.py:143 >> {'loss': 1.6203, 'learning_rate': 4.1896e-05, 'epoch': 0.79, 'throughput': 1448.44}
[INFO|2026-01-07 17:31:33] logging.py:143 >> {'loss': 1.6231, 'learning_rate': 4.1821e-05, 'epoch': 0.80, 'throughput': 1448.58}
[INFO|2026-01-07 17:31:51] logging.py:143 >> {'loss': 1.2860, 'learning_rate': 4.1746e-05, 'epoch': 0.80, 'throughput': 1448.69}
[INFO|2026-01-07 17:32:07] logging.py:143 >> {'loss': 1.6365, 'learning_rate': 4.1671e-05, 'epoch': 0.80, 'throughput': 1448.29}
[INFO|2026-01-07 17:32:26] logging.py:143 >> {'loss': 1.4854, 'learning_rate': 4.1596e-05, 'epoch': 0.81, 'throughput': 1448.58}
[INFO|2026-01-07 17:32:47] logging.py:143 >> {'loss': 1.4473, 'learning_rate': 4.1520e-05, 'epoch': 0.81, 'throughput': 1449.06}
[INFO|2026-01-07 17:33:06] logging.py:143 >> {'loss': 1.6607, 'learning_rate': 4.1444e-05, 'epoch': 0.82, 'throughput': 1449.15}
[INFO|2026-01-07 17:33:24] logging.py:143 >> {'loss': 1.4622, 'learning_rate': 4.1367e-05, 'epoch': 0.82, 'throughput': 1449.22}
[INFO|2026-01-07 17:33:42] logging.py:143 >> {'loss': 1.4496, 'learning_rate': 4.1291e-05, 'epoch': 0.82, 'throughput': 1449.35}
[INFO|2026-01-07 17:33:59] logging.py:143 >> {'loss': 1.7525, 'learning_rate': 4.1214e-05, 'epoch': 0.83, 'throughput': 1449.27}
[INFO|2026-01-07 17:34:17] logging.py:143 >> {'loss': 1.6701, 'learning_rate': 4.1137e-05, 'epoch': 0.83, 'throughput': 1449.37}
[INFO|2026-01-07 17:34:32] logging.py:143 >> {'loss': 1.6348, 'learning_rate': 4.1059e-05, 'epoch': 0.83, 'throughput': 1449.01}
[INFO|2026-01-07 17:34:49] logging.py:143 >> {'loss': 1.6782, 'learning_rate': 4.0982e-05, 'epoch': 0.84, 'throughput': 1449.00}
[INFO|2026-01-07 17:35:04] logging.py:143 >> {'loss': 1.5114, 'learning_rate': 4.0904e-05, 'epoch': 0.84, 'throughput': 1448.79}
[INFO|2026-01-07 17:35:22] logging.py:143 >> {'loss': 1.5297, 'learning_rate': 4.0826e-05, 'epoch': 0.85, 'throughput': 1448.83}
[INFO|2026-01-07 17:35:41] logging.py:143 >> {'loss': 1.4466, 'learning_rate': 4.0747e-05, 'epoch': 0.85, 'throughput': 1449.24}
[INFO|2026-01-07 17:35:41] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1100
[INFO|2026-01-07 17:35:41] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:35:41] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:35:41] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1100\chat_template.jinja
[INFO|2026-01-07 17:35:41] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1100\tokenizer_config.json
[INFO|2026-01-07 17:35:41] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1100\special_tokens_map.json
[INFO|2026-01-07 17:36:00] logging.py:143 >> {'loss': 1.8495, 'learning_rate': 4.0668e-05, 'epoch': 0.85, 'throughput': 1449.15}
[INFO|2026-01-07 17:36:18] logging.py:143 >> {'loss': 1.6160, 'learning_rate': 4.0590e-05, 'epoch': 0.86, 'throughput': 1449.21}
[INFO|2026-01-07 17:36:36] logging.py:143 >> {'loss': 1.6146, 'learning_rate': 4.0510e-05, 'epoch': 0.86, 'throughput': 1449.16}
[INFO|2026-01-07 17:36:52] logging.py:143 >> {'loss': 2.0874, 'learning_rate': 4.0431e-05, 'epoch': 0.87, 'throughput': 1448.98}
[INFO|2026-01-07 17:37:11] logging.py:143 >> {'loss': 1.3873, 'learning_rate': 4.0351e-05, 'epoch': 0.87, 'throughput': 1449.16}
[INFO|2026-01-07 17:37:27] logging.py:143 >> {'loss': 1.6945, 'learning_rate': 4.0271e-05, 'epoch': 0.87, 'throughput': 1448.96}
[INFO|2026-01-07 17:37:46] logging.py:143 >> {'loss': 1.6338, 'learning_rate': 4.0191e-05, 'epoch': 0.88, 'throughput': 1449.01}
[INFO|2026-01-07 17:38:03] logging.py:143 >> {'loss': 1.5215, 'learning_rate': 4.0110e-05, 'epoch': 0.88, 'throughput': 1448.91}
[INFO|2026-01-07 17:38:19] logging.py:143 >> {'loss': 1.5948, 'learning_rate': 4.0030e-05, 'epoch': 0.89, 'throughput': 1448.65}
[INFO|2026-01-07 17:38:37] logging.py:143 >> {'loss': 1.4340, 'learning_rate': 3.9949e-05, 'epoch': 0.89, 'throughput': 1448.80}
[INFO|2026-01-07 17:38:55] logging.py:143 >> {'loss': 1.3173, 'learning_rate': 3.9868e-05, 'epoch': 0.89, 'throughput': 1449.04}
[INFO|2026-01-07 17:39:13] logging.py:143 >> {'loss': 1.3085, 'learning_rate': 3.9786e-05, 'epoch': 0.90, 'throughput': 1449.05}
[INFO|2026-01-07 17:39:33] logging.py:143 >> {'loss': 1.4357, 'learning_rate': 3.9704e-05, 'epoch': 0.90, 'throughput': 1449.47}
[INFO|2026-01-07 17:39:50] logging.py:143 >> {'loss': 1.3259, 'learning_rate': 3.9623e-05, 'epoch': 0.90, 'throughput': 1449.46}
[INFO|2026-01-07 17:40:08] logging.py:143 >> {'loss': 1.6079, 'learning_rate': 3.9540e-05, 'epoch': 0.91, 'throughput': 1449.48}
[INFO|2026-01-07 17:40:27] logging.py:143 >> {'loss': 1.5498, 'learning_rate': 3.9458e-05, 'epoch': 0.91, 'throughput': 1449.78}
[INFO|2026-01-07 17:40:44] logging.py:143 >> {'loss': 1.4474, 'learning_rate': 3.9375e-05, 'epoch': 0.92, 'throughput': 1449.86}
[INFO|2026-01-07 17:41:02] logging.py:143 >> {'loss': 1.3841, 'learning_rate': 3.9292e-05, 'epoch': 0.92, 'throughput': 1449.62}
[INFO|2026-01-07 17:41:19] logging.py:143 >> {'loss': 1.7222, 'learning_rate': 3.9209e-05, 'epoch': 0.92, 'throughput': 1449.60}
[INFO|2026-01-07 17:41:39] logging.py:143 >> {'loss': 1.6465, 'learning_rate': 3.9126e-05, 'epoch': 0.93, 'throughput': 1449.88}
[INFO|2026-01-07 17:41:39] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1200
[INFO|2026-01-07 17:41:39] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:41:39] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:41:39] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1200\chat_template.jinja
[INFO|2026-01-07 17:41:39] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1200\tokenizer_config.json
[INFO|2026-01-07 17:41:39] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1200\special_tokens_map.json
[INFO|2026-01-07 17:41:53] logging.py:143 >> {'loss': 1.5593, 'learning_rate': 3.9042e-05, 'epoch': 0.93, 'throughput': 1449.29}
[INFO|2026-01-07 17:42:09] logging.py:143 >> {'loss': 1.9851, 'learning_rate': 3.8959e-05, 'epoch': 0.94, 'throughput': 1449.08}
[INFO|2026-01-07 17:42:26] logging.py:143 >> {'loss': 1.2861, 'learning_rate': 3.8875e-05, 'epoch': 0.94, 'throughput': 1449.12}
[INFO|2026-01-07 17:42:45] logging.py:143 >> {'loss': 1.5288, 'learning_rate': 3.8790e-05, 'epoch': 0.94, 'throughput': 1449.35}
[INFO|2026-01-07 17:43:02] logging.py:143 >> {'loss': 1.2528, 'learning_rate': 3.8706e-05, 'epoch': 0.95, 'throughput': 1449.26}
[INFO|2026-01-07 17:43:22] logging.py:143 >> {'loss': 1.5671, 'learning_rate': 3.8621e-05, 'epoch': 0.95, 'throughput': 1449.53}
[INFO|2026-01-07 17:43:41] logging.py:143 >> {'loss': 1.2371, 'learning_rate': 3.8536e-05, 'epoch': 0.95, 'throughput': 1449.82}
[INFO|2026-01-07 17:43:58] logging.py:143 >> {'loss': 1.7117, 'learning_rate': 3.8451e-05, 'epoch': 0.96, 'throughput': 1449.87}
[INFO|2026-01-07 17:44:15] logging.py:143 >> {'loss': 1.6050, 'learning_rate': 3.8366e-05, 'epoch': 0.96, 'throughput': 1449.91}
[INFO|2026-01-07 17:44:35] logging.py:143 >> {'loss': 1.5083, 'learning_rate': 3.8280e-05, 'epoch': 0.97, 'throughput': 1450.22}
[INFO|2026-01-07 17:44:52] logging.py:143 >> {'loss': 1.5284, 'learning_rate': 3.8194e-05, 'epoch': 0.97, 'throughput': 1450.30}
[INFO|2026-01-07 17:45:10] logging.py:143 >> {'loss': 1.3681, 'learning_rate': 3.8108e-05, 'epoch': 0.97, 'throughput': 1450.30}
[INFO|2026-01-07 17:45:27] logging.py:143 >> {'loss': 1.4656, 'learning_rate': 3.8022e-05, 'epoch': 0.98, 'throughput': 1450.34}
[INFO|2026-01-07 17:45:44] logging.py:143 >> {'loss': 1.5518, 'learning_rate': 3.7935e-05, 'epoch': 0.98, 'throughput': 1450.27}
[INFO|2026-01-07 17:46:02] logging.py:143 >> {'loss': 1.2950, 'learning_rate': 3.7849e-05, 'epoch': 0.99, 'throughput': 1450.26}
[INFO|2026-01-07 17:46:20] logging.py:143 >> {'loss': 1.5044, 'learning_rate': 3.7762e-05, 'epoch': 0.99, 'throughput': 1450.36}
[INFO|2026-01-07 17:46:38] logging.py:143 >> {'loss': 1.4623, 'learning_rate': 3.7675e-05, 'epoch': 0.99, 'throughput': 1450.38}
[INFO|2026-01-07 17:46:59] logging.py:143 >> {'loss': 1.1012, 'learning_rate': 3.7588e-05, 'epoch': 1.00, 'throughput': 1450.69}
[INFO|2026-01-07 17:47:16] logging.py:143 >> {'loss': 0.8731, 'learning_rate': 3.7500e-05, 'epoch': 1.00, 'throughput': 1450.89}
[INFO|2026-01-07 17:47:34] logging.py:143 >> {'loss': 1.7527, 'learning_rate': 3.7412e-05, 'epoch': 1.00, 'throughput': 1450.88}
[INFO|2026-01-07 17:47:34] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1300
[INFO|2026-01-07 17:47:34] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:47:34] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:47:34] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1300\chat_template.jinja
[INFO|2026-01-07 17:47:34] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1300\tokenizer_config.json
[INFO|2026-01-07 17:47:34] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1300\special_tokens_map.json
[INFO|2026-01-07 17:47:52] logging.py:143 >> {'loss': 1.3654, 'learning_rate': 3.7324e-05, 'epoch': 1.01, 'throughput': 1450.81}
[INFO|2026-01-07 17:48:08] logging.py:143 >> {'loss': 1.2418, 'learning_rate': 3.7236e-05, 'epoch': 1.01, 'throughput': 1450.59}
[INFO|2026-01-07 17:48:26] logging.py:143 >> {'loss': 1.2777, 'learning_rate': 3.7148e-05, 'epoch': 1.02, 'throughput': 1450.73}
[INFO|2026-01-07 17:48:43] logging.py:143 >> {'loss': 1.6902, 'learning_rate': 3.7059e-05, 'epoch': 1.02, 'throughput': 1450.68}
[INFO|2026-01-07 17:49:03] logging.py:143 >> {'loss': 1.5705, 'learning_rate': 3.6971e-05, 'epoch': 1.02, 'throughput': 1450.93}
[INFO|2026-01-07 17:49:23] logging.py:143 >> {'loss': 1.4673, 'learning_rate': 3.6882e-05, 'epoch': 1.03, 'throughput': 1451.26}
[INFO|2026-01-07 17:49:41] logging.py:143 >> {'loss': 1.6254, 'learning_rate': 3.6793e-05, 'epoch': 1.03, 'throughput': 1451.20}
[INFO|2026-01-07 17:50:01] logging.py:143 >> {'loss': 1.3235, 'learning_rate': 3.6703e-05, 'epoch': 1.04, 'throughput': 1451.45}
[INFO|2026-01-07 17:50:18] logging.py:143 >> {'loss': 1.4963, 'learning_rate': 3.6614e-05, 'epoch': 1.04, 'throughput': 1451.37}
[INFO|2026-01-07 17:50:37] logging.py:143 >> {'loss': 1.5513, 'learning_rate': 3.6524e-05, 'epoch': 1.04, 'throughput': 1451.39}
[INFO|2026-01-07 17:50:54] logging.py:143 >> {'loss': 1.5539, 'learning_rate': 3.6434e-05, 'epoch': 1.05, 'throughput': 1451.22}
[INFO|2026-01-07 17:51:14] logging.py:143 >> {'loss': 1.6809, 'learning_rate': 3.6344e-05, 'epoch': 1.05, 'throughput': 1451.48}
[INFO|2026-01-07 17:51:33] logging.py:143 >> {'loss': 1.1839, 'learning_rate': 3.6254e-05, 'epoch': 1.05, 'throughput': 1451.82}
[INFO|2026-01-07 17:51:51] logging.py:143 >> {'loss': 1.2550, 'learning_rate': 3.6164e-05, 'epoch': 1.06, 'throughput': 1451.83}
[INFO|2026-01-07 17:52:11] logging.py:143 >> {'loss': 1.4685, 'learning_rate': 3.6073e-05, 'epoch': 1.06, 'throughput': 1452.10}
[INFO|2026-01-07 17:52:27] logging.py:143 >> {'loss': 1.1016, 'learning_rate': 3.5982e-05, 'epoch': 1.07, 'throughput': 1452.03}
[INFO|2026-01-07 17:52:46] logging.py:143 >> {'loss': 1.3622, 'learning_rate': 3.5891e-05, 'epoch': 1.07, 'throughput': 1452.23}
[INFO|2026-01-07 17:53:03] logging.py:143 >> {'loss': 1.7417, 'learning_rate': 3.5800e-05, 'epoch': 1.07, 'throughput': 1452.23}
[INFO|2026-01-07 17:53:21] logging.py:143 >> {'loss': 1.4181, 'learning_rate': 3.5709e-05, 'epoch': 1.08, 'throughput': 1452.13}
[INFO|2026-01-07 17:53:40] logging.py:143 >> {'loss': 1.5207, 'learning_rate': 3.5617e-05, 'epoch': 1.08, 'throughput': 1452.15}
[INFO|2026-01-07 17:53:40] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1400
[INFO|2026-01-07 17:53:40] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:53:40] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:53:40] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1400\chat_template.jinja
[INFO|2026-01-07 17:53:40] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1400\tokenizer_config.json
[INFO|2026-01-07 17:53:40] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1400\special_tokens_map.json
[INFO|2026-01-07 17:54:02] logging.py:143 >> {'loss': 1.2937, 'learning_rate': 3.5526e-05, 'epoch': 1.09, 'throughput': 1452.50}
[INFO|2026-01-07 17:54:20] logging.py:143 >> {'loss': 1.3688, 'learning_rate': 3.5434e-05, 'epoch': 1.09, 'throughput': 1452.37}
[INFO|2026-01-07 17:54:38] logging.py:143 >> {'loss': 1.3466, 'learning_rate': 3.5342e-05, 'epoch': 1.09, 'throughput': 1452.37}
[INFO|2026-01-07 17:54:54] logging.py:143 >> {'loss': 1.6225, 'learning_rate': 3.5250e-05, 'epoch': 1.10, 'throughput': 1452.20}
[INFO|2026-01-07 17:55:11] logging.py:143 >> {'loss': 1.5270, 'learning_rate': 3.5157e-05, 'epoch': 1.10, 'throughput': 1452.33}
[INFO|2026-01-07 17:55:28] logging.py:143 >> {'loss': 1.5353, 'learning_rate': 3.5065e-05, 'epoch': 1.11, 'throughput': 1452.21}
[INFO|2026-01-07 17:55:46] logging.py:143 >> {'loss': 1.7473, 'learning_rate': 3.4972e-05, 'epoch': 1.11, 'throughput': 1452.15}
[INFO|2026-01-07 17:56:03] logging.py:143 >> {'loss': 1.5188, 'learning_rate': 3.4879e-05, 'epoch': 1.11, 'throughput': 1451.97}
[INFO|2026-01-07 17:56:19] logging.py:143 >> {'loss': 1.2691, 'learning_rate': 3.4786e-05, 'epoch': 1.12, 'throughput': 1451.94}
[INFO|2026-01-07 17:56:38] logging.py:143 >> {'loss': 1.6921, 'learning_rate': 3.4693e-05, 'epoch': 1.12, 'throughput': 1452.03}
[INFO|2026-01-07 17:56:56] logging.py:143 >> {'loss': 1.2814, 'learning_rate': 3.4600e-05, 'epoch': 1.12, 'throughput': 1452.01}
[INFO|2026-01-07 17:57:15] logging.py:143 >> {'loss': 1.4345, 'learning_rate': 3.4506e-05, 'epoch': 1.13, 'throughput': 1452.11}
[INFO|2026-01-07 17:57:32] logging.py:143 >> {'loss': 1.6922, 'learning_rate': 3.4413e-05, 'epoch': 1.13, 'throughput': 1452.10}
[INFO|2026-01-07 17:57:49] logging.py:143 >> {'loss': 1.3996, 'learning_rate': 3.4319e-05, 'epoch': 1.14, 'throughput': 1452.12}
[INFO|2026-01-07 17:58:05] logging.py:143 >> {'loss': 1.6832, 'learning_rate': 3.4225e-05, 'epoch': 1.14, 'throughput': 1451.95}
[INFO|2026-01-07 17:58:27] logging.py:143 >> {'loss': 1.4101, 'learning_rate': 3.4131e-05, 'epoch': 1.14, 'throughput': 1452.42}
[INFO|2026-01-07 17:58:43] logging.py:143 >> {'loss': 1.0718, 'learning_rate': 3.4037e-05, 'epoch': 1.15, 'throughput': 1452.36}
[INFO|2026-01-07 17:59:01] logging.py:143 >> {'loss': 1.5385, 'learning_rate': 3.3942e-05, 'epoch': 1.15, 'throughput': 1452.26}
[INFO|2026-01-07 17:59:17] logging.py:143 >> {'loss': 1.5590, 'learning_rate': 3.3848e-05, 'epoch': 1.16, 'throughput': 1452.11}
[INFO|2026-01-07 17:59:34] logging.py:143 >> {'loss': 1.5271, 'learning_rate': 3.3753e-05, 'epoch': 1.16, 'throughput': 1452.07}
[INFO|2026-01-07 17:59:34] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1500
[INFO|2026-01-07 17:59:34] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 17:59:34] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 17:59:34] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1500\chat_template.jinja
[INFO|2026-01-07 17:59:34] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1500\tokenizer_config.json
[INFO|2026-01-07 17:59:34] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1500\special_tokens_map.json
[INFO|2026-01-07 17:59:51] logging.py:143 >> {'loss': 1.8744, 'learning_rate': 3.3658e-05, 'epoch': 1.16, 'throughput': 1451.66}
[INFO|2026-01-07 18:00:11] logging.py:143 >> {'loss': 1.2914, 'learning_rate': 3.3563e-05, 'epoch': 1.17, 'throughput': 1452.08}
[INFO|2026-01-07 18:00:28] logging.py:143 >> {'loss': 1.1820, 'learning_rate': 3.3468e-05, 'epoch': 1.17, 'throughput': 1451.95}
[INFO|2026-01-07 18:00:45] logging.py:143 >> {'loss': 1.2098, 'learning_rate': 3.3373e-05, 'epoch': 1.17, 'throughput': 1451.96}
[INFO|2026-01-07 18:01:03] logging.py:143 >> {'loss': 1.5137, 'learning_rate': 3.3277e-05, 'epoch': 1.18, 'throughput': 1452.06}
[INFO|2026-01-07 18:01:21] logging.py:143 >> {'loss': 1.8736, 'learning_rate': 3.3182e-05, 'epoch': 1.18, 'throughput': 1451.96}
[INFO|2026-01-07 18:01:37] logging.py:143 >> {'loss': 1.2116, 'learning_rate': 3.3086e-05, 'epoch': 1.19, 'throughput': 1451.82}
[INFO|2026-01-07 18:01:53] logging.py:143 >> {'loss': 1.4354, 'learning_rate': 3.2990e-05, 'epoch': 1.19, 'throughput': 1451.60}
[INFO|2026-01-07 18:02:09] logging.py:143 >> {'loss': 1.3225, 'learning_rate': 3.2895e-05, 'epoch': 1.19, 'throughput': 1451.39}
[INFO|2026-01-07 18:02:26] logging.py:143 >> {'loss': 1.3908, 'learning_rate': 3.2799e-05, 'epoch': 1.20, 'throughput': 1451.35}
[INFO|2026-01-07 18:02:44] logging.py:143 >> {'loss': 1.4299, 'learning_rate': 3.2702e-05, 'epoch': 1.20, 'throughput': 1451.37}
[INFO|2026-01-07 18:03:03] logging.py:143 >> {'loss': 1.2691, 'learning_rate': 3.2606e-05, 'epoch': 1.21, 'throughput': 1451.37}
[INFO|2026-01-07 18:03:20] logging.py:143 >> {'loss': 1.4616, 'learning_rate': 3.2510e-05, 'epoch': 1.21, 'throughput': 1451.47}
[INFO|2026-01-07 18:03:39] logging.py:143 >> {'loss': 1.5039, 'learning_rate': 3.2413e-05, 'epoch': 1.21, 'throughput': 1451.60}
[INFO|2026-01-07 18:03:56] logging.py:143 >> {'loss': 1.0546, 'learning_rate': 3.2316e-05, 'epoch': 1.22, 'throughput': 1451.56}
[INFO|2026-01-07 18:04:11] logging.py:143 >> {'loss': 1.3795, 'learning_rate': 3.2220e-05, 'epoch': 1.22, 'throughput': 1451.39}
[INFO|2026-01-07 18:04:27] logging.py:143 >> {'loss': 1.3413, 'learning_rate': 3.2123e-05, 'epoch': 1.22, 'throughput': 1451.08}
[INFO|2026-01-07 18:04:45] logging.py:143 >> {'loss': 1.2312, 'learning_rate': 3.2026e-05, 'epoch': 1.23, 'throughput': 1451.07}
[INFO|2026-01-07 18:05:03] logging.py:143 >> {'loss': 1.3814, 'learning_rate': 3.1929e-05, 'epoch': 1.23, 'throughput': 1451.13}
[INFO|2026-01-07 18:05:19] logging.py:143 >> {'loss': 1.4209, 'learning_rate': 3.1831e-05, 'epoch': 1.24, 'throughput': 1451.05}
[INFO|2026-01-07 18:05:19] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1600
[INFO|2026-01-07 18:05:19] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:05:19] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:05:19] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1600\chat_template.jinja
[INFO|2026-01-07 18:05:19] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1600\tokenizer_config.json
[INFO|2026-01-07 18:05:19] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1600\special_tokens_map.json
[INFO|2026-01-07 18:05:36] logging.py:143 >> {'loss': 1.4203, 'learning_rate': 3.1734e-05, 'epoch': 1.24, 'throughput': 1450.75}
[INFO|2026-01-07 18:05:54] logging.py:143 >> {'loss': 1.3505, 'learning_rate': 3.1636e-05, 'epoch': 1.24, 'throughput': 1450.76}
[INFO|2026-01-07 18:06:10] logging.py:143 >> {'loss': 1.6069, 'learning_rate': 3.1539e-05, 'epoch': 1.25, 'throughput': 1450.61}
[INFO|2026-01-07 18:06:29] logging.py:143 >> {'loss': 1.4736, 'learning_rate': 3.1441e-05, 'epoch': 1.25, 'throughput': 1450.63}
[INFO|2026-01-07 18:06:45] logging.py:143 >> {'loss': 1.3919, 'learning_rate': 3.1343e-05, 'epoch': 1.26, 'throughput': 1450.40}
[INFO|2026-01-07 18:07:03] logging.py:143 >> {'loss': 1.5796, 'learning_rate': 3.1245e-05, 'epoch': 1.26, 'throughput': 1450.50}
[INFO|2026-01-07 18:07:21] logging.py:143 >> {'loss': 1.3634, 'learning_rate': 3.1147e-05, 'epoch': 1.26, 'throughput': 1450.73}
[INFO|2026-01-07 18:07:39] logging.py:143 >> {'loss': 1.4690, 'learning_rate': 3.1049e-05, 'epoch': 1.27, 'throughput': 1450.63}
[INFO|2026-01-07 18:07:59] logging.py:143 >> {'loss': 1.2186, 'learning_rate': 3.0951e-05, 'epoch': 1.27, 'throughput': 1450.86}
[INFO|2026-01-07 18:08:17] logging.py:143 >> {'loss': 1.5756, 'learning_rate': 3.0853e-05, 'epoch': 1.28, 'throughput': 1450.89}
[INFO|2026-01-07 18:08:34] logging.py:143 >> {'loss': 1.3475, 'learning_rate': 3.0754e-05, 'epoch': 1.28, 'throughput': 1450.82}
[INFO|2026-01-07 18:08:53] logging.py:143 >> {'loss': 1.3289, 'learning_rate': 3.0656e-05, 'epoch': 1.28, 'throughput': 1450.95}
[INFO|2026-01-07 18:09:11] logging.py:143 >> {'loss': 1.2469, 'learning_rate': 3.0557e-05, 'epoch': 1.29, 'throughput': 1451.06}
[INFO|2026-01-07 18:09:29] logging.py:143 >> {'loss': 1.8706, 'learning_rate': 3.0459e-05, 'epoch': 1.29, 'throughput': 1450.98}
[INFO|2026-01-07 18:09:49] logging.py:143 >> {'loss': 1.4756, 'learning_rate': 3.0360e-05, 'epoch': 1.29, 'throughput': 1451.00}
[INFO|2026-01-07 18:10:09] logging.py:143 >> {'loss': 1.2720, 'learning_rate': 3.0261e-05, 'epoch': 1.30, 'throughput': 1451.33}
[INFO|2026-01-07 18:10:25] logging.py:143 >> {'loss': 1.9000, 'learning_rate': 3.0162e-05, 'epoch': 1.30, 'throughput': 1451.18}
[INFO|2026-01-07 18:10:43] logging.py:143 >> {'loss': 1.6129, 'learning_rate': 3.0063e-05, 'epoch': 1.31, 'throughput': 1451.18}
[INFO|2026-01-07 18:11:00] logging.py:143 >> {'loss': 1.4590, 'learning_rate': 2.9964e-05, 'epoch': 1.31, 'throughput': 1451.06}
[INFO|2026-01-07 18:11:21] logging.py:143 >> {'loss': 1.0681, 'learning_rate': 2.9865e-05, 'epoch': 1.31, 'throughput': 1451.40}
[INFO|2026-01-07 18:11:21] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1700
[INFO|2026-01-07 18:11:21] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:11:21] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:11:21] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1700\chat_template.jinja
[INFO|2026-01-07 18:11:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1700\tokenizer_config.json
[INFO|2026-01-07 18:11:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1700\special_tokens_map.json
[INFO|2026-01-07 18:11:40] logging.py:143 >> {'loss': 1.2689, 'learning_rate': 2.9766e-05, 'epoch': 1.32, 'throughput': 1451.40}
[INFO|2026-01-07 18:11:56] logging.py:143 >> {'loss': 1.4253, 'learning_rate': 2.9666e-05, 'epoch': 1.32, 'throughput': 1451.16}
[INFO|2026-01-07 18:12:13] logging.py:143 >> {'loss': 1.3693, 'learning_rate': 2.9567e-05, 'epoch': 1.33, 'throughput': 1451.21}
[INFO|2026-01-07 18:12:29] logging.py:143 >> {'loss': 1.5063, 'learning_rate': 2.9467e-05, 'epoch': 1.33, 'throughput': 1450.91}
[INFO|2026-01-07 18:12:46] logging.py:143 >> {'loss': 1.5529, 'learning_rate': 2.9368e-05, 'epoch': 1.33, 'throughput': 1450.83}
[INFO|2026-01-07 18:13:03] logging.py:143 >> {'loss': 1.3197, 'learning_rate': 2.9268e-05, 'epoch': 1.34, 'throughput': 1450.94}
[INFO|2026-01-07 18:13:18] logging.py:143 >> {'loss': 1.6598, 'learning_rate': 2.9168e-05, 'epoch': 1.34, 'throughput': 1450.68}
[INFO|2026-01-07 18:13:36] logging.py:143 >> {'loss': 1.9803, 'learning_rate': 2.9069e-05, 'epoch': 1.34, 'throughput': 1450.64}
[INFO|2026-01-07 18:13:54] logging.py:143 >> {'loss': 1.2456, 'learning_rate': 2.8969e-05, 'epoch': 1.35, 'throughput': 1450.76}
[INFO|2026-01-07 18:14:13] logging.py:143 >> {'loss': 1.5420, 'learning_rate': 2.8869e-05, 'epoch': 1.35, 'throughput': 1450.92}
[INFO|2026-01-07 18:14:29] logging.py:143 >> {'loss': 1.4196, 'learning_rate': 2.8769e-05, 'epoch': 1.36, 'throughput': 1450.83}
[INFO|2026-01-07 18:14:47] logging.py:143 >> {'loss': 1.1316, 'learning_rate': 2.8669e-05, 'epoch': 1.36, 'throughput': 1450.85}
[INFO|2026-01-07 18:15:05] logging.py:143 >> {'loss': 1.2037, 'learning_rate': 2.8569e-05, 'epoch': 1.36, 'throughput': 1450.91}
[INFO|2026-01-07 18:15:22] logging.py:143 >> {'loss': 1.3030, 'learning_rate': 2.8469e-05, 'epoch': 1.37, 'throughput': 1450.91}
[INFO|2026-01-07 18:15:38] logging.py:143 >> {'loss': 1.2999, 'learning_rate': 2.8368e-05, 'epoch': 1.37, 'throughput': 1450.70}
[INFO|2026-01-07 18:15:56] logging.py:143 >> {'loss': 1.5275, 'learning_rate': 2.8268e-05, 'epoch': 1.38, 'throughput': 1450.66}
[INFO|2026-01-07 18:16:17] logging.py:143 >> {'loss': 1.1632, 'learning_rate': 2.8168e-05, 'epoch': 1.38, 'throughput': 1451.02}
[INFO|2026-01-07 18:16:33] logging.py:143 >> {'loss': 1.4146, 'learning_rate': 2.8067e-05, 'epoch': 1.38, 'throughput': 1450.82}
[INFO|2026-01-07 18:16:51] logging.py:143 >> {'loss': 1.9691, 'learning_rate': 2.7967e-05, 'epoch': 1.39, 'throughput': 1450.83}
[INFO|2026-01-07 18:17:10] logging.py:143 >> {'loss': 1.2218, 'learning_rate': 2.7867e-05, 'epoch': 1.39, 'throughput': 1450.93}
[INFO|2026-01-07 18:17:10] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1800
[INFO|2026-01-07 18:17:10] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:17:10] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:17:10] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1800\chat_template.jinja
[INFO|2026-01-07 18:17:10] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1800\tokenizer_config.json
[INFO|2026-01-07 18:17:10] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1800\special_tokens_map.json
[INFO|2026-01-07 18:17:28] logging.py:143 >> {'loss': 1.5300, 'learning_rate': 2.7766e-05, 'epoch': 1.39, 'throughput': 1450.89}
[INFO|2026-01-07 18:17:46] logging.py:143 >> {'loss': 1.4898, 'learning_rate': 2.7666e-05, 'epoch': 1.40, 'throughput': 1450.93}
[INFO|2026-01-07 18:18:06] logging.py:143 >> {'loss': 1.4737, 'learning_rate': 2.7565e-05, 'epoch': 1.40, 'throughput': 1451.13}
[INFO|2026-01-07 18:18:25] logging.py:143 >> {'loss': 1.3772, 'learning_rate': 2.7464e-05, 'epoch': 1.41, 'throughput': 1451.34}
[INFO|2026-01-07 18:18:42] logging.py:143 >> {'loss': 1.2772, 'learning_rate': 2.7364e-05, 'epoch': 1.41, 'throughput': 1451.25}
[INFO|2026-01-07 18:18:57] logging.py:143 >> {'loss': 1.7376, 'learning_rate': 2.7263e-05, 'epoch': 1.41, 'throughput': 1450.85}
[INFO|2026-01-07 18:19:14] logging.py:143 >> {'loss': 1.4622, 'learning_rate': 2.7162e-05, 'epoch': 1.42, 'throughput': 1450.82}
[INFO|2026-01-07 18:19:34] logging.py:143 >> {'loss': 1.2762, 'learning_rate': 2.7061e-05, 'epoch': 1.42, 'throughput': 1451.08}
[INFO|2026-01-07 18:19:53] logging.py:143 >> {'loss': 1.8664, 'learning_rate': 2.6960e-05, 'epoch': 1.43, 'throughput': 1451.17}
[INFO|2026-01-07 18:20:10] logging.py:143 >> {'loss': 1.3095, 'learning_rate': 2.6860e-05, 'epoch': 1.43, 'throughput': 1450.95}
[INFO|2026-01-07 18:20:26] logging.py:143 >> {'loss': 1.7176, 'learning_rate': 2.6759e-05, 'epoch': 1.43, 'throughput': 1450.83}
[INFO|2026-01-07 18:20:43] logging.py:143 >> {'loss': 1.3838, 'learning_rate': 2.6658e-05, 'epoch': 1.44, 'throughput': 1450.68}
[INFO|2026-01-07 18:20:59] logging.py:143 >> {'loss': 1.5296, 'learning_rate': 2.6557e-05, 'epoch': 1.44, 'throughput': 1450.58}
[INFO|2026-01-07 18:21:17] logging.py:143 >> {'loss': 1.2823, 'learning_rate': 2.6456e-05, 'epoch': 1.45, 'throughput': 1450.64}
[INFO|2026-01-07 18:21:35] logging.py:143 >> {'loss': 1.3908, 'learning_rate': 2.6355e-05, 'epoch': 1.45, 'throughput': 1450.69}
[INFO|2026-01-07 18:21:56] logging.py:143 >> {'loss': 1.3776, 'learning_rate': 2.6254e-05, 'epoch': 1.45, 'throughput': 1450.99}
[INFO|2026-01-07 18:22:14] logging.py:143 >> {'loss': 1.3326, 'learning_rate': 2.6153e-05, 'epoch': 1.46, 'throughput': 1450.99}
[INFO|2026-01-07 18:22:33] logging.py:143 >> {'loss': 1.6980, 'learning_rate': 2.6052e-05, 'epoch': 1.46, 'throughput': 1451.06}
[INFO|2026-01-07 18:22:49] logging.py:143 >> {'loss': 1.2683, 'learning_rate': 2.5951e-05, 'epoch': 1.46, 'throughput': 1450.98}
[INFO|2026-01-07 18:23:06] logging.py:143 >> {'loss': 1.7053, 'learning_rate': 2.5850e-05, 'epoch': 1.47, 'throughput': 1451.00}
[INFO|2026-01-07 18:23:06] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1900
[INFO|2026-01-07 18:23:06] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:23:06] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:23:06] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1900\chat_template.jinja
[INFO|2026-01-07 18:23:06] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1900\tokenizer_config.json
[INFO|2026-01-07 18:23:06] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-1900\special_tokens_map.json
[INFO|2026-01-07 18:23:23] logging.py:143 >> {'loss': 1.3401, 'learning_rate': 2.5748e-05, 'epoch': 1.47, 'throughput': 1450.75}
[INFO|2026-01-07 18:23:40] logging.py:143 >> {'loss': 1.2983, 'learning_rate': 2.5647e-05, 'epoch': 1.48, 'throughput': 1450.73}
[INFO|2026-01-07 18:23:58] logging.py:143 >> {'loss': 1.3875, 'learning_rate': 2.5546e-05, 'epoch': 1.48, 'throughput': 1450.67}
[INFO|2026-01-07 18:24:16] logging.py:143 >> {'loss': 1.3611, 'learning_rate': 2.5445e-05, 'epoch': 1.48, 'throughput': 1450.68}
[INFO|2026-01-07 18:24:33] logging.py:143 >> {'loss': 1.0714, 'learning_rate': 2.5344e-05, 'epoch': 1.49, 'throughput': 1450.72}
[INFO|2026-01-07 18:24:50] logging.py:143 >> {'loss': 1.3881, 'learning_rate': 2.5243e-05, 'epoch': 1.49, 'throughput': 1450.65}
[INFO|2026-01-07 18:25:08] logging.py:143 >> {'loss': 1.4208, 'learning_rate': 2.5142e-05, 'epoch': 1.50, 'throughput': 1450.66}
[INFO|2026-01-07 18:25:25] logging.py:143 >> {'loss': 1.5290, 'learning_rate': 2.5040e-05, 'epoch': 1.50, 'throughput': 1450.57}
[INFO|2026-01-07 18:25:41] logging.py:143 >> {'loss': 1.6702, 'learning_rate': 2.4939e-05, 'epoch': 1.50, 'throughput': 1450.43}
[INFO|2026-01-07 18:26:00] logging.py:143 >> {'loss': 1.3165, 'learning_rate': 2.4838e-05, 'epoch': 1.51, 'throughput': 1450.47}
[INFO|2026-01-07 18:26:21] logging.py:143 >> {'loss': 1.4199, 'learning_rate': 2.4737e-05, 'epoch': 1.51, 'throughput': 1450.57}
[INFO|2026-01-07 18:26:40] logging.py:143 >> {'loss': 1.3962, 'learning_rate': 2.4636e-05, 'epoch': 1.51, 'throughput': 1450.63}
[INFO|2026-01-07 18:26:57] logging.py:143 >> {'loss': 1.4891, 'learning_rate': 2.4535e-05, 'epoch': 1.52, 'throughput': 1450.53}
[INFO|2026-01-07 18:27:15] logging.py:143 >> {'loss': 1.2145, 'learning_rate': 2.4434e-05, 'epoch': 1.52, 'throughput': 1450.62}
[INFO|2026-01-07 18:27:34] logging.py:143 >> {'loss': 1.5111, 'learning_rate': 2.4332e-05, 'epoch': 1.53, 'throughput': 1450.79}
[INFO|2026-01-07 18:27:52] logging.py:143 >> {'loss': 1.2455, 'learning_rate': 2.4231e-05, 'epoch': 1.53, 'throughput': 1450.88}
[INFO|2026-01-07 18:28:09] logging.py:143 >> {'loss': 1.1340, 'learning_rate': 2.4130e-05, 'epoch': 1.53, 'throughput': 1450.88}
[INFO|2026-01-07 18:28:28] logging.py:143 >> {'loss': 1.2262, 'learning_rate': 2.4029e-05, 'epoch': 1.54, 'throughput': 1451.04}
[INFO|2026-01-07 18:28:44] logging.py:143 >> {'loss': 1.3136, 'learning_rate': 2.3928e-05, 'epoch': 1.54, 'throughput': 1450.88}
[INFO|2026-01-07 18:29:05] logging.py:143 >> {'loss': 1.3059, 'learning_rate': 2.3827e-05, 'epoch': 1.55, 'throughput': 1451.07}
[INFO|2026-01-07 18:29:05] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2000
[INFO|2026-01-07 18:29:05] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:29:05] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:29:05] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2000\chat_template.jinja
[INFO|2026-01-07 18:29:05] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2000\tokenizer_config.json
[INFO|2026-01-07 18:29:05] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2000\special_tokens_map.json
[INFO|2026-01-07 18:29:23] logging.py:143 >> {'loss': 1.3371, 'learning_rate': 2.3726e-05, 'epoch': 1.55, 'throughput': 1451.01}
[INFO|2026-01-07 18:29:41] logging.py:143 >> {'loss': 1.2342, 'learning_rate': 2.3625e-05, 'epoch': 1.55, 'throughput': 1451.05}
[INFO|2026-01-07 18:30:00] logging.py:143 >> {'loss': 1.3181, 'learning_rate': 2.3524e-05, 'epoch': 1.56, 'throughput': 1451.18}
[INFO|2026-01-07 18:30:16] logging.py:143 >> {'loss': 1.5931, 'learning_rate': 2.3423e-05, 'epoch': 1.56, 'throughput': 1450.94}
[INFO|2026-01-07 18:30:33] logging.py:143 >> {'loss': 1.3224, 'learning_rate': 2.3322e-05, 'epoch': 1.57, 'throughput': 1450.94}
[INFO|2026-01-07 18:30:51] logging.py:143 >> {'loss': 1.4362, 'learning_rate': 2.3221e-05, 'epoch': 1.57, 'throughput': 1450.91}
[INFO|2026-01-07 18:31:10] logging.py:143 >> {'loss': 1.3769, 'learning_rate': 2.3120e-05, 'epoch': 1.57, 'throughput': 1451.11}
[INFO|2026-01-07 18:31:25] logging.py:143 >> {'loss': 1.5298, 'learning_rate': 2.3019e-05, 'epoch': 1.58, 'throughput': 1450.92}
[INFO|2026-01-07 18:31:46] logging.py:143 >> {'loss': 1.2528, 'learning_rate': 2.2919e-05, 'epoch': 1.58, 'throughput': 1451.17}
[INFO|2026-01-07 18:32:03] logging.py:143 >> {'loss': 1.6557, 'learning_rate': 2.2818e-05, 'epoch': 1.58, 'throughput': 1451.02}
[INFO|2026-01-07 18:32:21] logging.py:143 >> {'loss': 1.7033, 'learning_rate': 2.2717e-05, 'epoch': 1.59, 'throughput': 1451.01}
[INFO|2026-01-07 18:32:40] logging.py:143 >> {'loss': 1.3461, 'learning_rate': 2.2616e-05, 'epoch': 1.59, 'throughput': 1451.12}
[INFO|2026-01-07 18:32:59] logging.py:143 >> {'loss': 1.2664, 'learning_rate': 2.2516e-05, 'epoch': 1.60, 'throughput': 1451.24}
[INFO|2026-01-07 18:33:16] logging.py:143 >> {'loss': 1.2273, 'learning_rate': 2.2415e-05, 'epoch': 1.60, 'throughput': 1451.15}
[INFO|2026-01-07 18:33:37] logging.py:143 >> {'loss': 1.2209, 'learning_rate': 2.2314e-05, 'epoch': 1.60, 'throughput': 1451.42}
[INFO|2026-01-07 18:33:53] logging.py:143 >> {'loss': 1.4674, 'learning_rate': 2.2214e-05, 'epoch': 1.61, 'throughput': 1451.31}
[INFO|2026-01-07 18:34:11] logging.py:143 >> {'loss': 1.5437, 'learning_rate': 2.2113e-05, 'epoch': 1.61, 'throughput': 1451.34}
[INFO|2026-01-07 18:34:27] logging.py:143 >> {'loss': 1.3924, 'learning_rate': 2.2013e-05, 'epoch': 1.62, 'throughput': 1451.20}
[INFO|2026-01-07 18:34:44] logging.py:143 >> {'loss': 1.6901, 'learning_rate': 2.1912e-05, 'epoch': 1.62, 'throughput': 1451.12}
[INFO|2026-01-07 18:35:03] logging.py:143 >> {'loss': 1.2072, 'learning_rate': 2.1812e-05, 'epoch': 1.62, 'throughput': 1451.28}
[INFO|2026-01-07 18:35:03] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2100
[INFO|2026-01-07 18:35:03] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:35:03] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:35:03] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2100\chat_template.jinja
[INFO|2026-01-07 18:35:03] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2100\tokenizer_config.json
[INFO|2026-01-07 18:35:03] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2100\special_tokens_map.json
[INFO|2026-01-07 18:35:21] logging.py:143 >> {'loss': 1.5331, 'learning_rate': 2.1712e-05, 'epoch': 1.63, 'throughput': 1451.17}
[INFO|2026-01-07 18:35:38] logging.py:143 >> {'loss': 1.5809, 'learning_rate': 2.1612e-05, 'epoch': 1.63, 'throughput': 1451.07}
[INFO|2026-01-07 18:35:55] logging.py:143 >> {'loss': 1.4709, 'learning_rate': 2.1511e-05, 'epoch': 1.63, 'throughput': 1450.93}
[INFO|2026-01-07 18:36:12] logging.py:143 >> {'loss': 1.1769, 'learning_rate': 2.1411e-05, 'epoch': 1.64, 'throughput': 1450.87}
[INFO|2026-01-07 18:36:27] logging.py:143 >> {'loss': 1.1903, 'learning_rate': 2.1311e-05, 'epoch': 1.64, 'throughput': 1450.72}
[INFO|2026-01-07 18:36:42] logging.py:143 >> {'loss': 1.3069, 'learning_rate': 2.1211e-05, 'epoch': 1.65, 'throughput': 1450.56}
[INFO|2026-01-07 18:37:04] logging.py:143 >> {'loss': 1.4004, 'learning_rate': 2.1111e-05, 'epoch': 1.65, 'throughput': 1450.89}
[INFO|2026-01-07 18:37:22] logging.py:143 >> {'loss': 1.2588, 'learning_rate': 2.1011e-05, 'epoch': 1.65, 'throughput': 1450.92}
[INFO|2026-01-07 18:37:41] logging.py:143 >> {'loss': 1.4694, 'learning_rate': 2.0911e-05, 'epoch': 1.66, 'throughput': 1451.07}
[INFO|2026-01-07 18:37:59] logging.py:143 >> {'loss': 1.1039, 'learning_rate': 2.0812e-05, 'epoch': 1.66, 'throughput': 1451.04}
[INFO|2026-01-07 18:38:16] logging.py:143 >> {'loss': 1.4808, 'learning_rate': 2.0712e-05, 'epoch': 1.67, 'throughput': 1451.05}
[INFO|2026-01-07 18:38:37] logging.py:143 >> {'loss': 1.4019, 'learning_rate': 2.0612e-05, 'epoch': 1.67, 'throughput': 1451.26}
[INFO|2026-01-07 18:38:54] logging.py:143 >> {'loss': 1.5457, 'learning_rate': 2.0513e-05, 'epoch': 1.67, 'throughput': 1451.21}
[INFO|2026-01-07 18:39:11] logging.py:143 >> {'loss': 1.5090, 'learning_rate': 2.0413e-05, 'epoch': 1.68, 'throughput': 1451.23}
[INFO|2026-01-07 18:39:28] logging.py:143 >> {'loss': 1.1792, 'learning_rate': 2.0314e-05, 'epoch': 1.68, 'throughput': 1451.28}
[INFO|2026-01-07 18:39:45] logging.py:143 >> {'loss': 1.4622, 'learning_rate': 2.0215e-05, 'epoch': 1.68, 'throughput': 1451.25}
[INFO|2026-01-07 18:40:02] logging.py:143 >> {'loss': 1.5572, 'learning_rate': 2.0115e-05, 'epoch': 1.69, 'throughput': 1451.23}
[INFO|2026-01-07 18:40:21] logging.py:143 >> {'loss': 1.3833, 'learning_rate': 2.0016e-05, 'epoch': 1.69, 'throughput': 1451.36}
[INFO|2026-01-07 18:40:39] logging.py:143 >> {'loss': 1.2820, 'learning_rate': 1.9917e-05, 'epoch': 1.70, 'throughput': 1451.44}
[INFO|2026-01-07 18:40:57] logging.py:143 >> {'loss': 1.4474, 'learning_rate': 1.9818e-05, 'epoch': 1.70, 'throughput': 1451.51}
[INFO|2026-01-07 18:40:57] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2200
[INFO|2026-01-07 18:40:57] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:40:57] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:40:57] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2200\chat_template.jinja
[INFO|2026-01-07 18:40:57] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2200\tokenizer_config.json
[INFO|2026-01-07 18:40:57] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2200\special_tokens_map.json
[INFO|2026-01-07 18:41:12] logging.py:143 >> {'loss': 1.5611, 'learning_rate': 1.9719e-05, 'epoch': 1.70, 'throughput': 1451.06}
[INFO|2026-01-07 18:41:32] logging.py:143 >> {'loss': 1.0037, 'learning_rate': 1.9620e-05, 'epoch': 1.71, 'throughput': 1451.28}
[INFO|2026-01-07 18:41:49] logging.py:143 >> {'loss': 1.6413, 'learning_rate': 1.9522e-05, 'epoch': 1.71, 'throughput': 1451.25}
[INFO|2026-01-07 18:42:07] logging.py:143 >> {'loss': 1.4640, 'learning_rate': 1.9423e-05, 'epoch': 1.72, 'throughput': 1451.27}
[INFO|2026-01-07 18:42:23] logging.py:143 >> {'loss': 1.1175, 'learning_rate': 1.9324e-05, 'epoch': 1.72, 'throughput': 1451.12}
[INFO|2026-01-07 18:42:41] logging.py:143 >> {'loss': 1.5430, 'learning_rate': 1.9226e-05, 'epoch': 1.72, 'throughput': 1451.10}
[INFO|2026-01-07 18:42:57] logging.py:143 >> {'loss': 1.2381, 'learning_rate': 1.9127e-05, 'epoch': 1.73, 'throughput': 1451.07}
[INFO|2026-01-07 18:43:15] logging.py:143 >> {'loss': 1.4445, 'learning_rate': 1.9029e-05, 'epoch': 1.73, 'throughput': 1451.06}
[INFO|2026-01-07 18:43:30] logging.py:143 >> {'loss': 1.6538, 'learning_rate': 1.8931e-05, 'epoch': 1.74, 'throughput': 1450.92}
[INFO|2026-01-07 18:43:48] logging.py:143 >> {'loss': 1.7470, 'learning_rate': 1.8833e-05, 'epoch': 1.74, 'throughput': 1450.92}
[INFO|2026-01-07 18:44:09] logging.py:143 >> {'loss': 1.3853, 'learning_rate': 1.8735e-05, 'epoch': 1.74, 'throughput': 1451.23}
[INFO|2026-01-07 18:44:26] logging.py:143 >> {'loss': 1.2471, 'learning_rate': 1.8637e-05, 'epoch': 1.75, 'throughput': 1451.23}
[INFO|2026-01-07 18:44:44] logging.py:143 >> {'loss': 1.4670, 'learning_rate': 1.8539e-05, 'epoch': 1.75, 'throughput': 1451.32}
[INFO|2026-01-07 18:45:00] logging.py:143 >> {'loss': 1.6170, 'learning_rate': 1.8442e-05, 'epoch': 1.75, 'throughput': 1451.08}
[INFO|2026-01-07 18:45:17] logging.py:143 >> {'loss': 1.2559, 'learning_rate': 1.8344e-05, 'epoch': 1.76, 'throughput': 1451.13}
[INFO|2026-01-07 18:45:36] logging.py:143 >> {'loss': 1.3904, 'learning_rate': 1.8247e-05, 'epoch': 1.76, 'throughput': 1451.27}
[INFO|2026-01-07 18:45:53] logging.py:143 >> {'loss': 1.4000, 'learning_rate': 1.8149e-05, 'epoch': 1.77, 'throughput': 1451.24}
[INFO|2026-01-07 18:46:10] logging.py:143 >> {'loss': 1.7415, 'learning_rate': 1.8052e-05, 'epoch': 1.77, 'throughput': 1451.16}
[INFO|2026-01-07 18:46:28] logging.py:143 >> {'loss': 1.4842, 'learning_rate': 1.7955e-05, 'epoch': 1.77, 'throughput': 1451.34}
[INFO|2026-01-07 18:46:45] logging.py:143 >> {'loss': 1.5090, 'learning_rate': 1.7858e-05, 'epoch': 1.78, 'throughput': 1451.29}
[INFO|2026-01-07 18:46:45] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2300
[INFO|2026-01-07 18:46:45] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:46:45] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:46:45] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2300\chat_template.jinja
[INFO|2026-01-07 18:46:45] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2300\tokenizer_config.json
[INFO|2026-01-07 18:46:45] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2300\special_tokens_map.json
[INFO|2026-01-07 18:47:03] logging.py:143 >> {'loss': 1.3782, 'learning_rate': 1.7761e-05, 'epoch': 1.78, 'throughput': 1451.19}
[INFO|2026-01-07 18:47:21] logging.py:143 >> {'loss': 1.4122, 'learning_rate': 1.7664e-05, 'epoch': 1.79, 'throughput': 1451.11}
[INFO|2026-01-07 18:47:38] logging.py:143 >> {'loss': 1.3858, 'learning_rate': 1.7568e-05, 'epoch': 1.79, 'throughput': 1451.13}
[INFO|2026-01-07 18:47:55] logging.py:143 >> {'loss': 1.3837, 'learning_rate': 1.7471e-05, 'epoch': 1.79, 'throughput': 1451.16}
[INFO|2026-01-07 18:48:12] logging.py:143 >> {'loss': 1.4493, 'learning_rate': 1.7375e-05, 'epoch': 1.80, 'throughput': 1451.01}
[INFO|2026-01-07 18:48:29] logging.py:143 >> {'loss': 1.4858, 'learning_rate': 1.7278e-05, 'epoch': 1.80, 'throughput': 1450.98}
[INFO|2026-01-07 18:48:51] logging.py:143 >> {'loss': 1.2606, 'learning_rate': 1.7182e-05, 'epoch': 1.80, 'throughput': 1451.22}
[INFO|2026-01-07 18:49:10] logging.py:143 >> {'loss': 1.3690, 'learning_rate': 1.7086e-05, 'epoch': 1.81, 'throughput': 1451.40}
[INFO|2026-01-07 18:49:28] logging.py:143 >> {'loss': 1.5609, 'learning_rate': 1.6990e-05, 'epoch': 1.81, 'throughput': 1451.46}
[INFO|2026-01-07 18:49:46] logging.py:143 >> {'loss': 1.6882, 'learning_rate': 1.6895e-05, 'epoch': 1.82, 'throughput': 1451.43}
[INFO|2026-01-07 18:50:04] logging.py:143 >> {'loss': 1.1954, 'learning_rate': 1.6799e-05, 'epoch': 1.82, 'throughput': 1451.51}
[INFO|2026-01-07 18:50:22] logging.py:143 >> {'loss': 1.3670, 'learning_rate': 1.6703e-05, 'epoch': 1.82, 'throughput': 1451.55}
[INFO|2026-01-07 18:50:39] logging.py:143 >> {'loss': 1.6971, 'learning_rate': 1.6608e-05, 'epoch': 1.83, 'throughput': 1451.58}
[INFO|2026-01-07 18:50:55] logging.py:143 >> {'loss': 1.6900, 'learning_rate': 1.6513e-05, 'epoch': 1.83, 'throughput': 1451.43}
[INFO|2026-01-07 18:51:11] logging.py:143 >> {'loss': 1.5234, 'learning_rate': 1.6418e-05, 'epoch': 1.84, 'throughput': 1451.21}
[INFO|2026-01-07 18:51:30] logging.py:143 >> {'loss': 1.6226, 'learning_rate': 1.6323e-05, 'epoch': 1.84, 'throughput': 1451.39}
[INFO|2026-01-07 18:51:49] logging.py:143 >> {'loss': 1.4584, 'learning_rate': 1.6228e-05, 'epoch': 1.84, 'throughput': 1451.52}
[INFO|2026-01-07 18:52:05] logging.py:143 >> {'loss': 1.2512, 'learning_rate': 1.6133e-05, 'epoch': 1.85, 'throughput': 1451.42}
[INFO|2026-01-07 18:52:21] logging.py:143 >> {'loss': 1.3482, 'learning_rate': 1.6039e-05, 'epoch': 1.85, 'throughput': 1451.32}
[INFO|2026-01-07 18:52:41] logging.py:143 >> {'loss': 1.5472, 'learning_rate': 1.5945e-05, 'epoch': 1.85, 'throughput': 1451.50}
[INFO|2026-01-07 18:52:41] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2400
[INFO|2026-01-07 18:52:41] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:52:41] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:52:41] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2400\chat_template.jinja
[INFO|2026-01-07 18:52:41] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2400\tokenizer_config.json
[INFO|2026-01-07 18:52:41] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2400\special_tokens_map.json
[INFO|2026-01-07 18:52:58] logging.py:143 >> {'loss': 1.4409, 'learning_rate': 1.5850e-05, 'epoch': 1.86, 'throughput': 1451.33}
[INFO|2026-01-07 18:53:13] logging.py:143 >> {'loss': 2.0087, 'learning_rate': 1.5756e-05, 'epoch': 1.86, 'throughput': 1451.07}
[INFO|2026-01-07 18:53:29] logging.py:143 >> {'loss': 1.3748, 'learning_rate': 1.5662e-05, 'epoch': 1.87, 'throughput': 1451.00}
[INFO|2026-01-07 18:53:47] logging.py:143 >> {'loss': 0.9193, 'learning_rate': 1.5569e-05, 'epoch': 1.87, 'throughput': 1451.11}
[INFO|2026-01-07 18:54:02] logging.py:143 >> {'loss': 1.5019, 'learning_rate': 1.5475e-05, 'epoch': 1.87, 'throughput': 1450.83}
[INFO|2026-01-07 18:54:22] logging.py:143 >> {'loss': 1.4449, 'learning_rate': 1.5382e-05, 'epoch': 1.88, 'throughput': 1451.03}
[INFO|2026-01-07 18:54:41] logging.py:143 >> {'loss': 1.5826, 'learning_rate': 1.5288e-05, 'epoch': 1.88, 'throughput': 1451.09}
[INFO|2026-01-07 18:54:57] logging.py:143 >> {'loss': 1.2667, 'learning_rate': 1.5195e-05, 'epoch': 1.89, 'throughput': 1451.04}
[INFO|2026-01-07 18:55:15] logging.py:143 >> {'loss': 1.1940, 'learning_rate': 1.5102e-05, 'epoch': 1.89, 'throughput': 1451.13}
[INFO|2026-01-07 18:55:32] logging.py:143 >> {'loss': 1.5605, 'learning_rate': 1.5009e-05, 'epoch': 1.89, 'throughput': 1451.05}
[INFO|2026-01-07 18:55:49] logging.py:143 >> {'loss': 1.4452, 'learning_rate': 1.4917e-05, 'epoch': 1.90, 'throughput': 1451.09}
[INFO|2026-01-07 18:56:04] logging.py:143 >> {'loss': 1.5105, 'learning_rate': 1.4824e-05, 'epoch': 1.90, 'throughput': 1450.82}
[INFO|2026-01-07 18:56:22] logging.py:143 >> {'loss': 1.4046, 'learning_rate': 1.4732e-05, 'epoch': 1.91, 'throughput': 1450.87}
[INFO|2026-01-07 18:56:39] logging.py:143 >> {'loss': 1.3740, 'learning_rate': 1.4640e-05, 'epoch': 1.91, 'throughput': 1450.89}
[INFO|2026-01-07 18:56:56] logging.py:143 >> {'loss': 1.6734, 'learning_rate': 1.4548e-05, 'epoch': 1.91, 'throughput': 1450.87}
[INFO|2026-01-07 18:57:13] logging.py:143 >> {'loss': 1.4535, 'learning_rate': 1.4456e-05, 'epoch': 1.92, 'throughput': 1450.85}
[INFO|2026-01-07 18:57:32] logging.py:143 >> {'loss': 1.2525, 'learning_rate': 1.4364e-05, 'epoch': 1.92, 'throughput': 1451.00}
[INFO|2026-01-07 18:57:51] logging.py:143 >> {'loss': 1.1322, 'learning_rate': 1.4273e-05, 'epoch': 1.92, 'throughput': 1451.12}
[INFO|2026-01-07 18:58:08] logging.py:143 >> {'loss': 1.6324, 'learning_rate': 1.4182e-05, 'epoch': 1.93, 'throughput': 1451.03}
[INFO|2026-01-07 18:58:23] logging.py:143 >> {'loss': 1.4000, 'learning_rate': 1.4090e-05, 'epoch': 1.93, 'throughput': 1450.85}
[INFO|2026-01-07 18:58:23] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2500
[INFO|2026-01-07 18:58:23] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 18:58:23] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 18:58:24] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2500\chat_template.jinja
[INFO|2026-01-07 18:58:24] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2500\tokenizer_config.json
[INFO|2026-01-07 18:58:24] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2500\special_tokens_map.json
[INFO|2026-01-07 18:58:42] logging.py:143 >> {'loss': 0.9369, 'learning_rate': 1.4000e-05, 'epoch': 1.94, 'throughput': 1450.86}
[INFO|2026-01-07 18:58:59] logging.py:143 >> {'loss': 1.5329, 'learning_rate': 1.3909e-05, 'epoch': 1.94, 'throughput': 1450.85}
[INFO|2026-01-07 18:59:15] logging.py:143 >> {'loss': 1.5081, 'learning_rate': 1.3818e-05, 'epoch': 1.94, 'throughput': 1450.73}
[INFO|2026-01-07 18:59:34] logging.py:143 >> {'loss': 1.1339, 'learning_rate': 1.3728e-05, 'epoch': 1.95, 'throughput': 1450.88}
[INFO|2026-01-07 18:59:50] logging.py:143 >> {'loss': 1.7893, 'learning_rate': 1.3638e-05, 'epoch': 1.95, 'throughput': 1450.88}
[INFO|2026-01-07 19:00:10] logging.py:143 >> {'loss': 1.3891, 'learning_rate': 1.3548e-05, 'epoch': 1.96, 'throughput': 1450.98}
[INFO|2026-01-07 19:00:25] logging.py:143 >> {'loss': 1.5153, 'learning_rate': 1.3458e-05, 'epoch': 1.96, 'throughput': 1450.82}
[INFO|2026-01-07 19:00:42] logging.py:143 >> {'loss': 1.3991, 'learning_rate': 1.3368e-05, 'epoch': 1.96, 'throughput': 1450.90}
[INFO|2026-01-07 19:01:00] logging.py:143 >> {'loss': 1.3908, 'learning_rate': 1.3279e-05, 'epoch': 1.97, 'throughput': 1450.91}
[INFO|2026-01-07 19:01:16] logging.py:143 >> {'loss': 1.5141, 'learning_rate': 1.3189e-05, 'epoch': 1.97, 'throughput': 1450.82}
[INFO|2026-01-07 19:01:34] logging.py:143 >> {'loss': 1.3608, 'learning_rate': 1.3100e-05, 'epoch': 1.97, 'throughput': 1450.86}
[INFO|2026-01-07 19:01:53] logging.py:143 >> {'loss': 1.4235, 'learning_rate': 1.3012e-05, 'epoch': 1.98, 'throughput': 1450.99}
[INFO|2026-01-07 19:02:13] logging.py:143 >> {'loss': 1.1488, 'learning_rate': 1.2923e-05, 'epoch': 1.98, 'throughput': 1451.12}
[INFO|2026-01-07 19:02:30] logging.py:143 >> {'loss': 1.5917, 'learning_rate': 1.2834e-05, 'epoch': 1.99, 'throughput': 1451.09}
[INFO|2026-01-07 19:02:48] logging.py:143 >> {'loss': 1.4297, 'learning_rate': 1.2746e-05, 'epoch': 1.99, 'throughput': 1451.06}
[INFO|2026-01-07 19:03:05] logging.py:143 >> {'loss': 1.5953, 'learning_rate': 1.2658e-05, 'epoch': 1.99, 'throughput': 1451.08}
[INFO|2026-01-07 19:03:24] logging.py:143 >> {'loss': 1.6893, 'learning_rate': 1.2570e-05, 'epoch': 2.00, 'throughput': 1451.25}
[INFO|2026-01-07 19:03:40] logging.py:143 >> {'loss': 1.1388, 'learning_rate': 1.2482e-05, 'epoch': 2.00, 'throughput': 1451.20}
[INFO|2026-01-07 19:03:59] logging.py:143 >> {'loss': 1.6229, 'learning_rate': 1.2395e-05, 'epoch': 2.01, 'throughput': 1451.20}
[INFO|2026-01-07 19:04:16] logging.py:143 >> {'loss': 1.4812, 'learning_rate': 1.2308e-05, 'epoch': 2.01, 'throughput': 1451.21}
[INFO|2026-01-07 19:04:16] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2600
[INFO|2026-01-07 19:04:16] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:04:16] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:04:16] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2600\chat_template.jinja
[INFO|2026-01-07 19:04:16] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2600\tokenizer_config.json
[INFO|2026-01-07 19:04:16] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2600\special_tokens_map.json
[INFO|2026-01-07 19:04:35] logging.py:143 >> {'loss': 1.3148, 'learning_rate': 1.2221e-05, 'epoch': 2.01, 'throughput': 1451.11}
[INFO|2026-01-07 19:04:50] logging.py:143 >> {'loss': 1.4556, 'learning_rate': 1.2134e-05, 'epoch': 2.02, 'throughput': 1451.01}
[INFO|2026-01-07 19:05:07] logging.py:143 >> {'loss': 1.3945, 'learning_rate': 1.2047e-05, 'epoch': 2.02, 'throughput': 1451.00}
[INFO|2026-01-07 19:05:23] logging.py:143 >> {'loss': 1.4171, 'learning_rate': 1.1961e-05, 'epoch': 2.02, 'throughput': 1450.88}
[INFO|2026-01-07 19:05:41] logging.py:143 >> {'loss': 1.2888, 'learning_rate': 1.1875e-05, 'epoch': 2.03, 'throughput': 1451.00}
[INFO|2026-01-07 19:05:59] logging.py:143 >> {'loss': 1.5793, 'learning_rate': 1.1789e-05, 'epoch': 2.03, 'throughput': 1450.89}
[INFO|2026-01-07 19:06:15] logging.py:143 >> {'loss': 1.1625, 'learning_rate': 1.1703e-05, 'epoch': 2.04, 'throughput': 1450.79}
[INFO|2026-01-07 19:06:31] logging.py:143 >> {'loss': 1.1903, 'learning_rate': 1.1617e-05, 'epoch': 2.04, 'throughput': 1450.61}
[INFO|2026-01-07 19:06:51] logging.py:143 >> {'loss': 1.4336, 'learning_rate': 1.1532e-05, 'epoch': 2.04, 'throughput': 1450.77}
[INFO|2026-01-07 19:07:07] logging.py:143 >> {'loss': 1.5459, 'learning_rate': 1.1447e-05, 'epoch': 2.05, 'throughput': 1450.74}
[INFO|2026-01-07 19:07:24] logging.py:143 >> {'loss': 1.3768, 'learning_rate': 1.1362e-05, 'epoch': 2.05, 'throughput': 1450.66}
[INFO|2026-01-07 19:07:40] logging.py:143 >> {'loss': 1.8453, 'learning_rate': 1.1277e-05, 'epoch': 2.06, 'throughput': 1450.50}
[INFO|2026-01-07 19:07:55] logging.py:143 >> {'loss': 1.1710, 'learning_rate': 1.1193e-05, 'epoch': 2.06, 'throughput': 1450.42}
[INFO|2026-01-07 19:08:14] logging.py:143 >> {'loss': 1.2196, 'learning_rate': 1.1109e-05, 'epoch': 2.06, 'throughput': 1450.51}
[INFO|2026-01-07 19:08:29] logging.py:143 >> {'loss': 1.4440, 'learning_rate': 1.1025e-05, 'epoch': 2.07, 'throughput': 1450.30}
[INFO|2026-01-07 19:08:47] logging.py:143 >> {'loss': 1.2429, 'learning_rate': 1.0941e-05, 'epoch': 2.07, 'throughput': 1450.32}
[INFO|2026-01-07 19:09:05] logging.py:143 >> {'loss': 1.3682, 'learning_rate': 1.0857e-05, 'epoch': 2.07, 'throughput': 1450.39}
[INFO|2026-01-07 19:09:21] logging.py:143 >> {'loss': 1.7065, 'learning_rate': 1.0774e-05, 'epoch': 2.08, 'throughput': 1450.28}
[INFO|2026-01-07 19:09:38] logging.py:143 >> {'loss': 1.4214, 'learning_rate': 1.0691e-05, 'epoch': 2.08, 'throughput': 1450.31}
[INFO|2026-01-07 19:09:57] logging.py:143 >> {'loss': 1.4776, 'learning_rate': 1.0608e-05, 'epoch': 2.09, 'throughput': 1450.38}
[INFO|2026-01-07 19:09:57] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2700
[INFO|2026-01-07 19:09:57] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:09:57] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:09:57] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2700\chat_template.jinja
[INFO|2026-01-07 19:09:57] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2700\tokenizer_config.json
[INFO|2026-01-07 19:09:57] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2700\special_tokens_map.json
[INFO|2026-01-07 19:10:16] logging.py:143 >> {'loss': 1.4688, 'learning_rate': 1.0526e-05, 'epoch': 2.09, 'throughput': 1450.47}
[INFO|2026-01-07 19:10:36] logging.py:143 >> {'loss': 1.3952, 'learning_rate': 1.0443e-05, 'epoch': 2.09, 'throughput': 1450.58}
[INFO|2026-01-07 19:10:57] logging.py:143 >> {'loss': 1.2378, 'learning_rate': 1.0361e-05, 'epoch': 2.10, 'throughput': 1450.78}
[INFO|2026-01-07 19:11:15] logging.py:143 >> {'loss': 1.2043, 'learning_rate': 1.0279e-05, 'epoch': 2.10, 'throughput': 1450.86}
[INFO|2026-01-07 19:11:33] logging.py:143 >> {'loss': 0.9927, 'learning_rate': 1.0198e-05, 'epoch': 2.11, 'throughput': 1450.91}
[INFO|2026-01-07 19:11:49] logging.py:143 >> {'loss': 1.5590, 'learning_rate': 1.0116e-05, 'epoch': 2.11, 'throughput': 1450.82}
[INFO|2026-01-07 19:12:06] logging.py:143 >> {'loss': 1.2637, 'learning_rate': 1.0035e-05, 'epoch': 2.11, 'throughput': 1450.81}
[INFO|2026-01-07 19:12:26] logging.py:143 >> {'loss': 1.4644, 'learning_rate': 9.9541e-06, 'epoch': 2.12, 'throughput': 1450.98}
[INFO|2026-01-07 19:12:44] logging.py:143 >> {'loss': 1.3687, 'learning_rate': 9.8734e-06, 'epoch': 2.12, 'throughput': 1450.96}
[INFO|2026-01-07 19:13:03] logging.py:143 >> {'loss': 1.2839, 'learning_rate': 9.7930e-06, 'epoch': 2.13, 'throughput': 1451.06}
[INFO|2026-01-07 19:13:19] logging.py:143 >> {'loss': 1.4339, 'learning_rate': 9.7128e-06, 'epoch': 2.13, 'throughput': 1451.04}
[INFO|2026-01-07 19:13:37] logging.py:143 >> {'loss': 1.5834, 'learning_rate': 9.6329e-06, 'epoch': 2.13, 'throughput': 1451.09}
[INFO|2026-01-07 19:13:55] logging.py:143 >> {'loss': 1.6419, 'learning_rate': 9.5533e-06, 'epoch': 2.14, 'throughput': 1451.09}
[INFO|2026-01-07 19:14:11] logging.py:143 >> {'loss': 1.2678, 'learning_rate': 9.4738e-06, 'epoch': 2.14, 'throughput': 1451.03}
[INFO|2026-01-07 19:14:27] logging.py:143 >> {'loss': 1.5028, 'learning_rate': 9.3947e-06, 'epoch': 2.14, 'throughput': 1450.98}
[INFO|2026-01-07 19:14:45] logging.py:143 >> {'loss': 1.3484, 'learning_rate': 9.3158e-06, 'epoch': 2.15, 'throughput': 1450.97}
[INFO|2026-01-07 19:15:02] logging.py:143 >> {'loss': 1.2797, 'learning_rate': 9.2371e-06, 'epoch': 2.15, 'throughput': 1450.92}
[INFO|2026-01-07 19:15:17] logging.py:143 >> {'loss': 1.3725, 'learning_rate': 9.1587e-06, 'epoch': 2.16, 'throughput': 1450.81}
[INFO|2026-01-07 19:15:36] logging.py:143 >> {'loss': 1.1987, 'learning_rate': 9.0806e-06, 'epoch': 2.16, 'throughput': 1450.92}
[INFO|2026-01-07 19:15:53] logging.py:143 >> {'loss': 1.3865, 'learning_rate': 9.0028e-06, 'epoch': 2.16, 'throughput': 1450.93}
[INFO|2026-01-07 19:15:53] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2800
[INFO|2026-01-07 19:15:53] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:15:53] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:15:53] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2800\chat_template.jinja
[INFO|2026-01-07 19:15:53] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2800\tokenizer_config.json
[INFO|2026-01-07 19:15:53] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2800\special_tokens_map.json
[INFO|2026-01-07 19:16:09] logging.py:143 >> {'loss': 1.4260, 'learning_rate': 8.9251e-06, 'epoch': 2.17, 'throughput': 1450.77}
[INFO|2026-01-07 19:16:26] logging.py:143 >> {'loss': 1.2725, 'learning_rate': 8.8478e-06, 'epoch': 2.17, 'throughput': 1450.73}
[INFO|2026-01-07 19:16:43] logging.py:143 >> {'loss': 1.2224, 'learning_rate': 8.7707e-06, 'epoch': 2.18, 'throughput': 1450.79}
[INFO|2026-01-07 19:16:59] logging.py:143 >> {'loss': 1.6314, 'learning_rate': 8.6939e-06, 'epoch': 2.18, 'throughput': 1450.64}
[INFO|2026-01-07 19:17:16] logging.py:143 >> {'loss': 1.3563, 'learning_rate': 8.6174e-06, 'epoch': 2.18, 'throughput': 1450.65}
[INFO|2026-01-07 19:17:33] logging.py:143 >> {'loss': 1.3378, 'learning_rate': 8.5411e-06, 'epoch': 2.19, 'throughput': 1450.62}
[INFO|2026-01-07 19:17:50] logging.py:143 >> {'loss': 1.3620, 'learning_rate': 8.4651e-06, 'epoch': 2.19, 'throughput': 1450.54}
[INFO|2026-01-07 19:18:07] logging.py:143 >> {'loss': 1.4238, 'learning_rate': 8.3893e-06, 'epoch': 2.19, 'throughput': 1450.52}
[INFO|2026-01-07 19:18:23] logging.py:143 >> {'loss': 1.0313, 'learning_rate': 8.3139e-06, 'epoch': 2.20, 'throughput': 1450.49}
[INFO|2026-01-07 19:18:40] logging.py:143 >> {'loss': 1.5056, 'learning_rate': 8.2387e-06, 'epoch': 2.20, 'throughput': 1450.45}
[INFO|2026-01-07 19:18:57] logging.py:143 >> {'loss': 1.3706, 'learning_rate': 8.1638e-06, 'epoch': 2.21, 'throughput': 1450.47}
[INFO|2026-01-07 19:19:15] logging.py:143 >> {'loss': 1.4897, 'learning_rate': 8.0891e-06, 'epoch': 2.21, 'throughput': 1450.51}
[INFO|2026-01-07 19:19:35] logging.py:143 >> {'loss': 1.3088, 'learning_rate': 8.0148e-06, 'epoch': 2.21, 'throughput': 1450.58}
[INFO|2026-01-07 19:19:51] logging.py:143 >> {'loss': 1.2333, 'learning_rate': 7.9407e-06, 'epoch': 2.22, 'throughput': 1450.53}
[INFO|2026-01-07 19:20:09] logging.py:143 >> {'loss': 1.4086, 'learning_rate': 7.8669e-06, 'epoch': 2.22, 'throughput': 1450.57}
[INFO|2026-01-07 19:20:27] logging.py:143 >> {'loss': 1.4677, 'learning_rate': 7.7933e-06, 'epoch': 2.23, 'throughput': 1450.58}
[INFO|2026-01-07 19:20:44] logging.py:143 >> {'loss': 1.5107, 'learning_rate': 7.7201e-06, 'epoch': 2.23, 'throughput': 1450.59}
[INFO|2026-01-07 19:21:02] logging.py:143 >> {'loss': 1.4752, 'learning_rate': 7.6471e-06, 'epoch': 2.23, 'throughput': 1450.50}
[INFO|2026-01-07 19:21:20] logging.py:143 >> {'loss': 1.8468, 'learning_rate': 7.5745e-06, 'epoch': 2.24, 'throughput': 1450.49}
[INFO|2026-01-07 19:21:37] logging.py:143 >> {'loss': 1.2234, 'learning_rate': 7.5021e-06, 'epoch': 2.24, 'throughput': 1450.45}
[INFO|2026-01-07 19:21:37] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2900
[INFO|2026-01-07 19:21:37] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:21:37] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:21:37] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2900\chat_template.jinja
[INFO|2026-01-07 19:21:37] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2900\tokenizer_config.json
[INFO|2026-01-07 19:21:37] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-2900\special_tokens_map.json
[INFO|2026-01-07 19:21:55] logging.py:143 >> {'loss': 1.3489, 'learning_rate': 7.4300e-06, 'epoch': 2.25, 'throughput': 1450.39}
[INFO|2026-01-07 19:22:13] logging.py:143 >> {'loss': 1.4170, 'learning_rate': 7.3581e-06, 'epoch': 2.25, 'throughput': 1450.44}
[INFO|2026-01-07 19:22:30] logging.py:143 >> {'loss': 1.3985, 'learning_rate': 7.2866e-06, 'epoch': 2.25, 'throughput': 1450.35}
[INFO|2026-01-07 19:22:47] logging.py:143 >> {'loss': 1.1116, 'learning_rate': 7.2154e-06, 'epoch': 2.26, 'throughput': 1450.36}
[INFO|2026-01-07 19:23:04] logging.py:143 >> {'loss': 1.3013, 'learning_rate': 7.1444e-06, 'epoch': 2.26, 'throughput': 1450.35}
[INFO|2026-01-07 19:23:22] logging.py:143 >> {'loss': 1.2343, 'learning_rate': 7.0738e-06, 'epoch': 2.26, 'throughput': 1450.41}
[INFO|2026-01-07 19:23:38] logging.py:143 >> {'loss': 1.3041, 'learning_rate': 7.0034e-06, 'epoch': 2.27, 'throughput': 1450.32}
[INFO|2026-01-07 19:23:56] logging.py:143 >> {'loss': 1.4163, 'learning_rate': 6.9333e-06, 'epoch': 2.27, 'throughput': 1450.33}
[INFO|2026-01-07 19:24:14] logging.py:143 >> {'loss': 1.2169, 'learning_rate': 6.8636e-06, 'epoch': 2.28, 'throughput': 1450.41}
[INFO|2026-01-07 19:24:31] logging.py:143 >> {'loss': 1.1988, 'learning_rate': 6.7941e-06, 'epoch': 2.28, 'throughput': 1450.33}
[INFO|2026-01-07 19:24:49] logging.py:143 >> {'loss': 1.2325, 'learning_rate': 6.7249e-06, 'epoch': 2.28, 'throughput': 1450.38}
[INFO|2026-01-07 19:25:05] logging.py:143 >> {'loss': 1.6341, 'learning_rate': 6.6560e-06, 'epoch': 2.29, 'throughput': 1450.32}
[INFO|2026-01-07 19:25:24] logging.py:143 >> {'loss': 1.2126, 'learning_rate': 6.5874e-06, 'epoch': 2.29, 'throughput': 1450.31}
[INFO|2026-01-07 19:25:43] logging.py:143 >> {'loss': 1.7384, 'learning_rate': 6.5192e-06, 'epoch': 2.30, 'throughput': 1450.44}
[INFO|2026-01-07 19:26:02] logging.py:143 >> {'loss': 1.3859, 'learning_rate': 6.4512e-06, 'epoch': 2.30, 'throughput': 1450.48}
[INFO|2026-01-07 19:26:21] logging.py:143 >> {'loss': 1.3683, 'learning_rate': 6.3835e-06, 'epoch': 2.30, 'throughput': 1450.56}
[INFO|2026-01-07 19:26:39] logging.py:143 >> {'loss': 1.3281, 'learning_rate': 6.3162e-06, 'epoch': 2.31, 'throughput': 1450.55}
[INFO|2026-01-07 19:26:57] logging.py:143 >> {'loss': 1.3734, 'learning_rate': 6.2491e-06, 'epoch': 2.31, 'throughput': 1450.63}
[INFO|2026-01-07 19:27:13] logging.py:143 >> {'loss': 1.2859, 'learning_rate': 6.1823e-06, 'epoch': 2.31, 'throughput': 1450.60}
[INFO|2026-01-07 19:27:29] logging.py:143 >> {'loss': 1.6578, 'learning_rate': 6.1159e-06, 'epoch': 2.32, 'throughput': 1450.46}
[INFO|2026-01-07 19:27:29] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3000
[INFO|2026-01-07 19:27:29] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:27:29] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:27:29] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3000\chat_template.jinja
[INFO|2026-01-07 19:27:29] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3000\tokenizer_config.json
[INFO|2026-01-07 19:27:29] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3000\special_tokens_map.json
[INFO|2026-01-07 19:27:48] logging.py:143 >> {'loss': 1.3501, 'learning_rate': 6.0498e-06, 'epoch': 2.32, 'throughput': 1450.57}
[INFO|2026-01-07 19:28:05] logging.py:143 >> {'loss': 1.2446, 'learning_rate': 5.9839e-06, 'epoch': 2.33, 'throughput': 1450.55}
[INFO|2026-01-07 19:28:23] logging.py:143 >> {'loss': 1.2179, 'learning_rate': 5.9184e-06, 'epoch': 2.33, 'throughput': 1450.60}
[INFO|2026-01-07 19:28:40] logging.py:143 >> {'loss': 1.1941, 'learning_rate': 5.8532e-06, 'epoch': 2.33, 'throughput': 1450.59}
[INFO|2026-01-07 19:28:57] logging.py:143 >> {'loss': 1.4422, 'learning_rate': 5.7883e-06, 'epoch': 2.34, 'throughput': 1450.63}
[INFO|2026-01-07 19:29:14] logging.py:143 >> {'loss': 1.5136, 'learning_rate': 5.7238e-06, 'epoch': 2.34, 'throughput': 1450.63}
[INFO|2026-01-07 19:29:33] logging.py:143 >> {'loss': 1.3902, 'learning_rate': 5.6595e-06, 'epoch': 2.35, 'throughput': 1450.69}
[INFO|2026-01-07 19:29:50] logging.py:143 >> {'loss': 1.3712, 'learning_rate': 5.5956e-06, 'epoch': 2.35, 'throughput': 1450.65}
[INFO|2026-01-07 19:30:09] logging.py:143 >> {'loss': 1.4231, 'learning_rate': 5.5319e-06, 'epoch': 2.35, 'throughput': 1450.78}
[INFO|2026-01-07 19:30:25] logging.py:143 >> {'loss': 1.5838, 'learning_rate': 5.4686e-06, 'epoch': 2.36, 'throughput': 1450.64}
[INFO|2026-01-07 19:30:45] logging.py:143 >> {'loss': 1.3735, 'learning_rate': 5.4057e-06, 'epoch': 2.36, 'throughput': 1450.75}
[INFO|2026-01-07 19:31:01] logging.py:143 >> {'loss': 1.3170, 'learning_rate': 5.3430e-06, 'epoch': 2.36, 'throughput': 1450.70}
[INFO|2026-01-07 19:31:16] logging.py:143 >> {'loss': 1.7597, 'learning_rate': 5.2806e-06, 'epoch': 2.37, 'throughput': 1450.49}
[INFO|2026-01-07 19:31:35] logging.py:143 >> {'loss': 1.1406, 'learning_rate': 5.2186e-06, 'epoch': 2.37, 'throughput': 1450.65}
[INFO|2026-01-07 19:31:52] logging.py:143 >> {'loss': 1.3128, 'learning_rate': 5.1569e-06, 'epoch': 2.38, 'throughput': 1450.63}
[INFO|2026-01-07 19:32:09] logging.py:143 >> {'loss': 1.3296, 'learning_rate': 5.0956e-06, 'epoch': 2.38, 'throughput': 1450.72}
[INFO|2026-01-07 19:32:26] logging.py:143 >> {'loss': 1.5386, 'learning_rate': 5.0345e-06, 'epoch': 2.38, 'throughput': 1450.66}
[INFO|2026-01-07 19:32:45] logging.py:143 >> {'loss': 1.3997, 'learning_rate': 4.9738e-06, 'epoch': 2.39, 'throughput': 1450.69}
[INFO|2026-01-07 19:33:03] logging.py:143 >> {'loss': 1.5016, 'learning_rate': 4.9134e-06, 'epoch': 2.39, 'throughput': 1450.66}
[INFO|2026-01-07 19:33:21] logging.py:143 >> {'loss': 1.3360, 'learning_rate': 4.8534e-06, 'epoch': 2.40, 'throughput': 1450.71}
[INFO|2026-01-07 19:33:21] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3100
[INFO|2026-01-07 19:33:21] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:33:21] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:33:21] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3100\chat_template.jinja
[INFO|2026-01-07 19:33:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3100\tokenizer_config.json
[INFO|2026-01-07 19:33:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3100\special_tokens_map.json
[INFO|2026-01-07 19:33:37] logging.py:143 >> {'loss': 1.3855, 'learning_rate': 4.7936e-06, 'epoch': 2.40, 'throughput': 1450.54}
[INFO|2026-01-07 19:33:53] logging.py:143 >> {'loss': 1.5093, 'learning_rate': 4.7342e-06, 'epoch': 2.40, 'throughput': 1450.43}
[INFO|2026-01-07 19:34:10] logging.py:143 >> {'loss': 1.3957, 'learning_rate': 4.6752e-06, 'epoch': 2.41, 'throughput': 1450.45}
[INFO|2026-01-07 19:34:27] logging.py:143 >> {'loss': 1.2769, 'learning_rate': 4.6164e-06, 'epoch': 2.41, 'throughput': 1450.44}
[INFO|2026-01-07 19:34:43] logging.py:143 >> {'loss': 1.6381, 'learning_rate': 4.5580e-06, 'epoch': 2.42, 'throughput': 1450.31}
[INFO|2026-01-07 19:34:59] logging.py:143 >> {'loss': 0.9411, 'learning_rate': 4.4999e-06, 'epoch': 2.42, 'throughput': 1450.29}
[INFO|2026-01-07 19:35:14] logging.py:143 >> {'loss': 1.3216, 'learning_rate': 4.4422e-06, 'epoch': 2.42, 'throughput': 1450.14}
[INFO|2026-01-07 19:35:30] logging.py:143 >> {'loss': 1.3965, 'learning_rate': 4.3848e-06, 'epoch': 2.43, 'throughput': 1450.00}
[INFO|2026-01-07 19:35:45] logging.py:143 >> {'loss': 1.3469, 'learning_rate': 4.3278e-06, 'epoch': 2.43, 'throughput': 1449.85}
[INFO|2026-01-07 19:36:01] logging.py:143 >> {'loss': 1.6105, 'learning_rate': 4.2710e-06, 'epoch': 2.43, 'throughput': 1449.82}
[INFO|2026-01-07 19:36:20] logging.py:143 >> {'loss': 1.4006, 'learning_rate': 4.2147e-06, 'epoch': 2.44, 'throughput': 1449.91}
[INFO|2026-01-07 19:36:38] logging.py:143 >> {'loss': 1.3891, 'learning_rate': 4.1586e-06, 'epoch': 2.44, 'throughput': 1450.02}
[INFO|2026-01-07 19:36:56] logging.py:143 >> {'loss': 1.2644, 'learning_rate': 4.1029e-06, 'epoch': 2.45, 'throughput': 1450.10}
[INFO|2026-01-07 19:37:13] logging.py:143 >> {'loss': 1.4823, 'learning_rate': 4.0476e-06, 'epoch': 2.45, 'throughput': 1450.03}
[INFO|2026-01-07 19:37:31] logging.py:143 >> {'loss': 1.4095, 'learning_rate': 3.9926e-06, 'epoch': 2.45, 'throughput': 1450.03}
[INFO|2026-01-07 19:37:49] logging.py:143 >> {'loss': 1.2955, 'learning_rate': 3.9379e-06, 'epoch': 2.46, 'throughput': 1450.12}
[INFO|2026-01-07 19:38:06] logging.py:143 >> {'loss': 1.2694, 'learning_rate': 3.8836e-06, 'epoch': 2.46, 'throughput': 1450.10}
[INFO|2026-01-07 19:38:25] logging.py:143 >> {'loss': 1.4534, 'learning_rate': 3.8296e-06, 'epoch': 2.47, 'throughput': 1450.18}
[INFO|2026-01-07 19:38:43] logging.py:143 >> {'loss': 1.2590, 'learning_rate': 3.7760e-06, 'epoch': 2.47, 'throughput': 1450.26}
[INFO|2026-01-07 19:39:01] logging.py:143 >> {'loss': 1.3829, 'learning_rate': 3.7227e-06, 'epoch': 2.47, 'throughput': 1450.34}
[INFO|2026-01-07 19:39:01] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3200
[INFO|2026-01-07 19:39:01] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:39:01] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:39:01] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3200\chat_template.jinja
[INFO|2026-01-07 19:39:01] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3200\tokenizer_config.json
[INFO|2026-01-07 19:39:01] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3200\special_tokens_map.json
[INFO|2026-01-07 19:39:19] logging.py:143 >> {'loss': 1.2338, 'learning_rate': 3.6697e-06, 'epoch': 2.48, 'throughput': 1450.39}
[INFO|2026-01-07 19:39:34] logging.py:143 >> {'loss': 1.5446, 'learning_rate': 3.6172e-06, 'epoch': 2.48, 'throughput': 1450.25}
[INFO|2026-01-07 19:39:52] logging.py:143 >> {'loss': 1.4049, 'learning_rate': 3.5649e-06, 'epoch': 2.48, 'throughput': 1450.26}
[INFO|2026-01-07 19:40:09] logging.py:143 >> {'loss': 1.2086, 'learning_rate': 3.5130e-06, 'epoch': 2.49, 'throughput': 1450.24}
[INFO|2026-01-07 19:40:25] logging.py:143 >> {'loss': 1.3740, 'learning_rate': 3.4615e-06, 'epoch': 2.49, 'throughput': 1450.16}
[INFO|2026-01-07 19:40:44] logging.py:143 >> {'loss': 1.5025, 'learning_rate': 3.4103e-06, 'epoch': 2.50, 'throughput': 1450.26}
[INFO|2026-01-07 19:41:02] logging.py:143 >> {'loss': 1.4290, 'learning_rate': 3.3595e-06, 'epoch': 2.50, 'throughput': 1450.25}
[INFO|2026-01-07 19:41:20] logging.py:143 >> {'loss': 1.3197, 'learning_rate': 3.3090e-06, 'epoch': 2.50, 'throughput': 1450.30}
[INFO|2026-01-07 19:41:37] logging.py:143 >> {'loss': 1.5693, 'learning_rate': 3.2589e-06, 'epoch': 2.51, 'throughput': 1450.27}
[INFO|2026-01-07 19:41:57] logging.py:143 >> {'loss': 1.2085, 'learning_rate': 3.2091e-06, 'epoch': 2.51, 'throughput': 1450.41}
[INFO|2026-01-07 19:42:14] logging.py:143 >> {'loss': 1.3924, 'learning_rate': 3.1597e-06, 'epoch': 2.52, 'throughput': 1450.45}
[INFO|2026-01-07 19:42:32] logging.py:143 >> {'loss': 1.3740, 'learning_rate': 3.1107e-06, 'epoch': 2.52, 'throughput': 1450.48}
[INFO|2026-01-07 19:42:50] logging.py:143 >> {'loss': 1.1856, 'learning_rate': 3.0620e-06, 'epoch': 2.52, 'throughput': 1450.55}
[INFO|2026-01-07 19:43:11] logging.py:143 >> {'loss': 1.4148, 'learning_rate': 3.0137e-06, 'epoch': 2.53, 'throughput': 1450.70}
[INFO|2026-01-07 19:43:29] logging.py:143 >> {'loss': 1.2612, 'learning_rate': 2.9657e-06, 'epoch': 2.53, 'throughput': 1450.72}
[INFO|2026-01-07 19:43:47] logging.py:143 >> {'loss': 1.5658, 'learning_rate': 2.9181e-06, 'epoch': 2.53, 'throughput': 1450.73}
[INFO|2026-01-07 19:44:03] logging.py:143 >> {'loss': 1.3706, 'learning_rate': 2.8708e-06, 'epoch': 2.54, 'throughput': 1450.63}
[INFO|2026-01-07 19:44:19] logging.py:143 >> {'loss': 1.5309, 'learning_rate': 2.8240e-06, 'epoch': 2.54, 'throughput': 1450.54}
[INFO|2026-01-07 19:44:36] logging.py:143 >> {'loss': 1.2257, 'learning_rate': 2.7774e-06, 'epoch': 2.55, 'throughput': 1450.57}
[INFO|2026-01-07 19:44:54] logging.py:143 >> {'loss': 1.6570, 'learning_rate': 2.7313e-06, 'epoch': 2.55, 'throughput': 1450.56}
[INFO|2026-01-07 19:44:54] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3300
[INFO|2026-01-07 19:44:54] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:44:54] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:44:54] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3300\chat_template.jinja
[INFO|2026-01-07 19:44:54] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3300\tokenizer_config.json
[INFO|2026-01-07 19:44:54] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3300\special_tokens_map.json
[INFO|2026-01-07 19:45:13] logging.py:143 >> {'loss': 1.2870, 'learning_rate': 2.6855e-06, 'epoch': 2.55, 'throughput': 1450.57}
[INFO|2026-01-07 19:45:34] logging.py:143 >> {'loss': 1.2005, 'learning_rate': 2.6400e-06, 'epoch': 2.56, 'throughput': 1450.77}
[INFO|2026-01-07 19:45:52] logging.py:143 >> {'loss': 1.2639, 'learning_rate': 2.5950e-06, 'epoch': 2.56, 'throughput': 1450.79}
[INFO|2026-01-07 19:46:07] logging.py:143 >> {'loss': 1.6735, 'learning_rate': 2.5503e-06, 'epoch': 2.57, 'throughput': 1450.75}
[INFO|2026-01-07 19:46:25] logging.py:143 >> {'loss': 1.4608, 'learning_rate': 2.5060e-06, 'epoch': 2.57, 'throughput': 1450.85}
[INFO|2026-01-07 19:46:42] logging.py:143 >> {'loss': 1.8394, 'learning_rate': 2.4620e-06, 'epoch': 2.57, 'throughput': 1450.79}
[INFO|2026-01-07 19:47:03] logging.py:143 >> {'loss': 1.0936, 'learning_rate': 2.4184e-06, 'epoch': 2.58, 'throughput': 1450.95}
[INFO|2026-01-07 19:47:19] logging.py:143 >> {'loss': 1.3537, 'learning_rate': 2.3752e-06, 'epoch': 2.58, 'throughput': 1450.90}
[INFO|2026-01-07 19:47:38] logging.py:143 >> {'loss': 1.4152, 'learning_rate': 2.3323e-06, 'epoch': 2.59, 'throughput': 1450.98}
[INFO|2026-01-07 19:47:55] logging.py:143 >> {'loss': 1.4630, 'learning_rate': 2.2899e-06, 'epoch': 2.59, 'throughput': 1450.94}
[INFO|2026-01-07 19:48:12] logging.py:143 >> {'loss': 0.9474, 'learning_rate': 2.2478e-06, 'epoch': 2.59, 'throughput': 1450.97}
[INFO|2026-01-07 19:48:31] logging.py:143 >> {'loss': 1.5592, 'learning_rate': 2.2060e-06, 'epoch': 2.60, 'throughput': 1451.02}
[INFO|2026-01-07 19:48:50] logging.py:143 >> {'loss': 1.4286, 'learning_rate': 2.1647e-06, 'epoch': 2.60, 'throughput': 1451.10}
[INFO|2026-01-07 19:49:08] logging.py:143 >> {'loss': 1.5159, 'learning_rate': 2.1237e-06, 'epoch': 2.60, 'throughput': 1451.06}
[INFO|2026-01-07 19:49:26] logging.py:143 >> {'loss': 1.3060, 'learning_rate': 2.0831e-06, 'epoch': 2.61, 'throughput': 1451.09}
[INFO|2026-01-07 19:49:45] logging.py:143 >> {'loss': 1.2576, 'learning_rate': 2.0428e-06, 'epoch': 2.61, 'throughput': 1451.19}
[INFO|2026-01-07 19:50:02] logging.py:143 >> {'loss': 1.3776, 'learning_rate': 2.0030e-06, 'epoch': 2.62, 'throughput': 1451.22}
[INFO|2026-01-07 19:50:19] logging.py:143 >> {'loss': 1.3210, 'learning_rate': 1.9635e-06, 'epoch': 2.62, 'throughput': 1451.21}
[INFO|2026-01-07 19:50:39] logging.py:143 >> {'loss': 1.2342, 'learning_rate': 1.9244e-06, 'epoch': 2.62, 'throughput': 1451.37}
[INFO|2026-01-07 19:50:57] logging.py:143 >> {'loss': 1.2920, 'learning_rate': 1.8856e-06, 'epoch': 2.63, 'throughput': 1451.43}
[INFO|2026-01-07 19:50:57] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3400
[INFO|2026-01-07 19:50:57] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:50:57] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:50:57] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3400\chat_template.jinja
[INFO|2026-01-07 19:50:57] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3400\tokenizer_config.json
[INFO|2026-01-07 19:50:57] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3400\special_tokens_map.json
[INFO|2026-01-07 19:51:15] logging.py:143 >> {'loss': 1.3223, 'learning_rate': 1.8473e-06, 'epoch': 2.63, 'throughput': 1451.38}
[INFO|2026-01-07 19:51:34] logging.py:143 >> {'loss': 1.4513, 'learning_rate': 1.8093e-06, 'epoch': 2.64, 'throughput': 1451.37}
[INFO|2026-01-07 19:51:53] logging.py:143 >> {'loss': 1.4779, 'learning_rate': 1.7717e-06, 'epoch': 2.64, 'throughput': 1451.45}
[INFO|2026-01-07 19:52:12] logging.py:143 >> {'loss': 1.2233, 'learning_rate': 1.7345e-06, 'epoch': 2.64, 'throughput': 1451.53}
[INFO|2026-01-07 19:52:31] logging.py:143 >> {'loss': 1.3067, 'learning_rate': 1.6977e-06, 'epoch': 2.65, 'throughput': 1451.59}
[INFO|2026-01-07 19:52:50] logging.py:143 >> {'loss': 1.2737, 'learning_rate': 1.6612e-06, 'epoch': 2.65, 'throughput': 1451.71}
[INFO|2026-01-07 19:53:07] logging.py:143 >> {'loss': 1.2401, 'learning_rate': 1.6251e-06, 'epoch': 2.65, 'throughput': 1451.60}
[INFO|2026-01-07 19:53:22] logging.py:143 >> {'loss': 1.6317, 'learning_rate': 1.5895e-06, 'epoch': 2.66, 'throughput': 1451.46}
[INFO|2026-01-07 19:53:37] logging.py:143 >> {'loss': 1.3659, 'learning_rate': 1.5542e-06, 'epoch': 2.66, 'throughput': 1451.38}
[INFO|2026-01-07 19:53:54] logging.py:143 >> {'loss': 1.4904, 'learning_rate': 1.5192e-06, 'epoch': 2.67, 'throughput': 1451.41}
[INFO|2026-01-07 19:54:11] logging.py:143 >> {'loss': 1.1928, 'learning_rate': 1.4847e-06, 'epoch': 2.67, 'throughput': 1451.43}
[INFO|2026-01-07 19:54:30] logging.py:143 >> {'loss': 1.3748, 'learning_rate': 1.4506e-06, 'epoch': 2.67, 'throughput': 1451.46}
[INFO|2026-01-07 19:54:48] logging.py:143 >> {'loss': 1.1893, 'learning_rate': 1.4168e-06, 'epoch': 2.68, 'throughput': 1451.49}
[INFO|2026-01-07 19:55:06] logging.py:143 >> {'loss': 1.5390, 'learning_rate': 1.3834e-06, 'epoch': 2.68, 'throughput': 1451.53}
[INFO|2026-01-07 19:55:24] logging.py:143 >> {'loss': 1.3702, 'learning_rate': 1.3504e-06, 'epoch': 2.69, 'throughput': 1451.54}
[INFO|2026-01-07 19:55:41] logging.py:143 >> {'loss': 1.1663, 'learning_rate': 1.3178e-06, 'epoch': 2.69, 'throughput': 1451.47}
[INFO|2026-01-07 19:56:00] logging.py:143 >> {'loss': 1.3468, 'learning_rate': 1.2856e-06, 'epoch': 2.69, 'throughput': 1451.56}
[INFO|2026-01-07 19:56:19] logging.py:143 >> {'loss': 1.2286, 'learning_rate': 1.2538e-06, 'epoch': 2.70, 'throughput': 1451.64}
[INFO|2026-01-07 19:56:35] logging.py:143 >> {'loss': 1.6877, 'learning_rate': 1.2223e-06, 'epoch': 2.70, 'throughput': 1451.52}
[INFO|2026-01-07 19:56:53] logging.py:143 >> {'loss': 1.2804, 'learning_rate': 1.1913e-06, 'epoch': 2.70, 'throughput': 1451.54}
[INFO|2026-01-07 19:56:53] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3500
[INFO|2026-01-07 19:56:53] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 19:56:53] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 19:56:53] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3500\chat_template.jinja
[INFO|2026-01-07 19:56:53] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3500\tokenizer_config.json
[INFO|2026-01-07 19:56:53] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3500\special_tokens_map.json
[INFO|2026-01-07 19:57:12] logging.py:143 >> {'loss': 1.3420, 'learning_rate': 1.1606e-06, 'epoch': 2.71, 'throughput': 1451.56}
[INFO|2026-01-07 19:57:27] logging.py:143 >> {'loss': 1.4593, 'learning_rate': 1.1304e-06, 'epoch': 2.71, 'throughput': 1451.48}
[INFO|2026-01-07 19:57:44] logging.py:143 >> {'loss': 1.2319, 'learning_rate': 1.1005e-06, 'epoch': 2.72, 'throughput': 1451.42}
[INFO|2026-01-07 19:58:02] logging.py:143 >> {'loss': 1.5888, 'learning_rate': 1.0710e-06, 'epoch': 2.72, 'throughput': 1451.44}
[INFO|2026-01-07 19:58:21] logging.py:143 >> {'loss': 1.2687, 'learning_rate': 1.0419e-06, 'epoch': 2.72, 'throughput': 1451.51}
[INFO|2026-01-07 19:58:37] logging.py:143 >> {'loss': 1.0752, 'learning_rate': 1.0132e-06, 'epoch': 2.73, 'throughput': 1451.50}
[INFO|2026-01-07 19:58:56] logging.py:143 >> {'loss': 1.5528, 'learning_rate': 9.8488e-07, 'epoch': 2.73, 'throughput': 1451.48}
[INFO|2026-01-07 19:59:15] logging.py:143 >> {'loss': 1.4011, 'learning_rate': 9.5697e-07, 'epoch': 2.74, 'throughput': 1451.53}
[INFO|2026-01-07 19:59:33] logging.py:143 >> {'loss': 1.1267, 'learning_rate': 9.2944e-07, 'epoch': 2.74, 'throughput': 1451.57}
[INFO|2026-01-07 19:59:50] logging.py:143 >> {'loss': 1.3040, 'learning_rate': 9.0231e-07, 'epoch': 2.74, 'throughput': 1451.56}
[INFO|2026-01-07 20:00:09] logging.py:143 >> {'loss': 1.3232, 'learning_rate': 8.7558e-07, 'epoch': 2.75, 'throughput': 1451.59}
[INFO|2026-01-07 20:00:28] logging.py:143 >> {'loss': 1.2118, 'learning_rate': 8.4924e-07, 'epoch': 2.75, 'throughput': 1451.70}
[INFO|2026-01-07 20:00:47] logging.py:143 >> {'loss': 1.3933, 'learning_rate': 8.2329e-07, 'epoch': 2.76, 'throughput': 1451.71}
[INFO|2026-01-07 20:01:03] logging.py:143 >> {'loss': 1.2571, 'learning_rate': 7.9774e-07, 'epoch': 2.76, 'throughput': 1451.69}
[INFO|2026-01-07 20:01:23] logging.py:143 >> {'loss': 1.4390, 'learning_rate': 7.7259e-07, 'epoch': 2.76, 'throughput': 1451.82}
[INFO|2026-01-07 20:01:41] logging.py:143 >> {'loss': 1.4528, 'learning_rate': 7.4784e-07, 'epoch': 2.77, 'throughput': 1451.81}
[INFO|2026-01-07 20:01:56] logging.py:143 >> {'loss': 1.6051, 'learning_rate': 7.2348e-07, 'epoch': 2.77, 'throughput': 1451.68}
[INFO|2026-01-07 20:02:12] logging.py:143 >> {'loss': 1.4844, 'learning_rate': 6.9952e-07, 'epoch': 2.77, 'throughput': 1451.60}
[INFO|2026-01-07 20:02:29] logging.py:143 >> {'loss': 1.5630, 'learning_rate': 6.7595e-07, 'epoch': 2.78, 'throughput': 1451.58}
[INFO|2026-01-07 20:02:49] logging.py:143 >> {'loss': 1.2962, 'learning_rate': 6.5279e-07, 'epoch': 2.78, 'throughput': 1451.71}
[INFO|2026-01-07 20:02:49] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3600
[INFO|2026-01-07 20:02:49] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 20:02:49] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 20:02:49] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3600\chat_template.jinja
[INFO|2026-01-07 20:02:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3600\tokenizer_config.json
[INFO|2026-01-07 20:02:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3600\special_tokens_map.json
[INFO|2026-01-07 20:03:09] logging.py:143 >> {'loss': 1.2689, 'learning_rate': 6.3002e-07, 'epoch': 2.79, 'throughput': 1451.81}
[INFO|2026-01-07 20:03:26] logging.py:143 >> {'loss': 1.4287, 'learning_rate': 6.0765e-07, 'epoch': 2.79, 'throughput': 1451.82}
[INFO|2026-01-07 20:03:43] logging.py:143 >> {'loss': 1.1175, 'learning_rate': 5.8569e-07, 'epoch': 2.79, 'throughput': 1451.87}
[INFO|2026-01-07 20:04:02] logging.py:143 >> {'loss': 1.2425, 'learning_rate': 5.6412e-07, 'epoch': 2.80, 'throughput': 1451.98}
[INFO|2026-01-07 20:04:19] logging.py:143 >> {'loss': 1.5314, 'learning_rate': 5.4295e-07, 'epoch': 2.80, 'throughput': 1451.96}
[INFO|2026-01-07 20:04:37] logging.py:143 >> {'loss': 1.6834, 'learning_rate': 5.2218e-07, 'epoch': 2.81, 'throughput': 1451.97}
[INFO|2026-01-07 20:04:55] logging.py:143 >> {'loss': 1.4675, 'learning_rate': 5.0182e-07, 'epoch': 2.81, 'throughput': 1451.99}
[INFO|2026-01-07 20:05:13] logging.py:143 >> {'loss': 1.3854, 'learning_rate': 4.8185e-07, 'epoch': 2.81, 'throughput': 1452.10}
[INFO|2026-01-07 20:05:31] logging.py:143 >> {'loss': 1.5857, 'learning_rate': 4.6228e-07, 'epoch': 2.82, 'throughput': 1452.08}
[INFO|2026-01-07 20:05:47] logging.py:143 >> {'loss': 1.3894, 'learning_rate': 4.4312e-07, 'epoch': 2.82, 'throughput': 1452.02}
[INFO|2026-01-07 20:06:05] logging.py:143 >> {'loss': 1.0670, 'learning_rate': 4.2436e-07, 'epoch': 2.82, 'throughput': 1452.06}
[INFO|2026-01-07 20:06:20] logging.py:143 >> {'loss': 1.3396, 'learning_rate': 4.0600e-07, 'epoch': 2.83, 'throughput': 1451.93}
[INFO|2026-01-07 20:06:36] logging.py:143 >> {'loss': 1.5705, 'learning_rate': 3.8805e-07, 'epoch': 2.83, 'throughput': 1451.90}
[INFO|2026-01-07 20:06:55] logging.py:143 >> {'loss': 1.6831, 'learning_rate': 3.7049e-07, 'epoch': 2.84, 'throughput': 1451.91}
[INFO|2026-01-07 20:07:11] logging.py:143 >> {'loss': 1.2997, 'learning_rate': 3.5335e-07, 'epoch': 2.84, 'throughput': 1451.86}
[INFO|2026-01-07 20:07:30] logging.py:143 >> {'loss': 1.2354, 'learning_rate': 3.3660e-07, 'epoch': 2.84, 'throughput': 1451.89}
[INFO|2026-01-07 20:07:47] logging.py:143 >> {'loss': 1.3309, 'learning_rate': 3.2026e-07, 'epoch': 2.85, 'throughput': 1451.86}
[INFO|2026-01-07 20:08:09] logging.py:143 >> {'loss': 1.3476, 'learning_rate': 3.0432e-07, 'epoch': 2.85, 'throughput': 1452.03}
[INFO|2026-01-07 20:08:26] logging.py:143 >> {'loss': 1.5670, 'learning_rate': 2.8879e-07, 'epoch': 2.86, 'throughput': 1452.01}
[INFO|2026-01-07 20:08:44] logging.py:143 >> {'loss': 1.1867, 'learning_rate': 2.7366e-07, 'epoch': 2.86, 'throughput': 1452.04}
[INFO|2026-01-07 20:08:44] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3700
[INFO|2026-01-07 20:08:44] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 20:08:44] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 20:08:44] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3700\chat_template.jinja
[INFO|2026-01-07 20:08:44] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3700\tokenizer_config.json
[INFO|2026-01-07 20:08:44] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3700\special_tokens_map.json
[INFO|2026-01-07 20:09:02] logging.py:143 >> {'loss': 1.4012, 'learning_rate': 2.5893e-07, 'epoch': 2.86, 'throughput': 1452.02}
[INFO|2026-01-07 20:09:19] logging.py:143 >> {'loss': 1.6049, 'learning_rate': 2.4461e-07, 'epoch': 2.87, 'throughput': 1452.02}
[INFO|2026-01-07 20:09:38] logging.py:143 >> {'loss': 1.1797, 'learning_rate': 2.3070e-07, 'epoch': 2.87, 'throughput': 1452.10}
[INFO|2026-01-07 20:09:58] logging.py:143 >> {'loss': 1.3042, 'learning_rate': 2.1719e-07, 'epoch': 2.87, 'throughput': 1452.17}
[INFO|2026-01-07 20:10:15] logging.py:143 >> {'loss': 1.4252, 'learning_rate': 2.0409e-07, 'epoch': 2.88, 'throughput': 1452.16}
[INFO|2026-01-07 20:10:34] logging.py:143 >> {'loss': 1.1936, 'learning_rate': 1.9139e-07, 'epoch': 2.88, 'throughput': 1452.21}
[INFO|2026-01-07 20:10:51] logging.py:143 >> {'loss': 1.3319, 'learning_rate': 1.7910e-07, 'epoch': 2.89, 'throughput': 1452.17}
[INFO|2026-01-07 20:11:09] logging.py:143 >> {'loss': 1.3326, 'learning_rate': 1.6722e-07, 'epoch': 2.89, 'throughput': 1452.22}
[INFO|2026-01-07 20:11:25] logging.py:143 >> {'loss': 1.6555, 'learning_rate': 1.5574e-07, 'epoch': 2.89, 'throughput': 1452.10}
[INFO|2026-01-07 20:11:41] logging.py:143 >> {'loss': 1.4090, 'learning_rate': 1.4467e-07, 'epoch': 2.90, 'throughput': 1452.04}
[INFO|2026-01-07 20:12:01] logging.py:143 >> {'loss': 1.2189, 'learning_rate': 1.3401e-07, 'epoch': 2.90, 'throughput': 1452.12}
[INFO|2026-01-07 20:12:20] logging.py:143 >> {'loss': 1.4556, 'learning_rate': 1.2375e-07, 'epoch': 2.91, 'throughput': 1452.15}
[INFO|2026-01-07 20:12:38] logging.py:143 >> {'loss': 1.4568, 'learning_rate': 1.1390e-07, 'epoch': 2.91, 'throughput': 1452.19}
[INFO|2026-01-07 20:12:57] logging.py:143 >> {'loss': 1.5172, 'learning_rate': 1.0446e-07, 'epoch': 2.91, 'throughput': 1452.26}
[INFO|2026-01-07 20:13:14] logging.py:143 >> {'loss': 1.3625, 'learning_rate': 9.5427e-08, 'epoch': 2.92, 'throughput': 1452.25}
[INFO|2026-01-07 20:13:32] logging.py:143 >> {'loss': 1.1008, 'learning_rate': 8.6800e-08, 'epoch': 2.92, 'throughput': 1452.30}
[INFO|2026-01-07 20:13:53] logging.py:143 >> {'loss': 1.3040, 'learning_rate': 7.8582e-08, 'epoch': 2.93, 'throughput': 1452.41}
[INFO|2026-01-07 20:14:12] logging.py:143 >> {'loss': 1.5054, 'learning_rate': 7.0772e-08, 'epoch': 2.93, 'throughput': 1452.45}
[INFO|2026-01-07 20:14:34] logging.py:143 >> {'loss': 1.1181, 'learning_rate': 6.3370e-08, 'epoch': 2.93, 'throughput': 1452.63}
[INFO|2026-01-07 20:14:52] logging.py:143 >> {'loss': 1.1867, 'learning_rate': 5.6376e-08, 'epoch': 2.94, 'throughput': 1452.66}
[INFO|2026-01-07 20:14:52] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3800
[INFO|2026-01-07 20:14:52] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 20:14:52] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 20:14:52] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3800\chat_template.jinja
[INFO|2026-01-07 20:14:52] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3800\tokenizer_config.json
[INFO|2026-01-07 20:14:52] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3800\special_tokens_map.json
[INFO|2026-01-07 20:15:10] logging.py:143 >> {'loss': 1.5721, 'learning_rate': 4.9790e-08, 'epoch': 2.94, 'throughput': 1452.62}
[INFO|2026-01-07 20:15:28] logging.py:143 >> {'loss': 1.5690, 'learning_rate': 4.3613e-08, 'epoch': 2.94, 'throughput': 1452.66}
[INFO|2026-01-07 20:15:43] logging.py:143 >> {'loss': 1.4547, 'learning_rate': 3.7845e-08, 'epoch': 2.95, 'throughput': 1452.57}
[INFO|2026-01-07 20:16:03] logging.py:143 >> {'loss': 1.1088, 'learning_rate': 3.2485e-08, 'epoch': 2.95, 'throughput': 1452.65}
[INFO|2026-01-07 20:16:24] logging.py:143 >> {'loss': 1.0245, 'learning_rate': 2.7534e-08, 'epoch': 2.96, 'throughput': 1452.83}
[INFO|2026-01-07 20:16:41] logging.py:143 >> {'loss': 1.3907, 'learning_rate': 2.2992e-08, 'epoch': 2.96, 'throughput': 1452.84}
[INFO|2026-01-07 20:16:59] logging.py:143 >> {'loss': 1.2328, 'learning_rate': 1.8859e-08, 'epoch': 2.96, 'throughput': 1452.80}
[INFO|2026-01-07 20:17:18] logging.py:143 >> {'loss': 1.1883, 'learning_rate': 1.5135e-08, 'epoch': 2.97, 'throughput': 1452.89}
[INFO|2026-01-07 20:17:36] logging.py:143 >> {'loss': 1.2534, 'learning_rate': 1.1820e-08, 'epoch': 2.97, 'throughput': 1452.90}
[INFO|2026-01-07 20:17:54] logging.py:143 >> {'loss': 1.4825, 'learning_rate': 8.9146e-09, 'epoch': 2.98, 'throughput': 1452.92}
[INFO|2026-01-07 20:18:13] logging.py:143 >> {'loss': 1.2913, 'learning_rate': 6.4179e-09, 'epoch': 2.98, 'throughput': 1452.99}
[INFO|2026-01-07 20:18:33] logging.py:143 >> {'loss': 1.4655, 'learning_rate': 4.3305e-09, 'epoch': 2.98, 'throughput': 1453.11}
[INFO|2026-01-07 20:18:53] logging.py:143 >> {'loss': 1.2901, 'learning_rate': 2.6524e-09, 'epoch': 2.99, 'throughput': 1453.19}
[INFO|2026-01-07 20:19:11] logging.py:143 >> {'loss': 1.7065, 'learning_rate': 1.3835e-09, 'epoch': 2.99, 'throughput': 1453.17}
[INFO|2026-01-07 20:19:27] logging.py:143 >> {'loss': 1.2878, 'learning_rate': 5.2393e-10, 'epoch': 2.99, 'throughput': 1453.09}
[INFO|2026-01-07 20:19:43] logging.py:143 >> {'loss': 1.4698, 'learning_rate': 7.3679e-11, 'epoch': 3.00, 'throughput': 1453.03}
[INFO|2026-01-07 20:19:52] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3882
[INFO|2026-01-07 20:19:52] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 20:19:52] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 20:19:52] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3882\chat_template.jinja
[INFO|2026-01-07 20:19:52] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3882\tokenizer_config.json
[INFO|2026-01-07 20:19:52] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\checkpoint-3882\special_tokens_map.json
[INFO|2026-01-07 20:19:52] trainer.py:2810 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|2026-01-07 20:19:52] trainer.py:4309 >> Saving model checkpoint to saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48
[INFO|2026-01-07 20:19:52] configuration_utils.py:763 >> loading configuration file C:\Users\yes\.cache\modelscope\hub\models\Qwen\Qwen1___5-0___5B-Chat\config.json
[INFO|2026-01-07 20:19:52] configuration_utils.py:839 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2026-01-07 20:19:52] tokenization_utils_base.py:2421 >> chat template saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\chat_template.jinja
[INFO|2026-01-07 20:19:52] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\tokenizer_config.json
[INFO|2026-01-07 20:19:52] tokenization_utils_base.py:2599 >> Special tokens file saved in saves\Qwen1.5-0.5B-Chat\lora\train_2026-01-07-16-29-48\special_tokens_map.json
[WARNING|2026-01-07 20:19:52] logging.py:148 >> No metric eval_loss to plot.
[WARNING|2026-01-07 20:19:52] logging.py:148 >> No metric eval_accuracy to plot.
[INFO|2026-01-07 20:19:52] modelcard.py:456 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
