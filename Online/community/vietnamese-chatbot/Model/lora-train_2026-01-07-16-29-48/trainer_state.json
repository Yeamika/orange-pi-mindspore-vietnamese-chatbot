{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3882,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003864734299516908,
      "grad_norm": 1.7072749137878418,
      "learning_rate": 4.9999869015984654e-05,
      "loss": 2.6074,
      "num_input_tokens_seen": 26352,
      "step": 5,
      "train_runtime": 17.3061,
      "train_tokens_per_second": 1522.704
    },
    {
      "epoch": 0.007729468599033816,
      "grad_norm": 0.742422878742218,
      "learning_rate": 4.999933689577465e-05,
      "loss": 2.427,
      "num_input_tokens_seen": 51808,
      "step": 10,
      "train_runtime": 33.8297,
      "train_tokens_per_second": 1531.435
    },
    {
      "epoch": 0.011594202898550725,
      "grad_norm": 0.9699307680130005,
      "learning_rate": 4.999839546157474e-05,
      "loss": 2.2248,
      "num_input_tokens_seen": 80240,
      "step": 15,
      "train_runtime": 51.5782,
      "train_tokens_per_second": 1555.695
    },
    {
      "epoch": 0.015458937198067632,
      "grad_norm": 0.8638522028923035,
      "learning_rate": 4.999704472879902e-05,
      "loss": 2.4657,
      "num_input_tokens_seen": 105840,
      "step": 20,
      "train_runtime": 68.4703,
      "train_tokens_per_second": 1545.779
    },
    {
      "epoch": 0.01932367149758454,
      "grad_norm": 0.7961790561676025,
      "learning_rate": 4.9995284719563015e-05,
      "loss": 2.3999,
      "num_input_tokens_seen": 126208,
      "step": 25,
      "train_runtime": 83.2892,
      "train_tokens_per_second": 1515.299
    },
    {
      "epoch": 0.02318840579710145,
      "grad_norm": 0.8920868635177612,
      "learning_rate": 4.9993115462683374e-05,
      "loss": 2.5051,
      "num_input_tokens_seen": 151392,
      "step": 30,
      "train_runtime": 100.016,
      "train_tokens_per_second": 1513.677
    },
    {
      "epoch": 0.02705314009661836,
      "grad_norm": 0.8816282153129578,
      "learning_rate": 4.9990536993677295e-05,
      "loss": 2.0395,
      "num_input_tokens_seen": 177120,
      "step": 35,
      "train_runtime": 117.0474,
      "train_tokens_per_second": 1513.233
    },
    {
      "epoch": 0.030917874396135265,
      "grad_norm": 0.7294060587882996,
      "learning_rate": 4.998754935476205e-05,
      "loss": 2.1609,
      "num_input_tokens_seen": 202160,
      "step": 40,
      "train_runtime": 133.3916,
      "train_tokens_per_second": 1515.537
    },
    {
      "epoch": 0.034782608695652174,
      "grad_norm": 0.9949403405189514,
      "learning_rate": 4.9984152594854234e-05,
      "loss": 2.0806,
      "num_input_tokens_seen": 230144,
      "step": 45,
      "train_runtime": 151.5158,
      "train_tokens_per_second": 1518.944
    },
    {
      "epoch": 0.03864734299516908,
      "grad_norm": 0.9216907024383545,
      "learning_rate": 4.998034676956898e-05,
      "loss": 1.7158,
      "num_input_tokens_seen": 259216,
      "step": 50,
      "train_runtime": 169.9659,
      "train_tokens_per_second": 1525.105
    },
    {
      "epoch": 0.04251207729468599,
      "grad_norm": 0.9052485227584839,
      "learning_rate": 4.997613194121902e-05,
      "loss": 2.1779,
      "num_input_tokens_seen": 287568,
      "step": 55,
      "train_runtime": 188.622,
      "train_tokens_per_second": 1524.573
    },
    {
      "epoch": 0.0463768115942029,
      "grad_norm": 1.0704790353775024,
      "learning_rate": 4.997150817881374e-05,
      "loss": 2.104,
      "num_input_tokens_seen": 309920,
      "step": 60,
      "train_runtime": 204.4906,
      "train_tokens_per_second": 1515.571
    },
    {
      "epoch": 0.050241545893719805,
      "grad_norm": 1.039364218711853,
      "learning_rate": 4.996647555805796e-05,
      "loss": 1.9757,
      "num_input_tokens_seen": 337376,
      "step": 65,
      "train_runtime": 222.7654,
      "train_tokens_per_second": 1514.49
    },
    {
      "epoch": 0.05410628019323672,
      "grad_norm": 1.0137428045272827,
      "learning_rate": 4.996103416135075e-05,
      "loss": 1.946,
      "num_input_tokens_seen": 360672,
      "step": 70,
      "train_runtime": 238.6923,
      "train_tokens_per_second": 1511.033
    },
    {
      "epoch": 0.057971014492753624,
      "grad_norm": 1.2172119617462158,
      "learning_rate": 4.995518407778407e-05,
      "loss": 1.5455,
      "num_input_tokens_seen": 389152,
      "step": 75,
      "train_runtime": 257.6661,
      "train_tokens_per_second": 1510.296
    },
    {
      "epoch": 0.06183574879227053,
      "grad_norm": 0.9473534822463989,
      "learning_rate": 4.99489254031413e-05,
      "loss": 1.7514,
      "num_input_tokens_seen": 419872,
      "step": 80,
      "train_runtime": 277.3068,
      "train_tokens_per_second": 1514.106
    },
    {
      "epoch": 0.06570048309178744,
      "grad_norm": 1.3425668478012085,
      "learning_rate": 4.994225823989567e-05,
      "loss": 1.9383,
      "num_input_tokens_seen": 445792,
      "step": 85,
      "train_runtime": 294.9436,
      "train_tokens_per_second": 1511.448
    },
    {
      "epoch": 0.06956521739130435,
      "grad_norm": 0.9383313059806824,
      "learning_rate": 4.993518269720862e-05,
      "loss": 1.885,
      "num_input_tokens_seen": 470048,
      "step": 90,
      "train_runtime": 311.5303,
      "train_tokens_per_second": 1508.835
    },
    {
      "epoch": 0.07342995169082125,
      "grad_norm": 1.2687973976135254,
      "learning_rate": 4.992769889092796e-05,
      "loss": 2.2096,
      "num_input_tokens_seen": 490704,
      "step": 95,
      "train_runtime": 326.7448,
      "train_tokens_per_second": 1501.796
    },
    {
      "epoch": 0.07729468599033816,
      "grad_norm": 0.878288745880127,
      "learning_rate": 4.9919806943586e-05,
      "loss": 2.1001,
      "num_input_tokens_seen": 520752,
      "step": 100,
      "train_runtime": 346.2722,
      "train_tokens_per_second": 1503.88
    },
    {
      "epoch": 0.08115942028985507,
      "grad_norm": 1.2944053411483765,
      "learning_rate": 4.991150698439756e-05,
      "loss": 1.6895,
      "num_input_tokens_seen": 542304,
      "step": 105,
      "train_runtime": 362.3129,
      "train_tokens_per_second": 1496.783
    },
    {
      "epoch": 0.08502415458937199,
      "grad_norm": 1.4418504238128662,
      "learning_rate": 4.99027991492578e-05,
      "loss": 1.7858,
      "num_input_tokens_seen": 567952,
      "step": 110,
      "train_runtime": 379.895,
      "train_tokens_per_second": 1495.023
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.9217000007629395,
      "learning_rate": 4.9893683580740066e-05,
      "loss": 1.8247,
      "num_input_tokens_seen": 593472,
      "step": 115,
      "train_runtime": 397.3762,
      "train_tokens_per_second": 1493.476
    },
    {
      "epoch": 0.0927536231884058,
      "grad_norm": 1.2276909351348877,
      "learning_rate": 4.9884160428093504e-05,
      "loss": 2.1066,
      "num_input_tokens_seen": 616992,
      "step": 120,
      "train_runtime": 413.8125,
      "train_tokens_per_second": 1490.994
    },
    {
      "epoch": 0.0966183574879227,
      "grad_norm": 2.1537017822265625,
      "learning_rate": 4.987422984724064e-05,
      "loss": 1.9646,
      "num_input_tokens_seen": 648128,
      "step": 125,
      "train_runtime": 434.1466,
      "train_tokens_per_second": 1492.878
    },
    {
      "epoch": 0.10048309178743961,
      "grad_norm": 1.871673345565796,
      "learning_rate": 4.986389200077479e-05,
      "loss": 1.8094,
      "num_input_tokens_seen": 671408,
      "step": 130,
      "train_runtime": 450.6158,
      "train_tokens_per_second": 1489.979
    },
    {
      "epoch": 0.10434782608695652,
      "grad_norm": 1.3364298343658447,
      "learning_rate": 4.985314705795747e-05,
      "loss": 1.9958,
      "num_input_tokens_seen": 690976,
      "step": 135,
      "train_runtime": 465.6253,
      "train_tokens_per_second": 1483.974
    },
    {
      "epoch": 0.10821256038647344,
      "grad_norm": 0.9901940226554871,
      "learning_rate": 4.984199519471556e-05,
      "loss": 1.7228,
      "num_input_tokens_seen": 716256,
      "step": 140,
      "train_runtime": 482.7811,
      "train_tokens_per_second": 1483.604
    },
    {
      "epoch": 0.11207729468599034,
      "grad_norm": 1.8675537109375,
      "learning_rate": 4.9830436593638455e-05,
      "loss": 1.9234,
      "num_input_tokens_seen": 737808,
      "step": 145,
      "train_runtime": 498.4689,
      "train_tokens_per_second": 1480.149
    },
    {
      "epoch": 0.11594202898550725,
      "grad_norm": 1.2043110132217407,
      "learning_rate": 4.981847144397505e-05,
      "loss": 1.6267,
      "num_input_tokens_seen": 768912,
      "step": 150,
      "train_runtime": 518.8191,
      "train_tokens_per_second": 1482.043
    },
    {
      "epoch": 0.11980676328502415,
      "grad_norm": 1.2222487926483154,
      "learning_rate": 4.980609994163066e-05,
      "loss": 1.791,
      "num_input_tokens_seen": 792032,
      "step": 155,
      "train_runtime": 535.1882,
      "train_tokens_per_second": 1479.913
    },
    {
      "epoch": 0.12367149758454106,
      "grad_norm": 1.2933670282363892,
      "learning_rate": 4.979332228916384e-05,
      "loss": 1.7532,
      "num_input_tokens_seen": 817440,
      "step": 160,
      "train_runtime": 552.6778,
      "train_tokens_per_second": 1479.053
    },
    {
      "epoch": 0.12753623188405797,
      "grad_norm": 1.4026272296905518,
      "learning_rate": 4.978013869578298e-05,
      "loss": 1.9917,
      "num_input_tokens_seen": 838816,
      "step": 165,
      "train_runtime": 568.0087,
      "train_tokens_per_second": 1476.766
    },
    {
      "epoch": 0.13140096618357489,
      "grad_norm": 0.9952778220176697,
      "learning_rate": 4.976654937734302e-05,
      "loss": 1.4709,
      "num_input_tokens_seen": 867280,
      "step": 170,
      "train_runtime": 586.6266,
      "train_tokens_per_second": 1478.419
    },
    {
      "epoch": 0.13526570048309178,
      "grad_norm": 1.4102182388305664,
      "learning_rate": 4.975255455634176e-05,
      "loss": 1.6237,
      "num_input_tokens_seen": 892144,
      "step": 175,
      "train_runtime": 603.797,
      "train_tokens_per_second": 1477.556
    },
    {
      "epoch": 0.1391304347826087,
      "grad_norm": 1.3263441324234009,
      "learning_rate": 4.973815446191631e-05,
      "loss": 1.5752,
      "num_input_tokens_seen": 915312,
      "step": 180,
      "train_runtime": 619.9651,
      "train_tokens_per_second": 1476.393
    },
    {
      "epoch": 0.14299516908212562,
      "grad_norm": 1.9561549425125122,
      "learning_rate": 4.972334932983935e-05,
      "loss": 1.6035,
      "num_input_tokens_seen": 941168,
      "step": 185,
      "train_runtime": 637.6135,
      "train_tokens_per_second": 1476.079
    },
    {
      "epoch": 0.1468599033816425,
      "grad_norm": 1.3776874542236328,
      "learning_rate": 4.9708139402515206e-05,
      "loss": 1.7119,
      "num_input_tokens_seen": 966320,
      "step": 190,
      "train_runtime": 654.8687,
      "train_tokens_per_second": 1475.593
    },
    {
      "epoch": 0.15072463768115943,
      "grad_norm": 1.5080245733261108,
      "learning_rate": 4.969252492897592e-05,
      "loss": 1.8642,
      "num_input_tokens_seen": 988176,
      "step": 195,
      "train_runtime": 670.6823,
      "train_tokens_per_second": 1473.389
    },
    {
      "epoch": 0.15458937198067632,
      "grad_norm": 2.4730238914489746,
      "learning_rate": 4.967650616487719e-05,
      "loss": 1.4742,
      "num_input_tokens_seen": 1015008,
      "step": 200,
      "train_runtime": 688.8963,
      "train_tokens_per_second": 1473.383
    },
    {
      "epoch": 0.15845410628019324,
      "grad_norm": 1.043963074684143,
      "learning_rate": 4.966008337249411e-05,
      "loss": 2.0491,
      "num_input_tokens_seen": 1038816,
      "step": 205,
      "train_runtime": 705.922,
      "train_tokens_per_second": 1471.573
    },
    {
      "epoch": 0.16231884057971013,
      "grad_norm": 1.2233885526657104,
      "learning_rate": 4.9643256820717e-05,
      "loss": 1.996,
      "num_input_tokens_seen": 1064032,
      "step": 210,
      "train_runtime": 723.3279,
      "train_tokens_per_second": 1471.023
    },
    {
      "epoch": 0.16618357487922705,
      "grad_norm": 1.1140562295913696,
      "learning_rate": 4.962602678504685e-05,
      "loss": 1.6837,
      "num_input_tokens_seen": 1092816,
      "step": 215,
      "train_runtime": 742.3918,
      "train_tokens_per_second": 1472.021
    },
    {
      "epoch": 0.17004830917874397,
      "grad_norm": 1.2068039178848267,
      "learning_rate": 4.9608393547590956e-05,
      "loss": 1.5468,
      "num_input_tokens_seen": 1119824,
      "step": 220,
      "train_runtime": 760.7978,
      "train_tokens_per_second": 1471.907
    },
    {
      "epoch": 0.17391304347826086,
      "grad_norm": 1.622971534729004,
      "learning_rate": 4.959035739705821e-05,
      "loss": 1.9058,
      "num_input_tokens_seen": 1145328,
      "step": 225,
      "train_runtime": 778.3508,
      "train_tokens_per_second": 1471.48
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.680248737335205,
      "learning_rate": 4.957191862875437e-05,
      "loss": 1.8959,
      "num_input_tokens_seen": 1171072,
      "step": 230,
      "train_runtime": 795.9229,
      "train_tokens_per_second": 1471.338
    },
    {
      "epoch": 0.18164251207729468,
      "grad_norm": 1.549170970916748,
      "learning_rate": 4.9553077544577284e-05,
      "loss": 1.6354,
      "num_input_tokens_seen": 1197328,
      "step": 235,
      "train_runtime": 814.0945,
      "train_tokens_per_second": 1470.748
    },
    {
      "epoch": 0.1855072463768116,
      "grad_norm": 1.4326244592666626,
      "learning_rate": 4.953383445301191e-05,
      "loss": 1.7558,
      "num_input_tokens_seen": 1220288,
      "step": 240,
      "train_runtime": 830.2819,
      "train_tokens_per_second": 1469.727
    },
    {
      "epoch": 0.18937198067632852,
      "grad_norm": 1.5553758144378662,
      "learning_rate": 4.9514189669125275e-05,
      "loss": 1.7939,
      "num_input_tokens_seen": 1247088,
      "step": 245,
      "train_runtime": 848.785,
      "train_tokens_per_second": 1469.263
    },
    {
      "epoch": 0.1932367149758454,
      "grad_norm": 1.639651894569397,
      "learning_rate": 4.949414351456131e-05,
      "loss": 1.8311,
      "num_input_tokens_seen": 1271920,
      "step": 250,
      "train_runtime": 865.9069,
      "train_tokens_per_second": 1468.888
    },
    {
      "epoch": 0.19710144927536233,
      "grad_norm": 1.4920212030410767,
      "learning_rate": 4.9473696317535565e-05,
      "loss": 1.5927,
      "num_input_tokens_seen": 1299200,
      "step": 255,
      "train_runtime": 884.2012,
      "train_tokens_per_second": 1469.349
    },
    {
      "epoch": 0.20096618357487922,
      "grad_norm": 1.3922184705734253,
      "learning_rate": 4.945284841282988e-05,
      "loss": 1.5323,
      "num_input_tokens_seen": 1324656,
      "step": 260,
      "train_runtime": 901.8291,
      "train_tokens_per_second": 1468.855
    },
    {
      "epoch": 0.20483091787439614,
      "grad_norm": 1.7472833395004272,
      "learning_rate": 4.943160014178686e-05,
      "loss": 1.8399,
      "num_input_tokens_seen": 1350144,
      "step": 265,
      "train_runtime": 919.3423,
      "train_tokens_per_second": 1468.598
    },
    {
      "epoch": 0.20869565217391303,
      "grad_norm": 1.759986162185669,
      "learning_rate": 4.9409951852304334e-05,
      "loss": 1.6921,
      "num_input_tokens_seen": 1375984,
      "step": 270,
      "train_runtime": 937.0942,
      "train_tokens_per_second": 1468.352
    },
    {
      "epoch": 0.21256038647342995,
      "grad_norm": 1.7227445840835571,
      "learning_rate": 4.938790389882961e-05,
      "loss": 1.7681,
      "num_input_tokens_seen": 1402128,
      "step": 275,
      "train_runtime": 955.2195,
      "train_tokens_per_second": 1467.86
    },
    {
      "epoch": 0.21642512077294687,
      "grad_norm": 1.7661634683609009,
      "learning_rate": 4.936545664235368e-05,
      "loss": 1.4287,
      "num_input_tokens_seen": 1430352,
      "step": 280,
      "train_runtime": 974.0021,
      "train_tokens_per_second": 1468.531
    },
    {
      "epoch": 0.22028985507246376,
      "grad_norm": 1.523392915725708,
      "learning_rate": 4.9342610450405357e-05,
      "loss": 1.6306,
      "num_input_tokens_seen": 1459824,
      "step": 285,
      "train_runtime": 993.7813,
      "train_tokens_per_second": 1468.959
    },
    {
      "epoch": 0.22415458937198068,
      "grad_norm": 1.848996877670288,
      "learning_rate": 4.9319365697045196e-05,
      "loss": 2.0344,
      "num_input_tokens_seen": 1480064,
      "step": 290,
      "train_runtime": 1009.2664,
      "train_tokens_per_second": 1466.475
    },
    {
      "epoch": 0.22801932367149758,
      "grad_norm": 1.4215352535247803,
      "learning_rate": 4.9295722762859383e-05,
      "loss": 1.7694,
      "num_input_tokens_seen": 1508112,
      "step": 295,
      "train_runtime": 1027.7505,
      "train_tokens_per_second": 1467.391
    },
    {
      "epoch": 0.2318840579710145,
      "grad_norm": 1.6761140823364258,
      "learning_rate": 4.927168203495356e-05,
      "loss": 1.6465,
      "num_input_tokens_seen": 1530496,
      "step": 300,
      "train_runtime": 1044.0477,
      "train_tokens_per_second": 1465.925
    },
    {
      "epoch": 0.2357487922705314,
      "grad_norm": 1.6413472890853882,
      "learning_rate": 4.92472439069464e-05,
      "loss": 2.1047,
      "num_input_tokens_seen": 1555952,
      "step": 305,
      "train_runtime": 1061.7814,
      "train_tokens_per_second": 1465.416
    },
    {
      "epoch": 0.2396135265700483,
      "grad_norm": 1.5453693866729736,
      "learning_rate": 4.922240877896324e-05,
      "loss": 1.7219,
      "num_input_tokens_seen": 1577744,
      "step": 310,
      "train_runtime": 1077.6101,
      "train_tokens_per_second": 1464.114
    },
    {
      "epoch": 0.24347826086956523,
      "grad_norm": 1.38131844997406,
      "learning_rate": 4.919717705762946e-05,
      "loss": 1.7493,
      "num_input_tokens_seen": 1603152,
      "step": 315,
      "train_runtime": 1095.3916,
      "train_tokens_per_second": 1463.542
    },
    {
      "epoch": 0.24734299516908212,
      "grad_norm": 1.558889627456665,
      "learning_rate": 4.91715491560639e-05,
      "loss": 1.4871,
      "num_input_tokens_seen": 1628256,
      "step": 320,
      "train_runtime": 1112.3604,
      "train_tokens_per_second": 1463.785
    },
    {
      "epoch": 0.25120772946859904,
      "grad_norm": 1.3277958631515503,
      "learning_rate": 4.9145525493872014e-05,
      "loss": 1.679,
      "num_input_tokens_seen": 1651664,
      "step": 325,
      "train_runtime": 1128.9545,
      "train_tokens_per_second": 1463.003
    },
    {
      "epoch": 0.25507246376811593,
      "grad_norm": 1.4309823513031006,
      "learning_rate": 4.911910649713907e-05,
      "loss": 1.6195,
      "num_input_tokens_seen": 1677152,
      "step": 330,
      "train_runtime": 1146.5689,
      "train_tokens_per_second": 1462.757
    },
    {
      "epoch": 0.2589371980676328,
      "grad_norm": 1.6540316343307495,
      "learning_rate": 4.909229259842315e-05,
      "loss": 1.5078,
      "num_input_tokens_seen": 1699200,
      "step": 335,
      "train_runtime": 1162.3605,
      "train_tokens_per_second": 1461.853
    },
    {
      "epoch": 0.26280193236714977,
      "grad_norm": 1.1981468200683594,
      "learning_rate": 4.9065084236748035e-05,
      "loss": 1.7342,
      "num_input_tokens_seen": 1720976,
      "step": 340,
      "train_runtime": 1177.9332,
      "train_tokens_per_second": 1461.013
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.059576988220215,
      "learning_rate": 4.903748185759609e-05,
      "loss": 2.0593,
      "num_input_tokens_seen": 1744544,
      "step": 345,
      "train_runtime": 1194.7188,
      "train_tokens_per_second": 1460.213
    },
    {
      "epoch": 0.27053140096618356,
      "grad_norm": 1.3334516286849976,
      "learning_rate": 4.900948591290088e-05,
      "loss": 1.4419,
      "num_input_tokens_seen": 1769920,
      "step": 350,
      "train_runtime": 1212.3442,
      "train_tokens_per_second": 1459.915
    },
    {
      "epoch": 0.2743961352657005,
      "grad_norm": 1.4971750974655151,
      "learning_rate": 4.898109686103983e-05,
      "loss": 1.6392,
      "num_input_tokens_seen": 1797760,
      "step": 355,
      "train_runtime": 1230.6436,
      "train_tokens_per_second": 1460.829
    },
    {
      "epoch": 0.2782608695652174,
      "grad_norm": 1.4517244100570679,
      "learning_rate": 4.8952315166826734e-05,
      "loss": 1.6408,
      "num_input_tokens_seen": 1821776,
      "step": 360,
      "train_runtime": 1247.5474,
      "train_tokens_per_second": 1460.286
    },
    {
      "epoch": 0.2821256038647343,
      "grad_norm": 1.9963877201080322,
      "learning_rate": 4.8923141301504086e-05,
      "loss": 1.5985,
      "num_input_tokens_seen": 1854112,
      "step": 365,
      "train_runtime": 1268.4227,
      "train_tokens_per_second": 1461.746
    },
    {
      "epoch": 0.28599033816425123,
      "grad_norm": 2.02428936958313,
      "learning_rate": 4.8893575742735396e-05,
      "loss": 1.7182,
      "num_input_tokens_seen": 1878080,
      "step": 370,
      "train_runtime": 1285.1648,
      "train_tokens_per_second": 1461.353
    },
    {
      "epoch": 0.2898550724637681,
      "grad_norm": 1.6807284355163574,
      "learning_rate": 4.886361897459739e-05,
      "loss": 1.7712,
      "num_input_tokens_seen": 1904688,
      "step": 375,
      "train_runtime": 1303.3443,
      "train_tokens_per_second": 1461.385
    },
    {
      "epoch": 0.293719806763285,
      "grad_norm": 1.8139766454696655,
      "learning_rate": 4.883327148757204e-05,
      "loss": 1.4844,
      "num_input_tokens_seen": 1929552,
      "step": 380,
      "train_runtime": 1320.829,
      "train_tokens_per_second": 1460.864
    },
    {
      "epoch": 0.2975845410628019,
      "grad_norm": 1.2758764028549194,
      "learning_rate": 4.8802533778538566e-05,
      "loss": 1.8397,
      "num_input_tokens_seen": 1955056,
      "step": 385,
      "train_runtime": 1338.7057,
      "train_tokens_per_second": 1460.408
    },
    {
      "epoch": 0.30144927536231886,
      "grad_norm": 2.116442918777466,
      "learning_rate": 4.877140635076529e-05,
      "loss": 1.9067,
      "num_input_tokens_seen": 1981312,
      "step": 390,
      "train_runtime": 1356.9068,
      "train_tokens_per_second": 1460.168
    },
    {
      "epoch": 0.30531400966183575,
      "grad_norm": 1.495652437210083,
      "learning_rate": 4.8739889713901405e-05,
      "loss": 1.5078,
      "num_input_tokens_seen": 2007968,
      "step": 395,
      "train_runtime": 1375.0008,
      "train_tokens_per_second": 1460.34
    },
    {
      "epoch": 0.30917874396135264,
      "grad_norm": 1.6073017120361328,
      "learning_rate": 4.87079843839686e-05,
      "loss": 1.8769,
      "num_input_tokens_seen": 2028944,
      "step": 400,
      "train_runtime": 1390.4378,
      "train_tokens_per_second": 1459.212
    },
    {
      "epoch": 0.3130434782608696,
      "grad_norm": 1.5958303213119507,
      "learning_rate": 4.8675690883352654e-05,
      "loss": 1.3969,
      "num_input_tokens_seen": 2056592,
      "step": 405,
      "train_runtime": 1409.0789,
      "train_tokens_per_second": 1459.529
    },
    {
      "epoch": 0.3169082125603865,
      "grad_norm": 1.3568402528762817,
      "learning_rate": 4.8643009740794856e-05,
      "loss": 1.5483,
      "num_input_tokens_seen": 2080912,
      "step": 410,
      "train_runtime": 1426.095,
      "train_tokens_per_second": 1459.168
    },
    {
      "epoch": 0.3207729468599034,
      "grad_norm": 1.612930417060852,
      "learning_rate": 4.860994149138335e-05,
      "loss": 1.5186,
      "num_input_tokens_seen": 2108592,
      "step": 415,
      "train_runtime": 1444.6727,
      "train_tokens_per_second": 1459.564
    },
    {
      "epoch": 0.32463768115942027,
      "grad_norm": 1.9070903062820435,
      "learning_rate": 4.857648667654438e-05,
      "loss": 1.4508,
      "num_input_tokens_seen": 2136144,
      "step": 420,
      "train_runtime": 1463.1074,
      "train_tokens_per_second": 1460.005
    },
    {
      "epoch": 0.3285024154589372,
      "grad_norm": 2.059037923812866,
      "learning_rate": 4.854264584403343e-05,
      "loss": 1.5427,
      "num_input_tokens_seen": 2160576,
      "step": 425,
      "train_runtime": 1480.152,
      "train_tokens_per_second": 1459.699
    },
    {
      "epoch": 0.3323671497584541,
      "grad_norm": 2.0386252403259277,
      "learning_rate": 4.850841954792624e-05,
      "loss": 1.3394,
      "num_input_tokens_seen": 2194416,
      "step": 430,
      "train_runtime": 1501.8279,
      "train_tokens_per_second": 1461.163
    },
    {
      "epoch": 0.336231884057971,
      "grad_norm": 1.2502504587173462,
      "learning_rate": 4.847380834860974e-05,
      "loss": 1.1452,
      "num_input_tokens_seen": 2220944,
      "step": 435,
      "train_runtime": 1519.8996,
      "train_tokens_per_second": 1461.244
    },
    {
      "epoch": 0.34009661835748795,
      "grad_norm": 1.683161973953247,
      "learning_rate": 4.843881281277289e-05,
      "loss": 1.8626,
      "num_input_tokens_seen": 2246208,
      "step": 440,
      "train_runtime": 1537.3113,
      "train_tokens_per_second": 1461.128
    },
    {
      "epoch": 0.34396135265700484,
      "grad_norm": 2.3034021854400635,
      "learning_rate": 4.84034335133974e-05,
      "loss": 1.7978,
      "num_input_tokens_seen": 2269632,
      "step": 445,
      "train_runtime": 1553.9237,
      "train_tokens_per_second": 1460.581
    },
    {
      "epoch": 0.34782608695652173,
      "grad_norm": 2.3737595081329346,
      "learning_rate": 4.8367671029748294e-05,
      "loss": 1.3782,
      "num_input_tokens_seen": 2300256,
      "step": 450,
      "train_runtime": 1573.7825,
      "train_tokens_per_second": 1461.61
    },
    {
      "epoch": 0.3516908212560386,
      "grad_norm": 2.1016845703125,
      "learning_rate": 4.833152594736451e-05,
      "loss": 1.7073,
      "num_input_tokens_seen": 2328976,
      "step": 455,
      "train_runtime": 1593.2565,
      "train_tokens_per_second": 1461.771
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.9470826387405396,
      "learning_rate": 4.829499885804927e-05,
      "loss": 1.5115,
      "num_input_tokens_seen": 2357248,
      "step": 460,
      "train_runtime": 1612.0516,
      "train_tokens_per_second": 1462.266
    },
    {
      "epoch": 0.35942028985507246,
      "grad_norm": 1.9891059398651123,
      "learning_rate": 4.825809035986037e-05,
      "loss": 1.6964,
      "num_input_tokens_seen": 2380176,
      "step": 465,
      "train_runtime": 1628.4355,
      "train_tokens_per_second": 1461.634
    },
    {
      "epoch": 0.36328502415458935,
      "grad_norm": 1.7362594604492188,
      "learning_rate": 4.822080105710043e-05,
      "loss": 1.7301,
      "num_input_tokens_seen": 2399968,
      "step": 470,
      "train_runtime": 1643.0211,
      "train_tokens_per_second": 1460.704
    },
    {
      "epoch": 0.3671497584541063,
      "grad_norm": 1.6288307905197144,
      "learning_rate": 4.8183131560306976e-05,
      "loss": 1.5337,
      "num_input_tokens_seen": 2428880,
      "step": 475,
      "train_runtime": 1662.246,
      "train_tokens_per_second": 1461.204
    },
    {
      "epoch": 0.3710144927536232,
      "grad_norm": 1.6216063499450684,
      "learning_rate": 4.8145082486242446e-05,
      "loss": 1.5954,
      "num_input_tokens_seen": 2450976,
      "step": 480,
      "train_runtime": 1678.0164,
      "train_tokens_per_second": 1460.639
    },
    {
      "epoch": 0.3748792270531401,
      "grad_norm": 1.6304829120635986,
      "learning_rate": 4.81066544578841e-05,
      "loss": 1.3825,
      "num_input_tokens_seen": 2476880,
      "step": 485,
      "train_runtime": 1695.6579,
      "train_tokens_per_second": 1460.719
    },
    {
      "epoch": 0.37874396135265703,
      "grad_norm": 1.1460140943527222,
      "learning_rate": 4.806784810441382e-05,
      "loss": 1.3837,
      "num_input_tokens_seen": 2504800,
      "step": 490,
      "train_runtime": 1714.001,
      "train_tokens_per_second": 1461.376
    },
    {
      "epoch": 0.3826086956521739,
      "grad_norm": 1.9961591958999634,
      "learning_rate": 4.8028664061207764e-05,
      "loss": 1.3528,
      "num_input_tokens_seen": 2531584,
      "step": 495,
      "train_runtime": 1731.9918,
      "train_tokens_per_second": 1461.66
    },
    {
      "epoch": 0.3864734299516908,
      "grad_norm": 1.952968955039978,
      "learning_rate": 4.798910296982606e-05,
      "loss": 1.7386,
      "num_input_tokens_seen": 2556960,
      "step": 500,
      "train_runtime": 1749.4847,
      "train_tokens_per_second": 1461.55
    },
    {
      "epoch": 0.3903381642512077,
      "grad_norm": 1.8036720752716064,
      "learning_rate": 4.79491654780022e-05,
      "loss": 1.6453,
      "num_input_tokens_seen": 2583072,
      "step": 505,
      "train_runtime": 1767.9375,
      "train_tokens_per_second": 1461.065
    },
    {
      "epoch": 0.39420289855072466,
      "grad_norm": 1.3944878578186035,
      "learning_rate": 4.7908852239632505e-05,
      "loss": 1.9343,
      "num_input_tokens_seen": 2610176,
      "step": 510,
      "train_runtime": 1786.4077,
      "train_tokens_per_second": 1461.131
    },
    {
      "epoch": 0.39806763285024155,
      "grad_norm": 1.5996222496032715,
      "learning_rate": 4.786816391476538e-05,
      "loss": 1.4994,
      "num_input_tokens_seen": 2637312,
      "step": 515,
      "train_runtime": 1804.537,
      "train_tokens_per_second": 1461.49
    },
    {
      "epoch": 0.40193236714975844,
      "grad_norm": 1.6936695575714111,
      "learning_rate": 4.78271011695905e-05,
      "loss": 1.6767,
      "num_input_tokens_seen": 2663360,
      "step": 520,
      "train_runtime": 1822.8758,
      "train_tokens_per_second": 1461.076
    },
    {
      "epoch": 0.4057971014492754,
      "grad_norm": 2.0307183265686035,
      "learning_rate": 4.7785664676427956e-05,
      "loss": 1.6184,
      "num_input_tokens_seen": 2689008,
      "step": 525,
      "train_runtime": 1840.647,
      "train_tokens_per_second": 1460.904
    },
    {
      "epoch": 0.4096618357487923,
      "grad_norm": 1.6474756002426147,
      "learning_rate": 4.7743855113717194e-05,
      "loss": 1.5209,
      "num_input_tokens_seen": 2717792,
      "step": 530,
      "train_runtime": 1859.7474,
      "train_tokens_per_second": 1461.377
    },
    {
      "epoch": 0.41352657004830917,
      "grad_norm": 1.7921828031539917,
      "learning_rate": 4.770167316600593e-05,
      "loss": 1.6116,
      "num_input_tokens_seen": 2744976,
      "step": 535,
      "train_runtime": 1878.4648,
      "train_tokens_per_second": 1461.287
    },
    {
      "epoch": 0.41739130434782606,
      "grad_norm": 2.056284189224243,
      "learning_rate": 4.7659119523938936e-05,
      "loss": 1.6554,
      "num_input_tokens_seen": 2771376,
      "step": 540,
      "train_runtime": 1896.3087,
      "train_tokens_per_second": 1461.458
    },
    {
      "epoch": 0.421256038647343,
      "grad_norm": 1.474610447883606,
      "learning_rate": 4.761619488424672e-05,
      "loss": 1.0694,
      "num_input_tokens_seen": 2804560,
      "step": 545,
      "train_runtime": 1917.7127,
      "train_tokens_per_second": 1462.451
    },
    {
      "epoch": 0.4251207729468599,
      "grad_norm": 2.471613645553589,
      "learning_rate": 4.757289994973417e-05,
      "loss": 1.4463,
      "num_input_tokens_seen": 2839456,
      "step": 550,
      "train_runtime": 1939.8048,
      "train_tokens_per_second": 1463.784
    },
    {
      "epoch": 0.4289855072463768,
      "grad_norm": 1.5076833963394165,
      "learning_rate": 4.752923542926896e-05,
      "loss": 1.2995,
      "num_input_tokens_seen": 2867952,
      "step": 555,
      "train_runtime": 1958.6524,
      "train_tokens_per_second": 1464.248
    },
    {
      "epoch": 0.43285024154589374,
      "grad_norm": 2.0755441188812256,
      "learning_rate": 4.748520203777003e-05,
      "loss": 1.8469,
      "num_input_tokens_seen": 2891808,
      "step": 560,
      "train_runtime": 1975.5156,
      "train_tokens_per_second": 1463.824
    },
    {
      "epoch": 0.43671497584541064,
      "grad_norm": 2.020674228668213,
      "learning_rate": 4.744080049619582e-05,
      "loss": 1.7679,
      "num_input_tokens_seen": 2916384,
      "step": 565,
      "train_runtime": 1992.8238,
      "train_tokens_per_second": 1463.443
    },
    {
      "epoch": 0.4405797101449275,
      "grad_norm": 2.1564598083496094,
      "learning_rate": 4.73960315315325e-05,
      "loss": 1.5461,
      "num_input_tokens_seen": 2937328,
      "step": 570,
      "train_runtime": 2008.2868,
      "train_tokens_per_second": 1462.604
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.7150639295578003,
      "learning_rate": 4.7350895876782066e-05,
      "loss": 1.8573,
      "num_input_tokens_seen": 2962000,
      "step": 575,
      "train_runtime": 2025.7538,
      "train_tokens_per_second": 1462.172
    },
    {
      "epoch": 0.44830917874396137,
      "grad_norm": 2.288722276687622,
      "learning_rate": 4.73053942709503e-05,
      "loss": 1.4282,
      "num_input_tokens_seen": 2989952,
      "step": 580,
      "train_runtime": 2045.0056,
      "train_tokens_per_second": 1462.075
    },
    {
      "epoch": 0.45217391304347826,
      "grad_norm": 2.457829475402832,
      "learning_rate": 4.725952745903474e-05,
      "loss": 1.6687,
      "num_input_tokens_seen": 3015072,
      "step": 585,
      "train_runtime": 2063.1078,
      "train_tokens_per_second": 1461.422
    },
    {
      "epoch": 0.45603864734299515,
      "grad_norm": 2.5438199043273926,
      "learning_rate": 4.721329619201238e-05,
      "loss": 1.7393,
      "num_input_tokens_seen": 3041120,
      "step": 590,
      "train_runtime": 2081.4238,
      "train_tokens_per_second": 1461.077
    },
    {
      "epoch": 0.4599033816425121,
      "grad_norm": 1.6716009378433228,
      "learning_rate": 4.7166701226827495e-05,
      "loss": 1.6126,
      "num_input_tokens_seen": 3071344,
      "step": 595,
      "train_runtime": 2101.7482,
      "train_tokens_per_second": 1461.328
    },
    {
      "epoch": 0.463768115942029,
      "grad_norm": 1.881467580795288,
      "learning_rate": 4.7119743326379175e-05,
      "loss": 1.6233,
      "num_input_tokens_seen": 3094672,
      "step": 600,
      "train_runtime": 2119.0468,
      "train_tokens_per_second": 1460.408
    },
    {
      "epoch": 0.4676328502415459,
      "grad_norm": 1.58624267578125,
      "learning_rate": 4.7072423259508814e-05,
      "loss": 1.4474,
      "num_input_tokens_seen": 3118656,
      "step": 605,
      "train_runtime": 2137.5779,
      "train_tokens_per_second": 1458.967
    },
    {
      "epoch": 0.4714975845410628,
      "grad_norm": 2.487788438796997,
      "learning_rate": 4.702474180098758e-05,
      "loss": 1.3311,
      "num_input_tokens_seen": 3148208,
      "step": 610,
      "train_runtime": 2158.1356,
      "train_tokens_per_second": 1458.763
    },
    {
      "epoch": 0.4753623188405797,
      "grad_norm": 1.431973934173584,
      "learning_rate": 4.6976699731503716e-05,
      "loss": 1.2148,
      "num_input_tokens_seen": 3173936,
      "step": 615,
      "train_runtime": 2183.0477,
      "train_tokens_per_second": 1453.901
    },
    {
      "epoch": 0.4792270531400966,
      "grad_norm": 3.0885307788848877,
      "learning_rate": 4.692829783764973e-05,
      "loss": 1.5333,
      "num_input_tokens_seen": 3198800,
      "step": 620,
      "train_runtime": 2206.7041,
      "train_tokens_per_second": 1449.583
    },
    {
      "epoch": 0.4830917874396135,
      "grad_norm": 1.8130557537078857,
      "learning_rate": 4.6879536911909514e-05,
      "loss": 1.4907,
      "num_input_tokens_seen": 3224096,
      "step": 625,
      "train_runtime": 2224.0313,
      "train_tokens_per_second": 1449.663
    },
    {
      "epoch": 0.48695652173913045,
      "grad_norm": 1.5093733072280884,
      "learning_rate": 4.683041775264542e-05,
      "loss": 1.1153,
      "num_input_tokens_seen": 3253504,
      "step": 630,
      "train_runtime": 2243.235,
      "train_tokens_per_second": 1450.363
    },
    {
      "epoch": 0.49082125603864735,
      "grad_norm": 2.021561861038208,
      "learning_rate": 4.6780941164085146e-05,
      "loss": 1.8125,
      "num_input_tokens_seen": 3278880,
      "step": 635,
      "train_runtime": 2260.6727,
      "train_tokens_per_second": 1450.4
    },
    {
      "epoch": 0.49468599033816424,
      "grad_norm": 1.932975172996521,
      "learning_rate": 4.6731107956308576e-05,
      "loss": 1.7564,
      "num_input_tokens_seen": 3304080,
      "step": 640,
      "train_runtime": 2278.0859,
      "train_tokens_per_second": 1450.375
    },
    {
      "epoch": 0.4985507246376812,
      "grad_norm": 1.8127434253692627,
      "learning_rate": 4.6680918945234496e-05,
      "loss": 1.7468,
      "num_input_tokens_seen": 3331312,
      "step": 645,
      "train_runtime": 2296.5603,
      "train_tokens_per_second": 1450.566
    },
    {
      "epoch": 0.5024154589371981,
      "grad_norm": 1.8401172161102295,
      "learning_rate": 4.663037495260729e-05,
      "loss": 2.0722,
      "num_input_tokens_seen": 3351488,
      "step": 650,
      "train_runtime": 2311.4692,
      "train_tokens_per_second": 1449.938
    },
    {
      "epoch": 0.506280193236715,
      "grad_norm": 1.3017724752426147,
      "learning_rate": 4.6579476805983445e-05,
      "loss": 1.4161,
      "num_input_tokens_seen": 3377136,
      "step": 655,
      "train_runtime": 2329.0862,
      "train_tokens_per_second": 1449.983
    },
    {
      "epoch": 0.5101449275362319,
      "grad_norm": 1.5665727853775024,
      "learning_rate": 4.6528225338718e-05,
      "loss": 1.3642,
      "num_input_tokens_seen": 3401696,
      "step": 660,
      "train_runtime": 2346.3914,
      "train_tokens_per_second": 1449.756
    },
    {
      "epoch": 0.5140096618357488,
      "grad_norm": 2.72639536857605,
      "learning_rate": 4.6476621389950915e-05,
      "loss": 1.9097,
      "num_input_tokens_seen": 3421616,
      "step": 665,
      "train_runtime": 2361.4112,
      "train_tokens_per_second": 1448.971
    },
    {
      "epoch": 0.5178743961352656,
      "grad_norm": 1.898619294166565,
      "learning_rate": 4.642466580459332e-05,
      "loss": 1.8367,
      "num_input_tokens_seen": 3453968,
      "step": 670,
      "train_runtime": 2382.1737,
      "train_tokens_per_second": 1449.923
    },
    {
      "epoch": 0.5217391304347826,
      "grad_norm": 1.6700752973556519,
      "learning_rate": 4.6372359433313695e-05,
      "loss": 1.671,
      "num_input_tokens_seen": 3482880,
      "step": 675,
      "train_runtime": 2401.6131,
      "train_tokens_per_second": 1450.225
    },
    {
      "epoch": 0.5256038647342995,
      "grad_norm": 3.0632104873657227,
      "learning_rate": 4.6319703132523954e-05,
      "loss": 1.912,
      "num_input_tokens_seen": 3507152,
      "step": 680,
      "train_runtime": 2418.7075,
      "train_tokens_per_second": 1450.011
    },
    {
      "epoch": 0.5294685990338164,
      "grad_norm": 2.1977574825286865,
      "learning_rate": 4.626669776436537e-05,
      "loss": 1.5996,
      "num_input_tokens_seen": 3529360,
      "step": 685,
      "train_runtime": 2434.6787,
      "train_tokens_per_second": 1449.62
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.8763611316680908,
      "learning_rate": 4.621334419669452e-05,
      "loss": 1.736,
      "num_input_tokens_seen": 3549168,
      "step": 690,
      "train_runtime": 2451.7311,
      "train_tokens_per_second": 1447.617
    },
    {
      "epoch": 0.5371980676328503,
      "grad_norm": 2.169464349746704,
      "learning_rate": 4.6159643303069034e-05,
      "loss": 1.6821,
      "num_input_tokens_seen": 3572304,
      "step": 695,
      "train_runtime": 2469.5646,
      "train_tokens_per_second": 1446.532
    },
    {
      "epoch": 0.5410628019323671,
      "grad_norm": 1.5526894330978394,
      "learning_rate": 4.6105595962733325e-05,
      "loss": 1.554,
      "num_input_tokens_seen": 3600256,
      "step": 700,
      "train_runtime": 2490.0388,
      "train_tokens_per_second": 1445.863
    },
    {
      "epoch": 0.5449275362318841,
      "grad_norm": 1.8858509063720703,
      "learning_rate": 4.6051203060604156e-05,
      "loss": 1.6636,
      "num_input_tokens_seen": 3627296,
      "step": 705,
      "train_runtime": 2508.8288,
      "train_tokens_per_second": 1445.812
    },
    {
      "epoch": 0.548792270531401,
      "grad_norm": 2.197615385055542,
      "learning_rate": 4.599646548725618e-05,
      "loss": 1.5502,
      "num_input_tokens_seen": 3656048,
      "step": 710,
      "train_runtime": 2527.9949,
      "train_tokens_per_second": 1446.224
    },
    {
      "epoch": 0.5526570048309178,
      "grad_norm": 1.9746198654174805,
      "learning_rate": 4.594138413890736e-05,
      "loss": 1.4389,
      "num_input_tokens_seen": 3680688,
      "step": 715,
      "train_runtime": 2545.0459,
      "train_tokens_per_second": 1446.217
    },
    {
      "epoch": 0.5565217391304348,
      "grad_norm": 1.8126899003982544,
      "learning_rate": 4.5885959917404264e-05,
      "loss": 1.3894,
      "num_input_tokens_seen": 3704928,
      "step": 720,
      "train_runtime": 2561.703,
      "train_tokens_per_second": 1446.275
    },
    {
      "epoch": 0.5603864734299517,
      "grad_norm": 1.4570505619049072,
      "learning_rate": 4.583019373020734e-05,
      "loss": 1.8154,
      "num_input_tokens_seen": 3727712,
      "step": 725,
      "train_runtime": 2577.969,
      "train_tokens_per_second": 1445.988
    },
    {
      "epoch": 0.5642512077294686,
      "grad_norm": 1.3271026611328125,
      "learning_rate": 4.577408649037603e-05,
      "loss": 1.2456,
      "num_input_tokens_seen": 3755824,
      "step": 730,
      "train_runtime": 2596.4846,
      "train_tokens_per_second": 1446.503
    },
    {
      "epoch": 0.5681159420289855,
      "grad_norm": 1.460919737815857,
      "learning_rate": 4.571763911655381e-05,
      "loss": 1.4994,
      "num_input_tokens_seen": 3780560,
      "step": 735,
      "train_runtime": 2613.6152,
      "train_tokens_per_second": 1446.487
    },
    {
      "epoch": 0.5719806763285025,
      "grad_norm": 1.834293246269226,
      "learning_rate": 4.5660852532953216e-05,
      "loss": 1.6176,
      "num_input_tokens_seen": 3803184,
      "step": 740,
      "train_runtime": 2629.4845,
      "train_tokens_per_second": 1446.361
    },
    {
      "epoch": 0.5758454106280193,
      "grad_norm": 2.196368455886841,
      "learning_rate": 4.5603727669340606e-05,
      "loss": 1.3931,
      "num_input_tokens_seen": 3828832,
      "step": 745,
      "train_runtime": 2647.1749,
      "train_tokens_per_second": 1446.384
    },
    {
      "epoch": 0.5797101449275363,
      "grad_norm": 1.6500718593597412,
      "learning_rate": 4.5546265461021045e-05,
      "loss": 1.4656,
      "num_input_tokens_seen": 3855408,
      "step": 750,
      "train_runtime": 2664.7971,
      "train_tokens_per_second": 1446.792
    },
    {
      "epoch": 0.5835748792270531,
      "grad_norm": 2.6740944385528564,
      "learning_rate": 4.5488466848822894e-05,
      "loss": 1.5694,
      "num_input_tokens_seen": 3881632,
      "step": 755,
      "train_runtime": 2682.5489,
      "train_tokens_per_second": 1446.994
    },
    {
      "epoch": 0.58743961352657,
      "grad_norm": 1.9390367269515991,
      "learning_rate": 4.54303327790825e-05,
      "loss": 1.5769,
      "num_input_tokens_seen": 3909728,
      "step": 760,
      "train_runtime": 2701.3767,
      "train_tokens_per_second": 1447.309
    },
    {
      "epoch": 0.591304347826087,
      "grad_norm": 2.2583930492401123,
      "learning_rate": 4.5371864203628604e-05,
      "loss": 1.9009,
      "num_input_tokens_seen": 3933376,
      "step": 765,
      "train_runtime": 2717.9642,
      "train_tokens_per_second": 1447.177
    },
    {
      "epoch": 0.5951690821256038,
      "grad_norm": 1.9840158224105835,
      "learning_rate": 4.531306207976687e-05,
      "loss": 1.4383,
      "num_input_tokens_seen": 3959840,
      "step": 770,
      "train_runtime": 2736.0596,
      "train_tokens_per_second": 1447.278
    },
    {
      "epoch": 0.5990338164251208,
      "grad_norm": 2.3712034225463867,
      "learning_rate": 4.525392737026407e-05,
      "loss": 1.2766,
      "num_input_tokens_seen": 3987456,
      "step": 775,
      "train_runtime": 2754.3989,
      "train_tokens_per_second": 1447.668
    },
    {
      "epoch": 0.6028985507246377,
      "grad_norm": 2.0899624824523926,
      "learning_rate": 4.519446104333248e-05,
      "loss": 1.5826,
      "num_input_tokens_seen": 4011984,
      "step": 780,
      "train_runtime": 2771.4,
      "train_tokens_per_second": 1447.638
    },
    {
      "epoch": 0.6067632850241546,
      "grad_norm": 1.7514920234680176,
      "learning_rate": 4.513466407261388e-05,
      "loss": 1.316,
      "num_input_tokens_seen": 4035568,
      "step": 785,
      "train_runtime": 2787.6393,
      "train_tokens_per_second": 1447.665
    },
    {
      "epoch": 0.6106280193236715,
      "grad_norm": 1.4961190223693848,
      "learning_rate": 4.507453743716372e-05,
      "loss": 1.6125,
      "num_input_tokens_seen": 4056592,
      "step": 790,
      "train_runtime": 2802.8015,
      "train_tokens_per_second": 1447.335
    },
    {
      "epoch": 0.6144927536231884,
      "grad_norm": 1.9658914804458618,
      "learning_rate": 4.501408212143502e-05,
      "loss": 1.5317,
      "num_input_tokens_seen": 4082864,
      "step": 795,
      "train_runtime": 2820.3954,
      "train_tokens_per_second": 1447.621
    },
    {
      "epoch": 0.6183574879227053,
      "grad_norm": 2.434882879257202,
      "learning_rate": 4.495329911526232e-05,
      "loss": 1.7109,
      "num_input_tokens_seen": 4100896,
      "step": 800,
      "train_runtime": 2834.2532,
      "train_tokens_per_second": 1446.905
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 2.0602872371673584,
      "learning_rate": 4.489218941384537e-05,
      "loss": 1.441,
      "num_input_tokens_seen": 4129504,
      "step": 805,
      "train_runtime": 2853.5364,
      "train_tokens_per_second": 1447.153
    },
    {
      "epoch": 0.6260869565217392,
      "grad_norm": 1.815108060836792,
      "learning_rate": 4.4830754017732965e-05,
      "loss": 1.3226,
      "num_input_tokens_seen": 4150608,
      "step": 810,
      "train_runtime": 2868.6791,
      "train_tokens_per_second": 1446.871
    },
    {
      "epoch": 0.629951690821256,
      "grad_norm": 2.1966490745544434,
      "learning_rate": 4.476899393280647e-05,
      "loss": 1.7669,
      "num_input_tokens_seen": 4174208,
      "step": 815,
      "train_runtime": 2885.3991,
      "train_tokens_per_second": 1446.666
    },
    {
      "epoch": 0.633816425120773,
      "grad_norm": 1.6734942197799683,
      "learning_rate": 4.470691017026336e-05,
      "loss": 1.7791,
      "num_input_tokens_seen": 4201840,
      "step": 820,
      "train_runtime": 2903.9465,
      "train_tokens_per_second": 1446.941
    },
    {
      "epoch": 0.6376811594202898,
      "grad_norm": 2.386258840560913,
      "learning_rate": 4.4644503746600716e-05,
      "loss": 1.6083,
      "num_input_tokens_seen": 4224288,
      "step": 825,
      "train_runtime": 2919.8955,
      "train_tokens_per_second": 1446.726
    },
    {
      "epoch": 0.6415458937198067,
      "grad_norm": 2.2794127464294434,
      "learning_rate": 4.4581775683598515e-05,
      "loss": 1.381,
      "num_input_tokens_seen": 4249280,
      "step": 830,
      "train_runtime": 2936.9548,
      "train_tokens_per_second": 1446.832
    },
    {
      "epoch": 0.6454106280193237,
      "grad_norm": 2.8486504554748535,
      "learning_rate": 4.4518727008302954e-05,
      "loss": 1.4217,
      "num_input_tokens_seen": 4272848,
      "step": 835,
      "train_runtime": 2953.4696,
      "train_tokens_per_second": 1446.721
    },
    {
      "epoch": 0.6492753623188405,
      "grad_norm": 2.0820517539978027,
      "learning_rate": 4.445535875300962e-05,
      "loss": 1.4638,
      "num_input_tokens_seen": 4298080,
      "step": 840,
      "train_runtime": 2970.9224,
      "train_tokens_per_second": 1446.716
    },
    {
      "epoch": 0.6531400966183575,
      "grad_norm": 1.6939237117767334,
      "learning_rate": 4.439167195524655e-05,
      "loss": 1.3993,
      "num_input_tokens_seen": 4322672,
      "step": 845,
      "train_runtime": 2987.7664,
      "train_tokens_per_second": 1446.79
    },
    {
      "epoch": 0.6570048309178744,
      "grad_norm": 2.229520082473755,
      "learning_rate": 4.432766765775731e-05,
      "loss": 1.824,
      "num_input_tokens_seen": 4349584,
      "step": 850,
      "train_runtime": 3006.0475,
      "train_tokens_per_second": 1446.945
    },
    {
      "epoch": 0.6608695652173913,
      "grad_norm": 2.8134703636169434,
      "learning_rate": 4.426334690848386e-05,
      "loss": 1.9334,
      "num_input_tokens_seen": 4373680,
      "step": 855,
      "train_runtime": 3022.735,
      "train_tokens_per_second": 1446.928
    },
    {
      "epoch": 0.6647342995169082,
      "grad_norm": 1.4858874082565308,
      "learning_rate": 4.4198710760549435e-05,
      "loss": 1.3226,
      "num_input_tokens_seen": 4401312,
      "step": 860,
      "train_runtime": 3041.0024,
      "train_tokens_per_second": 1447.323
    },
    {
      "epoch": 0.6685990338164252,
      "grad_norm": 2.062624216079712,
      "learning_rate": 4.413376027224129e-05,
      "loss": 1.6347,
      "num_input_tokens_seen": 4424960,
      "step": 865,
      "train_runtime": 3057.6836,
      "train_tokens_per_second": 1447.161
    },
    {
      "epoch": 0.672463768115942,
      "grad_norm": 2.3089938163757324,
      "learning_rate": 4.4068496506993375e-05,
      "loss": 1.8404,
      "num_input_tokens_seen": 4444368,
      "step": 870,
      "train_runtime": 3072.8596,
      "train_tokens_per_second": 1446.33
    },
    {
      "epoch": 0.6763285024154589,
      "grad_norm": 1.9912558794021606,
      "learning_rate": 4.400292053336892e-05,
      "loss": 1.5877,
      "num_input_tokens_seen": 4467696,
      "step": 875,
      "train_runtime": 3089.288,
      "train_tokens_per_second": 1446.19
    },
    {
      "epoch": 0.6801932367149759,
      "grad_norm": 1.68896484375,
      "learning_rate": 4.393703342504294e-05,
      "loss": 1.3713,
      "num_input_tokens_seen": 4495024,
      "step": 880,
      "train_runtime": 3107.9767,
      "train_tokens_per_second": 1446.286
    },
    {
      "epoch": 0.6840579710144927,
      "grad_norm": 2.080686092376709,
      "learning_rate": 4.387083626078465e-05,
      "loss": 1.5153,
      "num_input_tokens_seen": 4519552,
      "step": 885,
      "train_runtime": 3125.1953,
      "train_tokens_per_second": 1446.166
    },
    {
      "epoch": 0.6879227053140097,
      "grad_norm": 2.1809756755828857,
      "learning_rate": 4.380433012443982e-05,
      "loss": 1.9655,
      "num_input_tokens_seen": 4545472,
      "step": 890,
      "train_runtime": 3142.9186,
      "train_tokens_per_second": 1446.258
    },
    {
      "epoch": 0.6917874396135266,
      "grad_norm": 3.278411626815796,
      "learning_rate": 4.373751610491301e-05,
      "loss": 1.6522,
      "num_input_tokens_seen": 4570592,
      "step": 895,
      "train_runtime": 3160.4066,
      "train_tokens_per_second": 1446.204
    },
    {
      "epoch": 0.6956521739130435,
      "grad_norm": 2.0202910900115967,
      "learning_rate": 4.367039529614975e-05,
      "loss": 1.2513,
      "num_input_tokens_seen": 4600384,
      "step": 900,
      "train_runtime": 3180.1568,
      "train_tokens_per_second": 1446.59
    },
    {
      "epoch": 0.6995169082125604,
      "grad_norm": 1.7486929893493652,
      "learning_rate": 4.3602968797118646e-05,
      "loss": 1.575,
      "num_input_tokens_seen": 4626528,
      "step": 905,
      "train_runtime": 3198.2813,
      "train_tokens_per_second": 1446.567
    },
    {
      "epoch": 0.7033816425120772,
      "grad_norm": 2.3110768795013428,
      "learning_rate": 4.353523771179333e-05,
      "loss": 1.5339,
      "num_input_tokens_seen": 4656064,
      "step": 910,
      "train_runtime": 3217.7422,
      "train_tokens_per_second": 1446.997
    },
    {
      "epoch": 0.7072463768115942,
      "grad_norm": 2.0431506633758545,
      "learning_rate": 4.3467203149134466e-05,
      "loss": 1.4408,
      "num_input_tokens_seen": 4681360,
      "step": 915,
      "train_runtime": 3235.0979,
      "train_tokens_per_second": 1447.054
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 2.387406826019287,
      "learning_rate": 4.339886622307153e-05,
      "loss": 1.2585,
      "num_input_tokens_seen": 4708576,
      "step": 920,
      "train_runtime": 3253.376,
      "train_tokens_per_second": 1447.289
    },
    {
      "epoch": 0.714975845410628,
      "grad_norm": 1.9140821695327759,
      "learning_rate": 4.333022805248458e-05,
      "loss": 1.4254,
      "num_input_tokens_seen": 4733344,
      "step": 925,
      "train_runtime": 3270.485,
      "train_tokens_per_second": 1447.291
    },
    {
      "epoch": 0.7188405797101449,
      "grad_norm": 2.6562936305999756,
      "learning_rate": 4.326128976118598e-05,
      "loss": 1.3599,
      "num_input_tokens_seen": 4756784,
      "step": 930,
      "train_runtime": 3287.2291,
      "train_tokens_per_second": 1447.05
    },
    {
      "epoch": 0.7227053140096619,
      "grad_norm": 1.5948100090026855,
      "learning_rate": 4.319205247790194e-05,
      "loss": 1.6105,
      "num_input_tokens_seen": 4780000,
      "step": 935,
      "train_runtime": 3303.6798,
      "train_tokens_per_second": 1446.871
    },
    {
      "epoch": 0.7265700483091787,
      "grad_norm": 1.7410606145858765,
      "learning_rate": 4.312251733625409e-05,
      "loss": 1.5684,
      "num_input_tokens_seen": 4808896,
      "step": 940,
      "train_runtime": 3322.6795,
      "train_tokens_per_second": 1447.295
    },
    {
      "epoch": 0.7304347826086957,
      "grad_norm": 2.1462085247039795,
      "learning_rate": 4.305268547474085e-05,
      "loss": 1.6671,
      "num_input_tokens_seen": 4838960,
      "step": 945,
      "train_runtime": 3342.5161,
      "train_tokens_per_second": 1447.7
    },
    {
      "epoch": 0.7342995169082126,
      "grad_norm": 3.316098928451538,
      "learning_rate": 4.298255803671889e-05,
      "loss": 1.5427,
      "num_input_tokens_seen": 4861504,
      "step": 950,
      "train_runtime": 3358.5788,
      "train_tokens_per_second": 1447.488
    },
    {
      "epoch": 0.7381642512077294,
      "grad_norm": 2.471043348312378,
      "learning_rate": 4.2912136170384307e-05,
      "loss": 1.5009,
      "num_input_tokens_seen": 4886640,
      "step": 955,
      "train_runtime": 3375.9465,
      "train_tokens_per_second": 1447.487
    },
    {
      "epoch": 0.7420289855072464,
      "grad_norm": 1.7292819023132324,
      "learning_rate": 4.284142102875389e-05,
      "loss": 1.3797,
      "num_input_tokens_seen": 4914384,
      "step": 960,
      "train_runtime": 3394.3034,
      "train_tokens_per_second": 1447.833
    },
    {
      "epoch": 0.7458937198067633,
      "grad_norm": 1.568029761314392,
      "learning_rate": 4.277041376964623e-05,
      "loss": 1.3231,
      "num_input_tokens_seen": 4940288,
      "step": 965,
      "train_runtime": 3412.0054,
      "train_tokens_per_second": 1447.913
    },
    {
      "epoch": 0.7497584541062802,
      "grad_norm": 1.7139602899551392,
      "learning_rate": 4.269911555566273e-05,
      "loss": 1.467,
      "num_input_tokens_seen": 4968912,
      "step": 970,
      "train_runtime": 3431.1098,
      "train_tokens_per_second": 1448.194
    },
    {
      "epoch": 0.7536231884057971,
      "grad_norm": 1.7270585298538208,
      "learning_rate": 4.262752755416862e-05,
      "loss": 1.5811,
      "num_input_tokens_seen": 4990720,
      "step": 975,
      "train_runtime": 3446.8433,
      "train_tokens_per_second": 1447.91
    },
    {
      "epoch": 0.7574879227053141,
      "grad_norm": 1.7556490898132324,
      "learning_rate": 4.255565093727381e-05,
      "loss": 1.4168,
      "num_input_tokens_seen": 5025392,
      "step": 980,
      "train_runtime": 3468.9564,
      "train_tokens_per_second": 1448.675
    },
    {
      "epoch": 0.7613526570048309,
      "grad_norm": 2.207443952560425,
      "learning_rate": 4.248348688181371e-05,
      "loss": 1.8599,
      "num_input_tokens_seen": 5048288,
      "step": 985,
      "train_runtime": 3485.2718,
      "train_tokens_per_second": 1448.463
    },
    {
      "epoch": 0.7652173913043478,
      "grad_norm": 1.9485549926757812,
      "learning_rate": 4.2411036569329946e-05,
      "loss": 1.2478,
      "num_input_tokens_seen": 5070464,
      "step": 990,
      "train_runtime": 3501.0864,
      "train_tokens_per_second": 1448.254
    },
    {
      "epoch": 0.7690821256038647,
      "grad_norm": 2.916482448577881,
      "learning_rate": 4.2338301186051055e-05,
      "loss": 1.7238,
      "num_input_tokens_seen": 5098480,
      "step": 995,
      "train_runtime": 3520.5605,
      "train_tokens_per_second": 1448.201
    },
    {
      "epoch": 0.7729468599033816,
      "grad_norm": 1.8733539581298828,
      "learning_rate": 4.226528192287301e-05,
      "loss": 1.6268,
      "num_input_tokens_seen": 5120896,
      "step": 1000,
      "train_runtime": 3536.4002,
      "train_tokens_per_second": 1448.053
    },
    {
      "epoch": 0.7768115942028986,
      "grad_norm": 1.701277494430542,
      "learning_rate": 4.219197997533978e-05,
      "loss": 1.734,
      "num_input_tokens_seen": 5148352,
      "step": 1005,
      "train_runtime": 3554.912,
      "train_tokens_per_second": 1448.236
    },
    {
      "epoch": 0.7806763285024154,
      "grad_norm": 1.572952389717102,
      "learning_rate": 4.211839654362368e-05,
      "loss": 1.5039,
      "num_input_tokens_seen": 5175024,
      "step": 1010,
      "train_runtime": 3573.2932,
      "train_tokens_per_second": 1448.251
    },
    {
      "epoch": 0.7845410628019324,
      "grad_norm": 2.3189616203308105,
      "learning_rate": 4.20445328325058e-05,
      "loss": 1.2953,
      "num_input_tokens_seen": 5197440,
      "step": 1015,
      "train_runtime": 3589.0886,
      "train_tokens_per_second": 1448.123
    },
    {
      "epoch": 0.7884057971014493,
      "grad_norm": 1.7098301649093628,
      "learning_rate": 4.1970390051356224e-05,
      "loss": 1.1203,
      "num_input_tokens_seen": 5228448,
      "step": 1020,
      "train_runtime": 3609.2157,
      "train_tokens_per_second": 1448.638
    },
    {
      "epoch": 0.7922705314009661,
      "grad_norm": 1.7396498918533325,
      "learning_rate": 4.1895969414114254e-05,
      "loss": 1.6203,
      "num_input_tokens_seen": 5251824,
      "step": 1025,
      "train_runtime": 3625.8501,
      "train_tokens_per_second": 1448.439
    },
    {
      "epoch": 0.7961352657004831,
      "grad_norm": 2.2823383808135986,
      "learning_rate": 4.182127213926853e-05,
      "loss": 1.6231,
      "num_input_tokens_seen": 5279152,
      "step": 1030,
      "train_runtime": 3644.3606,
      "train_tokens_per_second": 1448.581
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.899058222770691,
      "learning_rate": 4.174629944983708e-05,
      "loss": 1.286,
      "num_input_tokens_seen": 5305760,
      "step": 1035,
      "train_runtime": 3662.4532,
      "train_tokens_per_second": 1448.69
    },
    {
      "epoch": 0.8038647342995169,
      "grad_norm": 2.331852912902832,
      "learning_rate": 4.167105257334728e-05,
      "loss": 1.6365,
      "num_input_tokens_seen": 5327248,
      "step": 1040,
      "train_runtime": 3678.3027,
      "train_tokens_per_second": 1448.29
    },
    {
      "epoch": 0.8077294685990338,
      "grad_norm": 1.825251817703247,
      "learning_rate": 4.159553274181578e-05,
      "loss": 1.4854,
      "num_input_tokens_seen": 5355344,
      "step": 1045,
      "train_runtime": 3696.959,
      "train_tokens_per_second": 1448.581
    },
    {
      "epoch": 0.8115942028985508,
      "grad_norm": 2.352856397628784,
      "learning_rate": 4.151974119172833e-05,
      "loss": 1.4473,
      "num_input_tokens_seen": 5387280,
      "step": 1050,
      "train_runtime": 3717.7719,
      "train_tokens_per_second": 1449.061
    },
    {
      "epoch": 0.8154589371980676,
      "grad_norm": 1.9928256273269653,
      "learning_rate": 4.14436791640195e-05,
      "loss": 1.6607,
      "num_input_tokens_seen": 5415152,
      "step": 1055,
      "train_runtime": 3736.7862,
      "train_tokens_per_second": 1449.147
    },
    {
      "epoch": 0.8193236714975846,
      "grad_norm": 2.0863845348358154,
      "learning_rate": 4.1367347904052415e-05,
      "loss": 1.4622,
      "num_input_tokens_seen": 5441984,
      "step": 1060,
      "train_runtime": 3755.1114,
      "train_tokens_per_second": 1449.22
    },
    {
      "epoch": 0.8231884057971014,
      "grad_norm": 1.9847005605697632,
      "learning_rate": 4.1290748661598325e-05,
      "loss": 1.4496,
      "num_input_tokens_seen": 5468864,
      "step": 1065,
      "train_runtime": 3773.3324,
      "train_tokens_per_second": 1449.346
    },
    {
      "epoch": 0.8270531400966183,
      "grad_norm": 2.769714832305908,
      "learning_rate": 4.121388269081615e-05,
      "loss": 1.7525,
      "num_input_tokens_seen": 5493312,
      "step": 1070,
      "train_runtime": 3790.3912,
      "train_tokens_per_second": 1449.273
    },
    {
      "epoch": 0.8309178743961353,
      "grad_norm": 1.7457101345062256,
      "learning_rate": 4.1136751250231966e-05,
      "loss": 1.6701,
      "num_input_tokens_seen": 5518544,
      "step": 1075,
      "train_runtime": 3807.5551,
      "train_tokens_per_second": 1449.367
    },
    {
      "epoch": 0.8347826086956521,
      "grad_norm": 2.136655330657959,
      "learning_rate": 4.1059355602718375e-05,
      "loss": 1.6348,
      "num_input_tokens_seen": 5539424,
      "step": 1080,
      "train_runtime": 3822.8994,
      "train_tokens_per_second": 1449.011
    },
    {
      "epoch": 0.8386473429951691,
      "grad_norm": 2.439619779586792,
      "learning_rate": 4.098169701547384e-05,
      "loss": 1.6782,
      "num_input_tokens_seen": 5563696,
      "step": 1085,
      "train_runtime": 3839.6862,
      "train_tokens_per_second": 1448.998
    },
    {
      "epoch": 0.842512077294686,
      "grad_norm": 2.328681230545044,
      "learning_rate": 4.0903776760001946e-05,
      "loss": 1.5114,
      "num_input_tokens_seen": 5585776,
      "step": 1090,
      "train_runtime": 3855.4676,
      "train_tokens_per_second": 1448.793
    },
    {
      "epoch": 0.8463768115942029,
      "grad_norm": 2.497927665710449,
      "learning_rate": 4.082559611209053e-05,
      "loss": 1.5297,
      "num_input_tokens_seen": 5611040,
      "step": 1095,
      "train_runtime": 3872.7969,
      "train_tokens_per_second": 1448.834
    },
    {
      "epoch": 0.8502415458937198,
      "grad_norm": 1.642534613609314,
      "learning_rate": 4.074715635179089e-05,
      "loss": 1.4466,
      "num_input_tokens_seen": 5640896,
      "step": 1100,
      "train_runtime": 3892.3169,
      "train_tokens_per_second": 1449.239
    },
    {
      "epoch": 0.8541062801932368,
      "grad_norm": 2.1086583137512207,
      "learning_rate": 4.0668458763396735e-05,
      "loss": 1.8495,
      "num_input_tokens_seen": 5667488,
      "step": 1105,
      "train_runtime": 3910.9071,
      "train_tokens_per_second": 1449.149
    },
    {
      "epoch": 0.8579710144927536,
      "grad_norm": 1.7117706537246704,
      "learning_rate": 4.058950463542319e-05,
      "loss": 1.616,
      "num_input_tokens_seen": 5693456,
      "step": 1110,
      "train_runtime": 3928.6738,
      "train_tokens_per_second": 1449.206
    },
    {
      "epoch": 0.8618357487922705,
      "grad_norm": 1.8964108228683472,
      "learning_rate": 4.0510295260585706e-05,
      "loss": 1.6146,
      "num_input_tokens_seen": 5720464,
      "step": 1115,
      "train_runtime": 3947.4392,
      "train_tokens_per_second": 1449.158
    },
    {
      "epoch": 0.8657004830917875,
      "grad_norm": 1.7973670959472656,
      "learning_rate": 4.0430831935778904e-05,
      "loss": 2.0874,
      "num_input_tokens_seen": 5742704,
      "step": 1120,
      "train_runtime": 3963.2706,
      "train_tokens_per_second": 1448.981
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 1.6743402481079102,
      "learning_rate": 4.035111596205531e-05,
      "loss": 1.3873,
      "num_input_tokens_seen": 5770496,
      "step": 1125,
      "train_runtime": 3981.9541,
      "train_tokens_per_second": 1449.162
    },
    {
      "epoch": 0.8734299516908213,
      "grad_norm": 2.048558473587036,
      "learning_rate": 4.0271148644604066e-05,
      "loss": 1.6945,
      "num_input_tokens_seen": 5793568,
      "step": 1130,
      "train_runtime": 3998.4431,
      "train_tokens_per_second": 1448.956
    },
    {
      "epoch": 0.8772946859903382,
      "grad_norm": 1.597010850906372,
      "learning_rate": 4.019093129272958e-05,
      "loss": 1.6338,
      "num_input_tokens_seen": 5820576,
      "step": 1135,
      "train_runtime": 4016.9252,
      "train_tokens_per_second": 1449.013
    },
    {
      "epoch": 0.881159420289855,
      "grad_norm": 1.7794618606567383,
      "learning_rate": 4.011046521983006e-05,
      "loss": 1.5215,
      "num_input_tokens_seen": 5844272,
      "step": 1140,
      "train_runtime": 4033.558,
      "train_tokens_per_second": 1448.912
    },
    {
      "epoch": 0.885024154589372,
      "grad_norm": 2.6288580894470215,
      "learning_rate": 4.002975174337602e-05,
      "loss": 1.5948,
      "num_input_tokens_seen": 5867152,
      "step": 1145,
      "train_runtime": 4050.0722,
      "train_tokens_per_second": 1448.654
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.326697587966919,
      "learning_rate": 3.9948792184888736e-05,
      "loss": 1.434,
      "num_input_tokens_seen": 5893344,
      "step": 1150,
      "train_runtime": 4067.7507,
      "train_tokens_per_second": 1448.797
    },
    {
      "epoch": 0.8927536231884058,
      "grad_norm": 1.9341121912002563,
      "learning_rate": 3.986758786991853e-05,
      "loss": 1.3173,
      "num_input_tokens_seen": 5921280,
      "step": 1155,
      "train_runtime": 4086.333,
      "train_tokens_per_second": 1449.045
    },
    {
      "epoch": 0.8966183574879227,
      "grad_norm": 2.0663468837738037,
      "learning_rate": 3.9786140128023193e-05,
      "loss": 1.3085,
      "num_input_tokens_seen": 5947056,
      "step": 1160,
      "train_runtime": 4104.117,
      "train_tokens_per_second": 1449.046
    },
    {
      "epoch": 0.9004830917874396,
      "grad_norm": 1.9963253736495972,
      "learning_rate": 3.970445029274607e-05,
      "loss": 1.4357,
      "num_input_tokens_seen": 5977728,
      "step": 1165,
      "train_runtime": 4124.0734,
      "train_tokens_per_second": 1449.472
    },
    {
      "epoch": 0.9043478260869565,
      "grad_norm": 1.6451776027679443,
      "learning_rate": 3.962251970159435e-05,
      "loss": 1.3259,
      "num_input_tokens_seen": 6002944,
      "step": 1170,
      "train_runtime": 4141.4909,
      "train_tokens_per_second": 1449.464
    },
    {
      "epoch": 0.9082125603864735,
      "grad_norm": 2.0327584743499756,
      "learning_rate": 3.9540349696017095e-05,
      "loss": 1.6079,
      "num_input_tokens_seen": 6028160,
      "step": 1175,
      "train_runtime": 4158.8404,
      "train_tokens_per_second": 1449.481
    },
    {
      "epoch": 0.9120772946859903,
      "grad_norm": 1.694486379623413,
      "learning_rate": 3.94579416213833e-05,
      "loss": 1.5498,
      "num_input_tokens_seen": 6057136,
      "step": 1180,
      "train_runtime": 4177.9674,
      "train_tokens_per_second": 1449.781
    },
    {
      "epoch": 0.9159420289855073,
      "grad_norm": 1.5856038331985474,
      "learning_rate": 3.9375296826959884e-05,
      "loss": 1.4474,
      "num_input_tokens_seen": 6082512,
      "step": 1185,
      "train_runtime": 4195.2367,
      "train_tokens_per_second": 1449.861
    },
    {
      "epoch": 0.9198067632850242,
      "grad_norm": 1.9983689785003662,
      "learning_rate": 3.9292416665889545e-05,
      "loss": 1.3841,
      "num_input_tokens_seen": 6106576,
      "step": 1190,
      "train_runtime": 4212.5469,
      "train_tokens_per_second": 1449.616
    },
    {
      "epoch": 0.923671497584541,
      "grad_norm": 2.999359130859375,
      "learning_rate": 3.9209302495168654e-05,
      "loss": 1.7222,
      "num_input_tokens_seen": 6131296,
      "step": 1195,
      "train_runtime": 4229.6466,
      "train_tokens_per_second": 1449.6
    },
    {
      "epoch": 0.927536231884058,
      "grad_norm": 1.6967494487762451,
      "learning_rate": 3.912595567562502e-05,
      "loss": 1.6465,
      "num_input_tokens_seen": 6161472,
      "step": 1200,
      "train_runtime": 4249.6482,
      "train_tokens_per_second": 1449.878
    },
    {
      "epoch": 0.9314009661835749,
      "grad_norm": 2.31192946434021,
      "learning_rate": 3.904237757189561e-05,
      "loss": 1.5593,
      "num_input_tokens_seen": 6179712,
      "step": 1205,
      "train_runtime": 4263.967,
      "train_tokens_per_second": 1449.287
    },
    {
      "epoch": 0.9352657004830918,
      "grad_norm": 2.366255760192871,
      "learning_rate": 3.8958569552404204e-05,
      "loss": 1.9851,
      "num_input_tokens_seen": 6201632,
      "step": 1210,
      "train_runtime": 4279.6911,
      "train_tokens_per_second": 1449.084
    },
    {
      "epoch": 0.9391304347826087,
      "grad_norm": 2.019252300262451,
      "learning_rate": 3.8874532989338985e-05,
      "loss": 1.2861,
      "num_input_tokens_seen": 6226560,
      "step": 1215,
      "train_runtime": 4296.7994,
      "train_tokens_per_second": 1449.116
    },
    {
      "epoch": 0.9429951690821256,
      "grad_norm": 3.0318310260772705,
      "learning_rate": 3.87902692586301e-05,
      "loss": 1.5288,
      "num_input_tokens_seen": 6255296,
      "step": 1220,
      "train_runtime": 4315.9219,
      "train_tokens_per_second": 1449.353
    },
    {
      "epoch": 0.9468599033816425,
      "grad_norm": 1.9577890634536743,
      "learning_rate": 3.8705779739927095e-05,
      "loss": 1.2528,
      "num_input_tokens_seen": 6279552,
      "step": 1225,
      "train_runtime": 4332.9295,
      "train_tokens_per_second": 1449.262
    },
    {
      "epoch": 0.9507246376811594,
      "grad_norm": 2.5781142711639404,
      "learning_rate": 3.8621065816576325e-05,
      "loss": 1.5671,
      "num_input_tokens_seen": 6309376,
      "step": 1230,
      "train_runtime": 4352.7082,
      "train_tokens_per_second": 1449.529
    },
    {
      "epoch": 0.9545893719806763,
      "grad_norm": 1.9060887098312378,
      "learning_rate": 3.8536128875598356e-05,
      "loss": 1.2371,
      "num_input_tokens_seen": 6338112,
      "step": 1235,
      "train_runtime": 4371.6487,
      "train_tokens_per_second": 1449.822
    },
    {
      "epoch": 0.9584541062801932,
      "grad_norm": 2.0301260948181152,
      "learning_rate": 3.845097030766519e-05,
      "loss": 1.7117,
      "num_input_tokens_seen": 6363824,
      "step": 1240,
      "train_runtime": 4389.2455,
      "train_tokens_per_second": 1449.867
    },
    {
      "epoch": 0.9623188405797102,
      "grad_norm": 2.087803363800049,
      "learning_rate": 3.836559150707755e-05,
      "loss": 1.605,
      "num_input_tokens_seen": 6388896,
      "step": 1245,
      "train_runtime": 4406.3972,
      "train_tokens_per_second": 1449.914
    },
    {
      "epoch": 0.966183574879227,
      "grad_norm": 2.3962841033935547,
      "learning_rate": 3.827999387174201e-05,
      "loss": 1.5083,
      "num_input_tokens_seen": 6418416,
      "step": 1250,
      "train_runtime": 4425.8161,
      "train_tokens_per_second": 1450.222
    },
    {
      "epoch": 0.970048309178744,
      "grad_norm": 2.609525680541992,
      "learning_rate": 3.819417880314813e-05,
      "loss": 1.5284,
      "num_input_tokens_seen": 6444352,
      "step": 1255,
      "train_runtime": 4443.455,
      "train_tokens_per_second": 1450.302
    },
    {
      "epoch": 0.9739130434782609,
      "grad_norm": 2.587637186050415,
      "learning_rate": 3.810814770634549e-05,
      "loss": 1.3681,
      "num_input_tokens_seen": 6469920,
      "step": 1260,
      "train_runtime": 4461.0846,
      "train_tokens_per_second": 1450.302
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.5680859088897705,
      "learning_rate": 3.802190198992073e-05,
      "loss": 1.4656,
      "num_input_tokens_seen": 6494640,
      "step": 1265,
      "train_runtime": 4478.0218,
      "train_tokens_per_second": 1450.337
    },
    {
      "epoch": 0.9816425120772947,
      "grad_norm": 2.228511333465576,
      "learning_rate": 3.793544306597443e-05,
      "loss": 1.5518,
      "num_input_tokens_seen": 6519616,
      "step": 1270,
      "train_runtime": 4495.4582,
      "train_tokens_per_second": 1450.267
    },
    {
      "epoch": 0.9855072463768116,
      "grad_norm": 1.8102245330810547,
      "learning_rate": 3.7848772350098015e-05,
      "loss": 1.295,
      "num_input_tokens_seen": 6544640,
      "step": 1275,
      "train_runtime": 4512.741,
      "train_tokens_per_second": 1450.258
    },
    {
      "epoch": 0.9893719806763285,
      "grad_norm": 2.6375133991241455,
      "learning_rate": 3.77618912613506e-05,
      "loss": 1.5044,
      "num_input_tokens_seen": 6572000,
      "step": 1280,
      "train_runtime": 4531.2839,
      "train_tokens_per_second": 1450.362
    },
    {
      "epoch": 0.9932367149758454,
      "grad_norm": 1.7687371969223022,
      "learning_rate": 3.7674801222235706e-05,
      "loss": 1.4623,
      "num_input_tokens_seen": 6597728,
      "step": 1285,
      "train_runtime": 4548.9702,
      "train_tokens_per_second": 1450.378
    },
    {
      "epoch": 0.9971014492753624,
      "grad_norm": 1.5600426197052002,
      "learning_rate": 3.7587503658678014e-05,
      "loss": 1.1012,
      "num_input_tokens_seen": 6629264,
      "step": 1290,
      "train_runtime": 4569.7356,
      "train_tokens_per_second": 1450.689
    },
    {
      "epoch": 1.0007729468599034,
      "grad_norm": 1.623727560043335,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.8731,
      "num_input_tokens_seen": 6655376,
      "step": 1295,
      "train_runtime": 4587.1124,
      "train_tokens_per_second": 1450.886
    },
    {
      "epoch": 1.0046376811594202,
      "grad_norm": 2.3533833026885986,
      "learning_rate": 3.741229167889853e-05,
      "loss": 1.7527,
      "num_input_tokens_seen": 6681392,
      "step": 1300,
      "train_runtime": 4605.0499,
      "train_tokens_per_second": 1450.884
    },
    {
      "epoch": 1.0085024154589373,
      "grad_norm": 1.8797335624694824,
      "learning_rate": 3.732438013142141e-05,
      "loss": 1.3654,
      "num_input_tokens_seen": 6706816,
      "step": 1305,
      "train_runtime": 4622.8217,
      "train_tokens_per_second": 1450.806
    },
    {
      "epoch": 1.012367149758454,
      "grad_norm": 1.9677213430404663,
      "learning_rate": 3.723626679694387e-05,
      "loss": 1.2418,
      "num_input_tokens_seen": 6728800,
      "step": 1310,
      "train_runtime": 4638.68,
      "train_tokens_per_second": 1450.585
    },
    {
      "epoch": 1.016231884057971,
      "grad_norm": 2.5398473739624023,
      "learning_rate": 3.714795311814499e-05,
      "loss": 1.2777,
      "num_input_tokens_seen": 6756192,
      "step": 1315,
      "train_runtime": 4657.0827,
      "train_tokens_per_second": 1450.735
    },
    {
      "epoch": 1.020096618357488,
      "grad_norm": 2.6441469192504883,
      "learning_rate": 3.705944054098409e-05,
      "loss": 1.6902,
      "num_input_tokens_seen": 6780720,
      "step": 1320,
      "train_runtime": 4674.1785,
      "train_tokens_per_second": 1450.676
    },
    {
      "epoch": 1.0239613526570048,
      "grad_norm": 1.8920947313308716,
      "learning_rate": 3.697073051467707e-05,
      "loss": 1.5705,
      "num_input_tokens_seen": 6810560,
      "step": 1325,
      "train_runtime": 4693.9334,
      "train_tokens_per_second": 1450.928
    },
    {
      "epoch": 1.0278260869565217,
      "grad_norm": 2.0720157623291016,
      "learning_rate": 3.688182449167262e-05,
      "loss": 1.4673,
      "num_input_tokens_seen": 6841424,
      "step": 1330,
      "train_runtime": 4714.1113,
      "train_tokens_per_second": 1451.265
    },
    {
      "epoch": 1.0316908212560387,
      "grad_norm": 2.0184459686279297,
      "learning_rate": 3.679272392762854e-05,
      "loss": 1.6254,
      "num_input_tokens_seen": 6866944,
      "step": 1335,
      "train_runtime": 4731.9184,
      "train_tokens_per_second": 1451.197
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 3.168208599090576,
      "learning_rate": 3.67034302813878e-05,
      "loss": 1.3235,
      "num_input_tokens_seen": 6897872,
      "step": 1340,
      "train_runtime": 4752.3914,
      "train_tokens_per_second": 1451.453
    },
    {
      "epoch": 1.0394202898550724,
      "grad_norm": 2.1378602981567383,
      "learning_rate": 3.661394501495474e-05,
      "loss": 1.4963,
      "num_input_tokens_seen": 6922064,
      "step": 1345,
      "train_runtime": 4769.3377,
      "train_tokens_per_second": 1451.368
    },
    {
      "epoch": 1.0432850241545895,
      "grad_norm": 2.6693966388702393,
      "learning_rate": 3.652426959347105e-05,
      "loss": 1.5513,
      "num_input_tokens_seen": 6948784,
      "step": 1350,
      "train_runtime": 4787.6669,
      "train_tokens_per_second": 1451.393
    },
    {
      "epoch": 1.0471497584541063,
      "grad_norm": 2.0902843475341797,
      "learning_rate": 3.643440548519189e-05,
      "loss": 1.5539,
      "num_input_tokens_seen": 6973184,
      "step": 1355,
      "train_runtime": 4805.0542,
      "train_tokens_per_second": 1451.219
    },
    {
      "epoch": 1.0510144927536231,
      "grad_norm": 2.4123735427856445,
      "learning_rate": 3.634435416146174e-05,
      "loss": 1.6809,
      "num_input_tokens_seen": 7003936,
      "step": 1360,
      "train_runtime": 4825.3699,
      "train_tokens_per_second": 1451.482
    },
    {
      "epoch": 1.0548792270531402,
      "grad_norm": 2.3797953128814697,
      "learning_rate": 3.6254117096690365e-05,
      "loss": 1.1839,
      "num_input_tokens_seen": 7032528,
      "step": 1365,
      "train_runtime": 4843.9316,
      "train_tokens_per_second": 1451.822
    },
    {
      "epoch": 1.058743961352657,
      "grad_norm": 2.1841232776641846,
      "learning_rate": 3.616369576832868e-05,
      "loss": 1.255,
      "num_input_tokens_seen": 7058864,
      "step": 1370,
      "train_runtime": 4862.0373,
      "train_tokens_per_second": 1451.833
    },
    {
      "epoch": 1.0626086956521739,
      "grad_norm": 2.4636101722717285,
      "learning_rate": 3.607309165684455e-05,
      "loss": 1.4685,
      "num_input_tokens_seen": 7088544,
      "step": 1375,
      "train_runtime": 4881.5979,
      "train_tokens_per_second": 1452.095
    },
    {
      "epoch": 1.066473429951691,
      "grad_norm": 1.8402019739151,
      "learning_rate": 3.598230624569852e-05,
      "loss": 1.1016,
      "num_input_tokens_seen": 7112560,
      "step": 1380,
      "train_runtime": 4898.3593,
      "train_tokens_per_second": 1452.029
    },
    {
      "epoch": 1.0703381642512078,
      "grad_norm": 2.2344539165496826,
      "learning_rate": 3.5891341021319594e-05,
      "loss": 1.3622,
      "num_input_tokens_seen": 7140816,
      "step": 1385,
      "train_runtime": 4917.1219,
      "train_tokens_per_second": 1452.235
    },
    {
      "epoch": 1.0742028985507246,
      "grad_norm": 1.7720316648483276,
      "learning_rate": 3.58001974730808e-05,
      "loss": 1.7417,
      "num_input_tokens_seen": 7165808,
      "step": 1390,
      "train_runtime": 4934.344,
      "train_tokens_per_second": 1452.231
    },
    {
      "epoch": 1.0780676328502414,
      "grad_norm": 1.8720351457595825,
      "learning_rate": 3.5708877093274926e-05,
      "loss": 1.4181,
      "num_input_tokens_seen": 7190800,
      "step": 1395,
      "train_runtime": 4951.8884,
      "train_tokens_per_second": 1452.133
    },
    {
      "epoch": 1.0819323671497585,
      "grad_norm": 2.117110013961792,
      "learning_rate": 3.561738137708998e-05,
      "loss": 1.5207,
      "num_input_tokens_seen": 7218176,
      "step": 1400,
      "train_runtime": 4970.6665,
      "train_tokens_per_second": 1452.155
    },
    {
      "epoch": 1.0857971014492753,
      "grad_norm": 2.0873167514801025,
      "learning_rate": 3.552571182258476e-05,
      "loss": 1.2937,
      "num_input_tokens_seen": 7252880,
      "step": 1405,
      "train_runtime": 4993.3921,
      "train_tokens_per_second": 1452.496
    },
    {
      "epoch": 1.0896618357487924,
      "grad_norm": 2.3191471099853516,
      "learning_rate": 3.543386993066432e-05,
      "loss": 1.3688,
      "num_input_tokens_seen": 7277536,
      "step": 1410,
      "train_runtime": 5010.7894,
      "train_tokens_per_second": 1452.373
    },
    {
      "epoch": 1.0935265700483092,
      "grad_norm": 2.232084274291992,
      "learning_rate": 3.53418572050554e-05,
      "loss": 1.3466,
      "num_input_tokens_seen": 7303280,
      "step": 1415,
      "train_runtime": 5028.5108,
      "train_tokens_per_second": 1452.374
    },
    {
      "epoch": 1.097391304347826,
      "grad_norm": 3.4033212661743164,
      "learning_rate": 3.524967515228179e-05,
      "loss": 1.6225,
      "num_input_tokens_seen": 7326080,
      "step": 1420,
      "train_runtime": 5044.8035,
      "train_tokens_per_second": 1452.203
    },
    {
      "epoch": 1.101256038647343,
      "grad_norm": 1.5708200931549072,
      "learning_rate": 3.515732528163968e-05,
      "loss": 1.527,
      "num_input_tokens_seen": 7352064,
      "step": 1425,
      "train_runtime": 5062.2628,
      "train_tokens_per_second": 1452.328
    },
    {
      "epoch": 1.10512077294686,
      "grad_norm": 1.7561362981796265,
      "learning_rate": 3.506480910517293e-05,
      "loss": 1.5353,
      "num_input_tokens_seen": 7376096,
      "step": 1430,
      "train_runtime": 5079.2219,
      "train_tokens_per_second": 1452.21
    },
    {
      "epoch": 1.1089855072463768,
      "grad_norm": 1.4764199256896973,
      "learning_rate": 3.497212813764833e-05,
      "loss": 1.7473,
      "num_input_tokens_seen": 7401216,
      "step": 1435,
      "train_runtime": 5096.7361,
      "train_tokens_per_second": 1452.148
    },
    {
      "epoch": 1.1128502415458936,
      "grad_norm": 2.0057337284088135,
      "learning_rate": 3.48792838965308e-05,
      "loss": 1.5188,
      "num_input_tokens_seen": 7425120,
      "step": 1440,
      "train_runtime": 5113.8082,
      "train_tokens_per_second": 1451.975
    },
    {
      "epoch": 1.1167149758454107,
      "grad_norm": 2.163954734802246,
      "learning_rate": 3.478627790195853e-05,
      "loss": 1.2691,
      "num_input_tokens_seen": 7448736,
      "step": 1445,
      "train_runtime": 5130.1948,
      "train_tokens_per_second": 1451.94
    },
    {
      "epoch": 1.1205797101449275,
      "grad_norm": 2.135056495666504,
      "learning_rate": 3.469311167671809e-05,
      "loss": 1.6921,
      "num_input_tokens_seen": 7476304,
      "step": 1450,
      "train_runtime": 5148.8568,
      "train_tokens_per_second": 1452.032
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 2.643907308578491,
      "learning_rate": 3.4599786746219535e-05,
      "loss": 1.2814,
      "num_input_tokens_seen": 7502288,
      "step": 1455,
      "train_runtime": 5166.8408,
      "train_tokens_per_second": 1452.007
    },
    {
      "epoch": 1.1283091787439614,
      "grad_norm": 1.5430188179016113,
      "learning_rate": 3.450630463847137e-05,
      "loss": 1.4345,
      "num_input_tokens_seen": 7530384,
      "step": 1460,
      "train_runtime": 5185.8048,
      "train_tokens_per_second": 1452.115
    },
    {
      "epoch": 1.1321739130434783,
      "grad_norm": 2.6602132320404053,
      "learning_rate": 3.441266688405557e-05,
      "loss": 1.6922,
      "num_input_tokens_seen": 7555216,
      "step": 1465,
      "train_runtime": 5202.966,
      "train_tokens_per_second": 1452.098
    },
    {
      "epoch": 1.136038647342995,
      "grad_norm": 2.836441993713379,
      "learning_rate": 3.43188750161025e-05,
      "loss": 1.3996,
      "num_input_tokens_seen": 7579680,
      "step": 1470,
      "train_runtime": 5219.7505,
      "train_tokens_per_second": 1452.115
    },
    {
      "epoch": 1.1399033816425121,
      "grad_norm": 2.6329941749572754,
      "learning_rate": 3.422493057026587e-05,
      "loss": 1.6832,
      "num_input_tokens_seen": 7603008,
      "step": 1475,
      "train_runtime": 5236.3948,
      "train_tokens_per_second": 1451.955
    },
    {
      "epoch": 1.143768115942029,
      "grad_norm": 2.145461082458496,
      "learning_rate": 3.41308350846975e-05,
      "loss": 1.4101,
      "num_input_tokens_seen": 7636256,
      "step": 1480,
      "train_runtime": 5257.6064,
      "train_tokens_per_second": 1452.421
    },
    {
      "epoch": 1.1476328502415458,
      "grad_norm": 2.278038740158081,
      "learning_rate": 3.40365901000222e-05,
      "loss": 1.0718,
      "num_input_tokens_seen": 7659408,
      "step": 1485,
      "train_runtime": 5273.7806,
      "train_tokens_per_second": 1452.356
    },
    {
      "epoch": 1.1514975845410629,
      "grad_norm": 2.885162591934204,
      "learning_rate": 3.3942197159312525e-05,
      "loss": 1.5385,
      "num_input_tokens_seen": 7684912,
      "step": 1490,
      "train_runtime": 5291.6786,
      "train_tokens_per_second": 1452.264
    },
    {
      "epoch": 1.1553623188405797,
      "grad_norm": 2.394899606704712,
      "learning_rate": 3.384765780806353e-05,
      "loss": 1.559,
      "num_input_tokens_seen": 7707360,
      "step": 1495,
      "train_runtime": 5307.7106,
      "train_tokens_per_second": 1452.106
    },
    {
      "epoch": 1.1592270531400966,
      "grad_norm": 2.333209276199341,
      "learning_rate": 3.375297359416741e-05,
      "loss": 1.5271,
      "num_input_tokens_seen": 7732480,
      "step": 1500,
      "train_runtime": 5325.1557,
      "train_tokens_per_second": 1452.066
    },
    {
      "epoch": 1.1630917874396136,
      "grad_norm": 3.1421802043914795,
      "learning_rate": 3.3658146067888235e-05,
      "loss": 1.8744,
      "num_input_tokens_seen": 7754240,
      "step": 1505,
      "train_runtime": 5341.6484,
      "train_tokens_per_second": 1451.657
    },
    {
      "epoch": 1.1669565217391304,
      "grad_norm": 1.9040340185165405,
      "learning_rate": 3.3563176781836515e-05,
      "loss": 1.2914,
      "num_input_tokens_seen": 7786752,
      "step": 1510,
      "train_runtime": 5362.4861,
      "train_tokens_per_second": 1452.079
    },
    {
      "epoch": 1.1708212560386473,
      "grad_norm": 2.7657430171966553,
      "learning_rate": 3.346806729094378e-05,
      "loss": 1.182,
      "num_input_tokens_seen": 7809552,
      "step": 1515,
      "train_runtime": 5378.6815,
      "train_tokens_per_second": 1451.945
    },
    {
      "epoch": 1.1746859903381643,
      "grad_norm": 2.0034260749816895,
      "learning_rate": 3.3372819152437144e-05,
      "loss": 1.2098,
      "num_input_tokens_seen": 7834864,
      "step": 1520,
      "train_runtime": 5396.0605,
      "train_tokens_per_second": 1451.96
    },
    {
      "epoch": 1.1785507246376812,
      "grad_norm": 2.0352742671966553,
      "learning_rate": 3.327743392581379e-05,
      "loss": 1.5137,
      "num_input_tokens_seen": 7862096,
      "step": 1525,
      "train_runtime": 5414.4395,
      "train_tokens_per_second": 1452.061
    },
    {
      "epoch": 1.182415458937198,
      "grad_norm": 2.813494920730591,
      "learning_rate": 3.318191317281546e-05,
      "loss": 1.8736,
      "num_input_tokens_seen": 7887136,
      "step": 1530,
      "train_runtime": 5432.052,
      "train_tokens_per_second": 1451.963
    },
    {
      "epoch": 1.186280193236715,
      "grad_norm": 2.072420358657837,
      "learning_rate": 3.308625845740286e-05,
      "loss": 1.2116,
      "num_input_tokens_seen": 7909792,
      "step": 1535,
      "train_runtime": 5448.199,
      "train_tokens_per_second": 1451.818
    },
    {
      "epoch": 1.190144927536232,
      "grad_norm": 2.0859367847442627,
      "learning_rate": 3.299047134573005e-05,
      "loss": 1.4354,
      "num_input_tokens_seen": 7931952,
      "step": 1540,
      "train_runtime": 5464.2857,
      "train_tokens_per_second": 1451.599
    },
    {
      "epoch": 1.1940096618357487,
      "grad_norm": 2.5520591735839844,
      "learning_rate": 3.289455340611882e-05,
      "loss": 1.3225,
      "num_input_tokens_seen": 7953408,
      "step": 1545,
      "train_runtime": 5479.8666,
      "train_tokens_per_second": 1451.387
    },
    {
      "epoch": 1.1978743961352656,
      "grad_norm": 1.895637035369873,
      "learning_rate": 3.279850620903302e-05,
      "loss": 1.3908,
      "num_input_tokens_seen": 7978688,
      "step": 1550,
      "train_runtime": 5497.4104,
      "train_tokens_per_second": 1451.354
    },
    {
      "epoch": 1.2017391304347826,
      "grad_norm": 2.376520872116089,
      "learning_rate": 3.2702331327052816e-05,
      "loss": 1.4299,
      "num_input_tokens_seen": 8005008,
      "step": 1555,
      "train_runtime": 5515.4888,
      "train_tokens_per_second": 1451.369
    },
    {
      "epoch": 1.2056038647342995,
      "grad_norm": 2.632258892059326,
      "learning_rate": 3.260603033484896e-05,
      "loss": 1.2691,
      "num_input_tokens_seen": 8031344,
      "step": 1560,
      "train_runtime": 5533.6454,
      "train_tokens_per_second": 1451.366
    },
    {
      "epoch": 1.2094685990338165,
      "grad_norm": 2.4404027462005615,
      "learning_rate": 3.2509604809157e-05,
      "loss": 1.4616,
      "num_input_tokens_seen": 8057328,
      "step": 1565,
      "train_runtime": 5551.1463,
      "train_tokens_per_second": 1451.471
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 1.7315136194229126,
      "learning_rate": 3.241305632875149e-05,
      "loss": 1.5039,
      "num_input_tokens_seen": 8084816,
      "step": 1570,
      "train_runtime": 5569.6086,
      "train_tokens_per_second": 1451.595
    },
    {
      "epoch": 1.2171980676328502,
      "grad_norm": 1.8259319067001343,
      "learning_rate": 3.23163864744201e-05,
      "loss": 1.0546,
      "num_input_tokens_seen": 8109808,
      "step": 1575,
      "train_runtime": 5586.9784,
      "train_tokens_per_second": 1451.555
    },
    {
      "epoch": 1.221062801932367,
      "grad_norm": 2.536670207977295,
      "learning_rate": 3.221959682893777e-05,
      "loss": 1.3795,
      "num_input_tokens_seen": 8131376,
      "step": 1580,
      "train_runtime": 5602.4914,
      "train_tokens_per_second": 1451.386
    },
    {
      "epoch": 1.224927536231884,
      "grad_norm": 1.820505142211914,
      "learning_rate": 3.2122688977040765e-05,
      "loss": 1.3413,
      "num_input_tokens_seen": 8151744,
      "step": 1585,
      "train_runtime": 5617.7105,
      "train_tokens_per_second": 1451.079
    },
    {
      "epoch": 1.228792270531401,
      "grad_norm": 2.3135483264923096,
      "learning_rate": 3.202566450540076e-05,
      "loss": 1.2312,
      "num_input_tokens_seen": 8178016,
      "step": 1590,
      "train_runtime": 5635.842,
      "train_tokens_per_second": 1451.073
    },
    {
      "epoch": 1.2326570048309178,
      "grad_norm": 2.324289321899414,
      "learning_rate": 3.192852500259883e-05,
      "loss": 1.3814,
      "num_input_tokens_seen": 8204352,
      "step": 1595,
      "train_runtime": 5653.7788,
      "train_tokens_per_second": 1451.127
    },
    {
      "epoch": 1.2365217391304348,
      "grad_norm": 2.0123260021209717,
      "learning_rate": 3.183127205909946e-05,
      "loss": 1.4209,
      "num_input_tokens_seen": 8227472,
      "step": 1600,
      "train_runtime": 5669.9953,
      "train_tokens_per_second": 1451.054
    },
    {
      "epoch": 1.2403864734299517,
      "grad_norm": 2.176379680633545,
      "learning_rate": 3.173390726722449e-05,
      "loss": 1.4203,
      "num_input_tokens_seen": 8250208,
      "step": 1605,
      "train_runtime": 5686.852,
      "train_tokens_per_second": 1450.751
    },
    {
      "epoch": 1.2442512077294685,
      "grad_norm": 2.329265832901001,
      "learning_rate": 3.163643222112708e-05,
      "loss": 1.3505,
      "num_input_tokens_seen": 8276544,
      "step": 1610,
      "train_runtime": 5704.9834,
      "train_tokens_per_second": 1450.757
    },
    {
      "epoch": 1.2481159420289856,
      "grad_norm": 3.0687477588653564,
      "learning_rate": 3.1538848516765546e-05,
      "loss": 1.6069,
      "num_input_tokens_seen": 8299392,
      "step": 1615,
      "train_runtime": 5721.3001,
      "train_tokens_per_second": 1450.613
    },
    {
      "epoch": 1.2519806763285024,
      "grad_norm": 2.4866271018981934,
      "learning_rate": 3.144115775187729e-05,
      "loss": 1.4736,
      "num_input_tokens_seen": 8326352,
      "step": 1620,
      "train_runtime": 5739.8124,
      "train_tokens_per_second": 1450.631
    },
    {
      "epoch": 1.2558454106280192,
      "grad_norm": 3.328301191329956,
      "learning_rate": 3.1343361525952595e-05,
      "loss": 1.3919,
      "num_input_tokens_seen": 8349024,
      "step": 1625,
      "train_runtime": 5756.3588,
      "train_tokens_per_second": 1450.4
    },
    {
      "epoch": 1.2597101449275363,
      "grad_norm": 2.2075488567352295,
      "learning_rate": 3.12454614402085e-05,
      "loss": 1.5796,
      "num_input_tokens_seen": 8374496,
      "step": 1630,
      "train_runtime": 5773.5058,
      "train_tokens_per_second": 1450.504
    },
    {
      "epoch": 1.2635748792270531,
      "grad_norm": 1.6076939105987549,
      "learning_rate": 3.114745909756247e-05,
      "loss": 1.3634,
      "num_input_tokens_seen": 8403168,
      "step": 1635,
      "train_runtime": 5792.3872,
      "train_tokens_per_second": 1450.726
    },
    {
      "epoch": 1.26743961352657,
      "grad_norm": 2.860856294631958,
      "learning_rate": 3.1049356102606286e-05,
      "loss": 1.469,
      "num_input_tokens_seen": 8428592,
      "step": 1640,
      "train_runtime": 5810.3026,
      "train_tokens_per_second": 1450.629
    },
    {
      "epoch": 1.271304347826087,
      "grad_norm": 3.0915334224700928,
      "learning_rate": 3.095115406157968e-05,
      "loss": 1.2186,
      "num_input_tokens_seen": 8458560,
      "step": 1645,
      "train_runtime": 5830.0197,
      "train_tokens_per_second": 1450.863
    },
    {
      "epoch": 1.2751690821256039,
      "grad_norm": 1.8722481727600098,
      "learning_rate": 3.0852854582344074e-05,
      "loss": 1.5756,
      "num_input_tokens_seen": 8484880,
      "step": 1650,
      "train_runtime": 5848.0623,
      "train_tokens_per_second": 1450.887
    },
    {
      "epoch": 1.2790338164251207,
      "grad_norm": 2.0880868434906006,
      "learning_rate": 3.0754459274356214e-05,
      "loss": 1.3475,
      "num_input_tokens_seen": 8508992,
      "step": 1655,
      "train_runtime": 5864.9486,
      "train_tokens_per_second": 1450.821
    },
    {
      "epoch": 1.2828985507246378,
      "grad_norm": 2.445793628692627,
      "learning_rate": 3.06559697486419e-05,
      "loss": 1.3289,
      "num_input_tokens_seen": 8537872,
      "step": 1660,
      "train_runtime": 5884.3507,
      "train_tokens_per_second": 1450.945
    },
    {
      "epoch": 1.2867632850241546,
      "grad_norm": 1.7828413248062134,
      "learning_rate": 3.0557387617769525e-05,
      "loss": 1.2469,
      "num_input_tokens_seen": 8564848,
      "step": 1665,
      "train_runtime": 5902.4836,
      "train_tokens_per_second": 1451.058
    },
    {
      "epoch": 1.2906280193236714,
      "grad_norm": 2.1990652084350586,
      "learning_rate": 3.045871449582372e-05,
      "loss": 1.8706,
      "num_input_tokens_seen": 8589328,
      "step": 1670,
      "train_runtime": 5919.6865,
      "train_tokens_per_second": 1450.977
    },
    {
      "epoch": 1.2944927536231883,
      "grad_norm": 2.768068313598633,
      "learning_rate": 3.0359951998378894e-05,
      "loss": 1.4756,
      "num_input_tokens_seen": 8618320,
      "step": 1675,
      "train_runtime": 5939.5895,
      "train_tokens_per_second": 1450.996
    },
    {
      "epoch": 1.2983574879227053,
      "grad_norm": 2.145045280456543,
      "learning_rate": 3.0261101742472825e-05,
      "loss": 1.272,
      "num_input_tokens_seen": 8650080,
      "step": 1680,
      "train_runtime": 5960.0941,
      "train_tokens_per_second": 1451.333
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 2.4551050662994385,
      "learning_rate": 3.016216534658014e-05,
      "loss": 1.9,
      "num_input_tokens_seen": 8672912,
      "step": 1685,
      "train_runtime": 5976.4482,
      "train_tokens_per_second": 1451.182
    },
    {
      "epoch": 1.3060869565217392,
      "grad_norm": 2.6900877952575684,
      "learning_rate": 3.0063144430585855e-05,
      "loss": 1.6129,
      "num_input_tokens_seen": 8698496,
      "step": 1690,
      "train_runtime": 5994.0857,
      "train_tokens_per_second": 1451.18
    },
    {
      "epoch": 1.309951690821256,
      "grad_norm": 2.470947027206421,
      "learning_rate": 2.99640406157588e-05,
      "loss": 1.459,
      "num_input_tokens_seen": 8721696,
      "step": 1695,
      "train_runtime": 6010.5663,
      "train_tokens_per_second": 1451.061
    },
    {
      "epoch": 1.313816425120773,
      "grad_norm": 1.6756457090377808,
      "learning_rate": 2.986485552472515e-05,
      "loss": 1.0681,
      "num_input_tokens_seen": 8754784,
      "step": 1700,
      "train_runtime": 6031.9724,
      "train_tokens_per_second": 1451.397
    },
    {
      "epoch": 1.3176811594202897,
      "grad_norm": 2.8686985969543457,
      "learning_rate": 2.9765590781441776e-05,
      "loss": 1.2689,
      "num_input_tokens_seen": 8781904,
      "step": 1705,
      "train_runtime": 6050.64,
      "train_tokens_per_second": 1451.401
    },
    {
      "epoch": 1.3215458937198068,
      "grad_norm": 2.854491710662842,
      "learning_rate": 2.9666248011169727e-05,
      "loss": 1.4253,
      "num_input_tokens_seen": 8803808,
      "step": 1710,
      "train_runtime": 6066.7454,
      "train_tokens_per_second": 1451.158
    },
    {
      "epoch": 1.3254106280193236,
      "grad_norm": 1.9620089530944824,
      "learning_rate": 2.9566828840447557e-05,
      "loss": 1.3693,
      "num_input_tokens_seen": 8829456,
      "step": 1715,
      "train_runtime": 6084.2155,
      "train_tokens_per_second": 1451.207
    },
    {
      "epoch": 1.3292753623188407,
      "grad_norm": 1.7361878156661987,
      "learning_rate": 2.946733489706476e-05,
      "loss": 1.5063,
      "num_input_tokens_seen": 8850176,
      "step": 1720,
      "train_runtime": 6099.7336,
      "train_tokens_per_second": 1450.912
    },
    {
      "epoch": 1.3331400966183575,
      "grad_norm": 2.9059605598449707,
      "learning_rate": 2.936776781003504e-05,
      "loss": 1.5529,
      "num_input_tokens_seen": 8874032,
      "step": 1725,
      "train_runtime": 6116.516,
      "train_tokens_per_second": 1450.831
    },
    {
      "epoch": 1.3370048309178744,
      "grad_norm": 2.574427604675293,
      "learning_rate": 2.926812920956972e-05,
      "loss": 1.3197,
      "num_input_tokens_seen": 8899856,
      "step": 1730,
      "train_runtime": 6133.8559,
      "train_tokens_per_second": 1450.94
    },
    {
      "epoch": 1.3408695652173912,
      "grad_norm": 2.463435173034668,
      "learning_rate": 2.9168420727050972e-05,
      "loss": 1.6598,
      "num_input_tokens_seen": 8920352,
      "step": 1735,
      "train_runtime": 6149.0674,
      "train_tokens_per_second": 1450.684
    },
    {
      "epoch": 1.3447342995169083,
      "grad_norm": 3.2687487602233887,
      "learning_rate": 2.9068643995005194e-05,
      "loss": 1.9803,
      "num_input_tokens_seen": 8945552,
      "step": 1740,
      "train_runtime": 6166.6183,
      "train_tokens_per_second": 1450.641
    },
    {
      "epoch": 1.348599033816425,
      "grad_norm": 2.7598750591278076,
      "learning_rate": 2.8968800647076207e-05,
      "loss": 1.2456,
      "num_input_tokens_seen": 8973568,
      "step": 1745,
      "train_runtime": 6185.445,
      "train_tokens_per_second": 1450.755
    },
    {
      "epoch": 1.3524637681159422,
      "grad_norm": 2.633810520172119,
      "learning_rate": 2.8868892317998525e-05,
      "loss": 1.542,
      "num_input_tokens_seen": 9000992,
      "step": 1750,
      "train_runtime": 6203.6645,
      "train_tokens_per_second": 1450.915
    },
    {
      "epoch": 1.356328502415459,
      "grad_norm": 2.449735641479492,
      "learning_rate": 2.876892064357061e-05,
      "loss": 1.4196,
      "num_input_tokens_seen": 9024496,
      "step": 1755,
      "train_runtime": 6220.2269,
      "train_tokens_per_second": 1450.831
    },
    {
      "epoch": 1.3601932367149758,
      "grad_norm": 2.02167010307312,
      "learning_rate": 2.8668887260628074e-05,
      "loss": 1.1316,
      "num_input_tokens_seen": 9050352,
      "step": 1760,
      "train_runtime": 6237.9697,
      "train_tokens_per_second": 1450.849
    },
    {
      "epoch": 1.3640579710144927,
      "grad_norm": 2.3242814540863037,
      "learning_rate": 2.8568793807016892e-05,
      "loss": 1.2037,
      "num_input_tokens_seen": 9076960,
      "step": 1765,
      "train_runtime": 6256.0474,
      "train_tokens_per_second": 1450.91
    },
    {
      "epoch": 1.3679227053140097,
      "grad_norm": 2.16668701171875,
      "learning_rate": 2.8468641921566557e-05,
      "loss": 1.303,
      "num_input_tokens_seen": 9102112,
      "step": 1770,
      "train_runtime": 6273.3755,
      "train_tokens_per_second": 1450.911
    },
    {
      "epoch": 1.3717874396135266,
      "grad_norm": 2.1860663890838623,
      "learning_rate": 2.8368433244063275e-05,
      "loss": 1.2999,
      "num_input_tokens_seen": 9123312,
      "step": 1775,
      "train_runtime": 6288.9229,
      "train_tokens_per_second": 1450.695
    },
    {
      "epoch": 1.3756521739130434,
      "grad_norm": 2.4843857288360596,
      "learning_rate": 2.8268169415223127e-05,
      "loss": 1.5275,
      "num_input_tokens_seen": 9149600,
      "step": 1780,
      "train_runtime": 6307.1923,
      "train_tokens_per_second": 1450.661
    },
    {
      "epoch": 1.3795169082125605,
      "grad_norm": 1.6449555158615112,
      "learning_rate": 2.8167852076665152e-05,
      "loss": 1.1632,
      "num_input_tokens_seen": 9181792,
      "step": 1785,
      "train_runtime": 6327.8113,
      "train_tokens_per_second": 1451.022
    },
    {
      "epoch": 1.3833816425120773,
      "grad_norm": 2.0711896419525146,
      "learning_rate": 2.8067482870884527e-05,
      "loss": 1.4146,
      "num_input_tokens_seen": 9204304,
      "step": 1790,
      "train_runtime": 6344.1982,
      "train_tokens_per_second": 1450.822
    },
    {
      "epoch": 1.3872463768115941,
      "grad_norm": 3.292640209197998,
      "learning_rate": 2.7967063441225633e-05,
      "loss": 1.9691,
      "num_input_tokens_seen": 9229904,
      "step": 1795,
      "train_runtime": 6361.8235,
      "train_tokens_per_second": 1450.827
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 1.8756451606750488,
      "learning_rate": 2.7866595431855193e-05,
      "loss": 1.2218,
      "num_input_tokens_seen": 9258544,
      "step": 1800,
      "train_runtime": 6381.1206,
      "train_tokens_per_second": 1450.928
    },
    {
      "epoch": 1.394975845410628,
      "grad_norm": 2.169039249420166,
      "learning_rate": 2.776608048773529e-05,
      "loss": 1.53,
      "num_input_tokens_seen": 9284912,
      "step": 1805,
      "train_runtime": 6399.4656,
      "train_tokens_per_second": 1450.889
    },
    {
      "epoch": 1.3988405797101449,
      "grad_norm": 2.081861972808838,
      "learning_rate": 2.766552025459651e-05,
      "loss": 1.4898,
      "num_input_tokens_seen": 9310704,
      "step": 1810,
      "train_runtime": 6417.0528,
      "train_tokens_per_second": 1450.931
    },
    {
      "epoch": 1.402705314009662,
      "grad_norm": 2.2686479091644287,
      "learning_rate": 2.7564916378910917e-05,
      "loss": 1.4737,
      "num_input_tokens_seen": 9340256,
      "step": 1815,
      "train_runtime": 6436.5518,
      "train_tokens_per_second": 1451.127
    },
    {
      "epoch": 1.4065700483091788,
      "grad_norm": 1.920547604560852,
      "learning_rate": 2.746427050786517e-05,
      "loss": 1.3772,
      "num_input_tokens_seen": 9369696,
      "step": 1820,
      "train_runtime": 6455.8992,
      "train_tokens_per_second": 1451.339
    },
    {
      "epoch": 1.4104347826086956,
      "grad_norm": 1.8954490423202515,
      "learning_rate": 2.7363584289333478e-05,
      "loss": 1.2772,
      "num_input_tokens_seen": 9394576,
      "step": 1825,
      "train_runtime": 6473.4569,
      "train_tokens_per_second": 1451.246
    },
    {
      "epoch": 1.4142995169082124,
      "grad_norm": 2.8125743865966797,
      "learning_rate": 2.726285937185069e-05,
      "loss": 1.7376,
      "num_input_tokens_seen": 9412544,
      "step": 1830,
      "train_runtime": 6487.6162,
      "train_tokens_per_second": 1450.848
    },
    {
      "epoch": 1.4181642512077295,
      "grad_norm": 3.1384007930755615,
      "learning_rate": 2.7162097404585264e-05,
      "loss": 1.4622,
      "num_input_tokens_seen": 9437472,
      "step": 1835,
      "train_runtime": 6504.9193,
      "train_tokens_per_second": 1450.821
    },
    {
      "epoch": 1.4220289855072463,
      "grad_norm": 3.8436896800994873,
      "learning_rate": 2.7061300037312277e-05,
      "loss": 1.2762,
      "num_input_tokens_seen": 9468720,
      "step": 1840,
      "train_runtime": 6525.2872,
      "train_tokens_per_second": 1451.081
    },
    {
      "epoch": 1.4258937198067634,
      "grad_norm": 2.9216768741607666,
      "learning_rate": 2.69604689203864e-05,
      "loss": 1.8664,
      "num_input_tokens_seen": 9496928,
      "step": 1845,
      "train_runtime": 6544.3458,
      "train_tokens_per_second": 1451.165
    },
    {
      "epoch": 1.4297584541062802,
      "grad_norm": 1.7012804746627808,
      "learning_rate": 2.6859605704714885e-05,
      "loss": 1.3095,
      "num_input_tokens_seen": 9519136,
      "step": 1850,
      "train_runtime": 6560.6012,
      "train_tokens_per_second": 1450.955
    },
    {
      "epoch": 1.433623188405797,
      "grad_norm": 2.9866247177124023,
      "learning_rate": 2.6758712041730567e-05,
      "loss": 1.7176,
      "num_input_tokens_seen": 9542368,
      "step": 1855,
      "train_runtime": 6577.1901,
      "train_tokens_per_second": 1450.827
    },
    {
      "epoch": 1.437487922705314,
      "grad_norm": 2.953352689743042,
      "learning_rate": 2.665778958336475e-05,
      "loss": 1.3838,
      "num_input_tokens_seen": 9566304,
      "step": 1860,
      "train_runtime": 6594.3732,
      "train_tokens_per_second": 1450.677
    },
    {
      "epoch": 1.441352657004831,
      "grad_norm": 1.9354711771011353,
      "learning_rate": 2.6556839982020244e-05,
      "loss": 1.5296,
      "num_input_tokens_seen": 9587728,
      "step": 1865,
      "train_runtime": 6609.5971,
      "train_tokens_per_second": 1450.577
    },
    {
      "epoch": 1.4452173913043478,
      "grad_norm": 2.0837364196777344,
      "learning_rate": 2.6455864890544234e-05,
      "loss": 1.2823,
      "num_input_tokens_seen": 9614368,
      "step": 1870,
      "train_runtime": 6627.6519,
      "train_tokens_per_second": 1450.645
    },
    {
      "epoch": 1.4490821256038648,
      "grad_norm": 2.113981008529663,
      "learning_rate": 2.6354865962201298e-05,
      "loss": 1.3908,
      "num_input_tokens_seen": 9641824,
      "step": 1875,
      "train_runtime": 6646.3724,
      "train_tokens_per_second": 1450.69
    },
    {
      "epoch": 1.4529468599033817,
      "grad_norm": 2.5757789611816406,
      "learning_rate": 2.6253844850646257e-05,
      "loss": 1.3776,
      "num_input_tokens_seen": 9674336,
      "step": 1880,
      "train_runtime": 6667.4266,
      "train_tokens_per_second": 1450.985
    },
    {
      "epoch": 1.4568115942028985,
      "grad_norm": 1.9258406162261963,
      "learning_rate": 2.6152803209897153e-05,
      "loss": 1.3326,
      "num_input_tokens_seen": 9699744,
      "step": 1885,
      "train_runtime": 6684.903,
      "train_tokens_per_second": 1450.992
    },
    {
      "epoch": 1.4606763285024154,
      "grad_norm": 2.24171781539917,
      "learning_rate": 2.605174269430815e-05,
      "loss": 1.698,
      "num_input_tokens_seen": 9728160,
      "step": 1890,
      "train_runtime": 6704.1604,
      "train_tokens_per_second": 1451.063
    },
    {
      "epoch": 1.4645410628019324,
      "grad_norm": 2.1952028274536133,
      "learning_rate": 2.595066495854246e-05,
      "loss": 1.2683,
      "num_input_tokens_seen": 9750672,
      "step": 1895,
      "train_runtime": 6720.0762,
      "train_tokens_per_second": 1450.976
    },
    {
      "epoch": 1.4684057971014493,
      "grad_norm": 3.1435465812683105,
      "learning_rate": 2.5849571657545217e-05,
      "loss": 1.7053,
      "num_input_tokens_seen": 9775696,
      "step": 1900,
      "train_runtime": 6737.2145,
      "train_tokens_per_second": 1451.0
    },
    {
      "epoch": 1.4722705314009663,
      "grad_norm": 2.3528552055358887,
      "learning_rate": 2.5748464446516434e-05,
      "loss": 1.3401,
      "num_input_tokens_seen": 9797968,
      "step": 1905,
      "train_runtime": 6753.7099,
      "train_tokens_per_second": 1450.753
    },
    {
      "epoch": 1.4761352657004831,
      "grad_norm": 1.7146822214126587,
      "learning_rate": 2.5647344980883846e-05,
      "loss": 1.2983,
      "num_input_tokens_seen": 9823168,
      "step": 1910,
      "train_runtime": 6771.1975,
      "train_tokens_per_second": 1450.728
    },
    {
      "epoch": 1.48,
      "grad_norm": 2.164874792098999,
      "learning_rate": 2.5546214916275856e-05,
      "loss": 1.3875,
      "num_input_tokens_seen": 9848624,
      "step": 1915,
      "train_runtime": 6789.0079,
      "train_tokens_per_second": 1450.672
    },
    {
      "epoch": 1.4838647342995168,
      "grad_norm": 2.698176383972168,
      "learning_rate": 2.5445075908494383e-05,
      "loss": 1.3611,
      "num_input_tokens_seen": 9874304,
      "step": 1920,
      "train_runtime": 6806.6814,
      "train_tokens_per_second": 1450.678
    },
    {
      "epoch": 1.4877294685990339,
      "grad_norm": 2.222529888153076,
      "learning_rate": 2.534392961348779e-05,
      "loss": 1.0714,
      "num_input_tokens_seen": 9899520,
      "step": 1925,
      "train_runtime": 6823.8471,
      "train_tokens_per_second": 1450.724
    },
    {
      "epoch": 1.4915942028985507,
      "grad_norm": 1.843082070350647,
      "learning_rate": 2.5242777687323732e-05,
      "loss": 1.3881,
      "num_input_tokens_seen": 9924304,
      "step": 1930,
      "train_runtime": 6841.2682,
      "train_tokens_per_second": 1450.653
    },
    {
      "epoch": 1.4954589371980676,
      "grad_norm": 2.7383649349212646,
      "learning_rate": 2.5141621786162094e-05,
      "loss": 1.4208,
      "num_input_tokens_seen": 9950576,
      "step": 1935,
      "train_runtime": 6859.3473,
      "train_tokens_per_second": 1450.659
    },
    {
      "epoch": 1.4993236714975846,
      "grad_norm": 2.063163995742798,
      "learning_rate": 2.5040463566227818e-05,
      "loss": 1.529,
      "num_input_tokens_seen": 9974688,
      "step": 1940,
      "train_runtime": 6876.3705,
      "train_tokens_per_second": 1450.575
    },
    {
      "epoch": 1.5031884057971014,
      "grad_norm": 3.1040360927581787,
      "learning_rate": 2.493930468378381e-05,
      "loss": 1.6702,
      "num_input_tokens_seen": 9996928,
      "step": 1945,
      "train_runtime": 6892.3779,
      "train_tokens_per_second": 1450.432
    },
    {
      "epoch": 1.5070531400966183,
      "grad_norm": 3.426669120788574,
      "learning_rate": 2.483814679510386e-05,
      "loss": 1.3165,
      "num_input_tokens_seen": 10024720,
      "step": 1950,
      "train_runtime": 6911.3381,
      "train_tokens_per_second": 1450.475
    },
    {
      "epoch": 1.5109178743961351,
      "grad_norm": 1.399091124534607,
      "learning_rate": 2.4736991556445433e-05,
      "loss": 1.4199,
      "num_input_tokens_seen": 10055408,
      "step": 1955,
      "train_runtime": 6932.0384,
      "train_tokens_per_second": 1450.57
    },
    {
      "epoch": 1.5147826086956522,
      "grad_norm": 3.2863523960113525,
      "learning_rate": 2.4635840624022638e-05,
      "loss": 1.3962,
      "num_input_tokens_seen": 10082848,
      "step": 1960,
      "train_runtime": 6950.65,
      "train_tokens_per_second": 1450.634
    },
    {
      "epoch": 1.5186473429951692,
      "grad_norm": 2.250732183456421,
      "learning_rate": 2.4534695653979075e-05,
      "loss": 1.4891,
      "num_input_tokens_seen": 10106864,
      "step": 1965,
      "train_runtime": 6967.7156,
      "train_tokens_per_second": 1450.528
    },
    {
      "epoch": 1.522512077294686,
      "grad_norm": 2.4680376052856445,
      "learning_rate": 2.443355830236073e-05,
      "loss": 1.2145,
      "num_input_tokens_seen": 10133584,
      "step": 1970,
      "train_runtime": 6985.6925,
      "train_tokens_per_second": 1450.62
    },
    {
      "epoch": 1.526376811594203,
      "grad_norm": 1.9434194564819336,
      "learning_rate": 2.433243022508883e-05,
      "loss": 1.5111,
      "num_input_tokens_seen": 10163040,
      "step": 1975,
      "train_runtime": 7005.161,
      "train_tokens_per_second": 1450.793
    },
    {
      "epoch": 1.5302415458937197,
      "grad_norm": 1.8196252584457397,
      "learning_rate": 2.423131307793276e-05,
      "loss": 1.2455,
      "num_input_tokens_seen": 10189216,
      "step": 1980,
      "train_runtime": 7022.807,
      "train_tokens_per_second": 1450.875
    },
    {
      "epoch": 1.5341062801932366,
      "grad_norm": 2.1315805912017822,
      "learning_rate": 2.4130208516482958e-05,
      "loss": 1.134,
      "num_input_tokens_seen": 10214944,
      "step": 1985,
      "train_runtime": 7040.4923,
      "train_tokens_per_second": 1450.885
    },
    {
      "epoch": 1.5379710144927536,
      "grad_norm": 1.8629648685455322,
      "learning_rate": 2.40291181961238e-05,
      "loss": 1.2262,
      "num_input_tokens_seen": 10243392,
      "step": 1990,
      "train_runtime": 7059.3543,
      "train_tokens_per_second": 1451.038
    },
    {
      "epoch": 1.5418357487922705,
      "grad_norm": 2.7717223167419434,
      "learning_rate": 2.3928043772006468e-05,
      "loss": 1.3136,
      "num_input_tokens_seen": 10264880,
      "step": 1995,
      "train_runtime": 7074.9439,
      "train_tokens_per_second": 1450.878
    },
    {
      "epoch": 1.5457004830917875,
      "grad_norm": 2.2389047145843506,
      "learning_rate": 2.3826986899021884e-05,
      "loss": 1.3059,
      "num_input_tokens_seen": 10296096,
      "step": 2000,
      "train_runtime": 7095.5084,
      "train_tokens_per_second": 1451.072
    },
    {
      "epoch": 1.5495652173913044,
      "grad_norm": 1.8559423685073853,
      "learning_rate": 2.372594923177362e-05,
      "loss": 1.3371,
      "num_input_tokens_seen": 10321856,
      "step": 2005,
      "train_runtime": 7113.5447,
      "train_tokens_per_second": 1451.014
    },
    {
      "epoch": 1.5534299516908212,
      "grad_norm": 2.21710467338562,
      "learning_rate": 2.3624932424550795e-05,
      "loss": 1.2342,
      "num_input_tokens_seen": 10349392,
      "step": 2010,
      "train_runtime": 7132.348,
      "train_tokens_per_second": 1451.05
    },
    {
      "epoch": 1.557294685990338,
      "grad_norm": 2.0892174243927,
      "learning_rate": 2.3523938131300953e-05,
      "loss": 1.3181,
      "num_input_tokens_seen": 10377984,
      "step": 2015,
      "train_runtime": 7151.3981,
      "train_tokens_per_second": 1451.183
    },
    {
      "epoch": 1.561159420289855,
      "grad_norm": 3.131957769393921,
      "learning_rate": 2.342296800560305e-05,
      "loss": 1.5931,
      "num_input_tokens_seen": 10398704,
      "step": 2020,
      "train_runtime": 7166.8706,
      "train_tokens_per_second": 1450.941
    },
    {
      "epoch": 1.565024154589372,
      "grad_norm": 2.035142660140991,
      "learning_rate": 2.3322023700640332e-05,
      "loss": 1.3224,
      "num_input_tokens_seen": 10423232,
      "step": 2025,
      "train_runtime": 7183.7566,
      "train_tokens_per_second": 1450.944
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 3.0786566734313965,
      "learning_rate": 2.3221106869173294e-05,
      "loss": 1.4362,
      "num_input_tokens_seen": 10449024,
      "step": 2030,
      "train_runtime": 7201.6953,
      "train_tokens_per_second": 1450.912
    },
    {
      "epoch": 1.5727536231884058,
      "grad_norm": 2.111593246459961,
      "learning_rate": 2.3120219163512593e-05,
      "loss": 1.3769,
      "num_input_tokens_seen": 10478208,
      "step": 2035,
      "train_runtime": 7220.8351,
      "train_tokens_per_second": 1451.107
    },
    {
      "epoch": 1.5766183574879227,
      "grad_norm": 2.0520191192626953,
      "learning_rate": 2.301936223549201e-05,
      "loss": 1.5298,
      "num_input_tokens_seen": 10499376,
      "step": 2040,
      "train_runtime": 7236.3418,
      "train_tokens_per_second": 1450.923
    },
    {
      "epoch": 1.5804830917874395,
      "grad_norm": 1.8901021480560303,
      "learning_rate": 2.2918537736441424e-05,
      "loss": 1.2528,
      "num_input_tokens_seen": 10531232,
      "step": 2045,
      "train_runtime": 7257.0599,
      "train_tokens_per_second": 1451.171
    },
    {
      "epoch": 1.5843478260869566,
      "grad_norm": 2.466843366622925,
      "learning_rate": 2.281774731715974e-05,
      "loss": 1.6557,
      "num_input_tokens_seen": 10554128,
      "step": 2050,
      "train_runtime": 7273.6156,
      "train_tokens_per_second": 1451.015
    },
    {
      "epoch": 1.5882125603864734,
      "grad_norm": 2.5429723262786865,
      "learning_rate": 2.2716992627887852e-05,
      "loss": 1.7033,
      "num_input_tokens_seen": 10580656,
      "step": 2055,
      "train_runtime": 7291.9041,
      "train_tokens_per_second": 1451.014
    },
    {
      "epoch": 1.5920772946859905,
      "grad_norm": 2.156040906906128,
      "learning_rate": 2.261627531828169e-05,
      "loss": 1.3461,
      "num_input_tokens_seen": 10609056,
      "step": 2060,
      "train_runtime": 7310.9415,
      "train_tokens_per_second": 1451.12
    },
    {
      "epoch": 1.5959420289855073,
      "grad_norm": 2.08815598487854,
      "learning_rate": 2.251559703738515e-05,
      "loss": 1.2664,
      "num_input_tokens_seen": 10638016,
      "step": 2065,
      "train_runtime": 7330.2996,
      "train_tokens_per_second": 1451.239
    },
    {
      "epoch": 1.5998067632850241,
      "grad_norm": 2.520460367202759,
      "learning_rate": 2.2414959433603114e-05,
      "loss": 1.2273,
      "num_input_tokens_seen": 10661344,
      "step": 2070,
      "train_runtime": 7346.848,
      "train_tokens_per_second": 1451.145
    },
    {
      "epoch": 1.603671497584541,
      "grad_norm": 2.22696590423584,
      "learning_rate": 2.231436415467443e-05,
      "loss": 1.2209,
      "num_input_tokens_seen": 10693536,
      "step": 2075,
      "train_runtime": 7367.6545,
      "train_tokens_per_second": 1451.417
    },
    {
      "epoch": 1.6075362318840578,
      "grad_norm": 2.0706326961517334,
      "learning_rate": 2.221381284764499e-05,
      "loss": 1.4674,
      "num_input_tokens_seen": 10716816,
      "step": 2080,
      "train_runtime": 7384.2376,
      "train_tokens_per_second": 1451.31
    },
    {
      "epoch": 1.6114009661835749,
      "grad_norm": 2.7888145446777344,
      "learning_rate": 2.2113307158840744e-05,
      "loss": 1.5437,
      "num_input_tokens_seen": 10743136,
      "step": 2085,
      "train_runtime": 7402.228,
      "train_tokens_per_second": 1451.338
    },
    {
      "epoch": 1.615265700483092,
      "grad_norm": 2.325228691101074,
      "learning_rate": 2.2012848733840714e-05,
      "loss": 1.3924,
      "num_input_tokens_seen": 10765424,
      "step": 2090,
      "train_runtime": 7418.286,
      "train_tokens_per_second": 1451.201
    },
    {
      "epoch": 1.6191304347826088,
      "grad_norm": 2.2050511837005615,
      "learning_rate": 2.191243921745006e-05,
      "loss": 1.6901,
      "num_input_tokens_seen": 10789008,
      "step": 2095,
      "train_runtime": 7434.9775,
      "train_tokens_per_second": 1451.115
    },
    {
      "epoch": 1.6229951690821256,
      "grad_norm": 2.132606029510498,
      "learning_rate": 2.1812080253673194e-05,
      "loss": 1.2072,
      "num_input_tokens_seen": 10817728,
      "step": 2100,
      "train_runtime": 7453.9027,
      "train_tokens_per_second": 1451.284
    },
    {
      "epoch": 1.6268599033816424,
      "grad_norm": 2.3583250045776367,
      "learning_rate": 2.171177348568682e-05,
      "loss": 1.5331,
      "num_input_tokens_seen": 10843664,
      "step": 2105,
      "train_runtime": 7472.3574,
      "train_tokens_per_second": 1451.17
    },
    {
      "epoch": 1.6307246376811593,
      "grad_norm": 2.651578664779663,
      "learning_rate": 2.1611520555813038e-05,
      "loss": 1.5809,
      "num_input_tokens_seen": 10867424,
      "step": 2110,
      "train_runtime": 7489.2593,
      "train_tokens_per_second": 1451.068
    },
    {
      "epoch": 1.6345893719806763,
      "grad_norm": 2.5492515563964844,
      "learning_rate": 2.151132310549243e-05,
      "loss": 1.4709,
      "num_input_tokens_seen": 10890032,
      "step": 2115,
      "train_runtime": 7505.5304,
      "train_tokens_per_second": 1450.934
    },
    {
      "epoch": 1.6384541062801934,
      "grad_norm": 2.9082236289978027,
      "learning_rate": 2.1411182775257252e-05,
      "loss": 1.1769,
      "num_input_tokens_seen": 10914608,
      "step": 2120,
      "train_runtime": 7522.7809,
      "train_tokens_per_second": 1450.874
    },
    {
      "epoch": 1.6423188405797102,
      "grad_norm": 2.613694667816162,
      "learning_rate": 2.1311101204704525e-05,
      "loss": 1.1903,
      "num_input_tokens_seen": 10935664,
      "step": 2125,
      "train_runtime": 7538.1065,
      "train_tokens_per_second": 1450.718
    },
    {
      "epoch": 1.646183574879227,
      "grad_norm": 2.677786111831665,
      "learning_rate": 2.121108003246919e-05,
      "loss": 1.3069,
      "num_input_tokens_seen": 10956576,
      "step": 2130,
      "train_runtime": 7553.3392,
      "train_tokens_per_second": 1450.561
    },
    {
      "epoch": 1.650048309178744,
      "grad_norm": 2.2341837882995605,
      "learning_rate": 2.1111120896197274e-05,
      "loss": 1.4004,
      "num_input_tokens_seen": 10991008,
      "step": 2135,
      "train_runtime": 7575.3588,
      "train_tokens_per_second": 1450.889
    },
    {
      "epoch": 1.6539130434782607,
      "grad_norm": 2.3892173767089844,
      "learning_rate": 2.101122543251911e-05,
      "loss": 1.2588,
      "num_input_tokens_seen": 11017312,
      "step": 2140,
      "train_runtime": 7593.3121,
      "train_tokens_per_second": 1450.923
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 2.228363275527954,
      "learning_rate": 2.091139527702252e-05,
      "loss": 1.4694,
      "num_input_tokens_seen": 11046096,
      "step": 2145,
      "train_runtime": 7612.4057,
      "train_tokens_per_second": 1451.065
    },
    {
      "epoch": 1.6616425120772946,
      "grad_norm": 2.380434513092041,
      "learning_rate": 2.081163206422602e-05,
      "loss": 1.1039,
      "num_input_tokens_seen": 11071024,
      "step": 2150,
      "train_runtime": 7629.7149,
      "train_tokens_per_second": 1451.04
    },
    {
      "epoch": 1.6655072463768117,
      "grad_norm": 2.5892374515533447,
      "learning_rate": 2.071193742755208e-05,
      "loss": 1.4808,
      "num_input_tokens_seen": 11096336,
      "step": 2155,
      "train_runtime": 7647.132,
      "train_tokens_per_second": 1451.045
    },
    {
      "epoch": 1.6693719806763285,
      "grad_norm": 1.7971765995025635,
      "learning_rate": 2.0612312999300383e-05,
      "loss": 1.4019,
      "num_input_tokens_seen": 11128016,
      "step": 2160,
      "train_runtime": 7667.8313,
      "train_tokens_per_second": 1451.26
    },
    {
      "epoch": 1.6732367149758454,
      "grad_norm": 3.1264188289642334,
      "learning_rate": 2.0512760410621087e-05,
      "loss": 1.5457,
      "num_input_tokens_seen": 11151936,
      "step": 2165,
      "train_runtime": 7684.6037,
      "train_tokens_per_second": 1451.205
    },
    {
      "epoch": 1.6771014492753622,
      "grad_norm": 2.3339459896087646,
      "learning_rate": 2.04132812914881e-05,
      "loss": 1.509,
      "num_input_tokens_seen": 11177008,
      "step": 2170,
      "train_runtime": 7701.7655,
      "train_tokens_per_second": 1451.227
    },
    {
      "epoch": 1.6809661835748793,
      "grad_norm": 1.7570122480392456,
      "learning_rate": 2.0313877270672444e-05,
      "loss": 1.1792,
      "num_input_tokens_seen": 11203056,
      "step": 2175,
      "train_runtime": 7719.4354,
      "train_tokens_per_second": 1451.279
    },
    {
      "epoch": 1.684830917874396,
      "grad_norm": 1.8056830167770386,
      "learning_rate": 2.021454997571554e-05,
      "loss": 1.4622,
      "num_input_tokens_seen": 11226416,
      "step": 2180,
      "train_runtime": 7735.7137,
      "train_tokens_per_second": 1451.245
    },
    {
      "epoch": 1.6886956521739132,
      "grad_norm": 2.0753893852233887,
      "learning_rate": 2.0115301032902588e-05,
      "loss": 1.5572,
      "num_input_tokens_seen": 11251456,
      "step": 2185,
      "train_runtime": 7753.0502,
      "train_tokens_per_second": 1451.23
    },
    {
      "epoch": 1.69256038647343,
      "grad_norm": 2.4718031883239746,
      "learning_rate": 2.00161320672359e-05,
      "loss": 1.3833,
      "num_input_tokens_seen": 11279600,
      "step": 2190,
      "train_runtime": 7771.7687,
      "train_tokens_per_second": 1451.356
    },
    {
      "epoch": 1.6964251207729468,
      "grad_norm": 1.866843342781067,
      "learning_rate": 1.991704470240836e-05,
      "loss": 1.282,
      "num_input_tokens_seen": 11306704,
      "step": 2195,
      "train_runtime": 7789.966,
      "train_tokens_per_second": 1451.445
    },
    {
      "epoch": 1.7002898550724637,
      "grad_norm": 2.054692268371582,
      "learning_rate": 1.9818040560776782e-05,
      "loss": 1.4474,
      "num_input_tokens_seen": 11333168,
      "step": 2200,
      "train_runtime": 7807.8211,
      "train_tokens_per_second": 1451.515
    },
    {
      "epoch": 1.7041545893719807,
      "grad_norm": 2.238142728805542,
      "learning_rate": 1.9719121263335377e-05,
      "loss": 1.5611,
      "num_input_tokens_seen": 11351792,
      "step": 2205,
      "train_runtime": 7823.094,
      "train_tokens_per_second": 1451.062
    },
    {
      "epoch": 1.7080193236714976,
      "grad_norm": 1.7946829795837402,
      "learning_rate": 1.9620288429689182e-05,
      "loss": 1.0037,
      "num_input_tokens_seen": 11382112,
      "step": 2210,
      "train_runtime": 7842.7861,
      "train_tokens_per_second": 1451.284
    },
    {
      "epoch": 1.7118840579710146,
      "grad_norm": 2.0732264518737793,
      "learning_rate": 1.952154367802758e-05,
      "loss": 1.6413,
      "num_input_tokens_seen": 11406576,
      "step": 2215,
      "train_runtime": 7859.8384,
      "train_tokens_per_second": 1451.248
    },
    {
      "epoch": 1.7157487922705315,
      "grad_norm": 1.8836145401000977,
      "learning_rate": 1.94228886250978e-05,
      "loss": 1.464,
      "num_input_tokens_seen": 11433392,
      "step": 2220,
      "train_runtime": 7878.2032,
      "train_tokens_per_second": 1451.269
    },
    {
      "epoch": 1.7196135265700483,
      "grad_norm": 4.860753536224365,
      "learning_rate": 1.9324324886178412e-05,
      "loss": 1.1175,
      "num_input_tokens_seen": 11455344,
      "step": 2225,
      "train_runtime": 7894.1379,
      "train_tokens_per_second": 1451.12
    },
    {
      "epoch": 1.7234782608695651,
      "grad_norm": 3.7877893447875977,
      "learning_rate": 1.9225854075052902e-05,
      "loss": 1.543,
      "num_input_tokens_seen": 11480912,
      "step": 2230,
      "train_runtime": 7911.8853,
      "train_tokens_per_second": 1451.097
    },
    {
      "epoch": 1.727342995169082,
      "grad_norm": 2.204704523086548,
      "learning_rate": 1.9127477803983256e-05,
      "loss": 1.2381,
      "num_input_tokens_seen": 11504224,
      "step": 2235,
      "train_runtime": 7928.115,
      "train_tokens_per_second": 1451.067
    },
    {
      "epoch": 1.731207729468599,
      "grad_norm": 2.300243377685547,
      "learning_rate": 1.9029197683683574e-05,
      "loss": 1.4445,
      "num_input_tokens_seen": 11529792,
      "step": 2240,
      "train_runtime": 7945.7965,
      "train_tokens_per_second": 1451.056
    },
    {
      "epoch": 1.735072463768116,
      "grad_norm": 3.2840592861175537,
      "learning_rate": 1.893101532329366e-05,
      "loss": 1.6538,
      "num_input_tokens_seen": 11551344,
      "step": 2245,
      "train_runtime": 7961.3925,
      "train_tokens_per_second": 1450.92
    },
    {
      "epoch": 1.738937198067633,
      "grad_norm": 2.53975248336792,
      "learning_rate": 1.8832932330352683e-05,
      "loss": 1.747,
      "num_input_tokens_seen": 11577024,
      "step": 2250,
      "train_runtime": 7979.0954,
      "train_tokens_per_second": 1450.919
    },
    {
      "epoch": 1.7428019323671498,
      "grad_norm": 1.8018323183059692,
      "learning_rate": 1.8734950310772898e-05,
      "loss": 1.3853,
      "num_input_tokens_seen": 11609296,
      "step": 2255,
      "train_runtime": 7999.644,
      "train_tokens_per_second": 1451.227
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 3.2579689025878906,
      "learning_rate": 1.863707086881331e-05,
      "loss": 1.2471,
      "num_input_tokens_seen": 11634848,
      "step": 2260,
      "train_runtime": 8017.2051,
      "train_tokens_per_second": 1451.235
    },
    {
      "epoch": 1.7505314009661834,
      "grad_norm": 1.9969762563705444,
      "learning_rate": 1.8539295607053436e-05,
      "loss": 1.467,
      "num_input_tokens_seen": 11661168,
      "step": 2265,
      "train_runtime": 8034.8879,
      "train_tokens_per_second": 1451.317
    },
    {
      "epoch": 1.7543961352657005,
      "grad_norm": 2.639521360397339,
      "learning_rate": 1.8441626126367013e-05,
      "loss": 1.617,
      "num_input_tokens_seen": 11682032,
      "step": 2270,
      "train_runtime": 8050.5914,
      "train_tokens_per_second": 1451.077
    },
    {
      "epoch": 1.7582608695652175,
      "grad_norm": 1.7903980016708374,
      "learning_rate": 1.8344064025895865e-05,
      "loss": 1.2559,
      "num_input_tokens_seen": 11707568,
      "step": 2275,
      "train_runtime": 8067.9133,
      "train_tokens_per_second": 1451.127
    },
    {
      "epoch": 1.7621256038647344,
      "grad_norm": 1.8908926248550415,
      "learning_rate": 1.8246610903023658e-05,
      "loss": 1.3904,
      "num_input_tokens_seen": 11736256,
      "step": 2280,
      "train_runtime": 8086.8863,
      "train_tokens_per_second": 1451.27
    },
    {
      "epoch": 1.7659903381642512,
      "grad_norm": 2.2427377700805664,
      "learning_rate": 1.8149268353349797e-05,
      "loss": 1.4,
      "num_input_tokens_seen": 11760528,
      "step": 2285,
      "train_runtime": 8103.7606,
      "train_tokens_per_second": 1451.243
    },
    {
      "epoch": 1.769855072463768,
      "grad_norm": 2.1520955562591553,
      "learning_rate": 1.8052037970663217e-05,
      "loss": 1.7415,
      "num_input_tokens_seen": 11785472,
      "step": 2290,
      "train_runtime": 8121.3959,
      "train_tokens_per_second": 1451.163
    },
    {
      "epoch": 1.773719806763285,
      "grad_norm": 2.6606485843658447,
      "learning_rate": 1.7954921346916408e-05,
      "loss": 1.4842,
      "num_input_tokens_seen": 11813072,
      "step": 2295,
      "train_runtime": 8139.44,
      "train_tokens_per_second": 1451.337
    },
    {
      "epoch": 1.777584541062802,
      "grad_norm": 2.610778570175171,
      "learning_rate": 1.785792007219924e-05,
      "loss": 1.509,
      "num_input_tokens_seen": 11836224,
      "step": 2300,
      "train_runtime": 8155.678,
      "train_tokens_per_second": 1451.286
    },
    {
      "epoch": 1.781449275362319,
      "grad_norm": 2.3581180572509766,
      "learning_rate": 1.7761035734713004e-05,
      "loss": 1.3782,
      "num_input_tokens_seen": 11862336,
      "step": 2305,
      "train_runtime": 8174.2311,
      "train_tokens_per_second": 1451.187
    },
    {
      "epoch": 1.7853140096618358,
      "grad_norm": 2.3382010459899902,
      "learning_rate": 1.7664269920744348e-05,
      "loss": 1.4122,
      "num_input_tokens_seen": 11887264,
      "step": 2310,
      "train_runtime": 8191.8215,
      "train_tokens_per_second": 1451.114
    },
    {
      "epoch": 1.7891787439613527,
      "grad_norm": 2.2752017974853516,
      "learning_rate": 1.7567624214639362e-05,
      "loss": 1.3858,
      "num_input_tokens_seen": 11912992,
      "step": 2315,
      "train_runtime": 8209.4316,
      "train_tokens_per_second": 1451.135
    },
    {
      "epoch": 1.7930434782608695,
      "grad_norm": 3.544787883758545,
      "learning_rate": 1.747110019877759e-05,
      "loss": 1.3837,
      "num_input_tokens_seen": 11937872,
      "step": 2320,
      "train_runtime": 8226.4352,
      "train_tokens_per_second": 1451.16
    },
    {
      "epoch": 1.7969082125603864,
      "grad_norm": 2.3305716514587402,
      "learning_rate": 1.7374699453546172e-05,
      "loss": 1.4493,
      "num_input_tokens_seen": 11960560,
      "step": 2325,
      "train_runtime": 8242.9182,
      "train_tokens_per_second": 1451.01
    },
    {
      "epoch": 1.8007729468599034,
      "grad_norm": 2.0009756088256836,
      "learning_rate": 1.7278423557313893e-05,
      "loss": 1.4858,
      "num_input_tokens_seen": 11985760,
      "step": 2330,
      "train_runtime": 8260.4776,
      "train_tokens_per_second": 1450.977
    },
    {
      "epoch": 1.8046376811594202,
      "grad_norm": 1.630458116531372,
      "learning_rate": 1.718227408640544e-05,
      "loss": 1.2606,
      "num_input_tokens_seen": 12018560,
      "step": 2335,
      "train_runtime": 8281.6769,
      "train_tokens_per_second": 1451.223
    },
    {
      "epoch": 1.8085024154589373,
      "grad_norm": 1.691657304763794,
      "learning_rate": 1.7086252615075487e-05,
      "loss": 1.369,
      "num_input_tokens_seen": 12047632,
      "step": 2340,
      "train_runtime": 8300.677,
      "train_tokens_per_second": 1451.404
    },
    {
      "epoch": 1.8123671497584541,
      "grad_norm": 2.480085849761963,
      "learning_rate": 1.699036071548302e-05,
      "loss": 1.5609,
      "num_input_tokens_seen": 12074672,
      "step": 2345,
      "train_runtime": 8319.001,
      "train_tokens_per_second": 1451.457
    },
    {
      "epoch": 1.816231884057971,
      "grad_norm": 2.592681884765625,
      "learning_rate": 1.6894599957665503e-05,
      "loss": 1.6882,
      "num_input_tokens_seen": 12100864,
      "step": 2350,
      "train_runtime": 8337.1856,
      "train_tokens_per_second": 1451.433
    },
    {
      "epoch": 1.8200966183574878,
      "grad_norm": 3.7318854331970215,
      "learning_rate": 1.6798971909513245e-05,
      "loss": 1.1954,
      "num_input_tokens_seen": 12127424,
      "step": 2355,
      "train_runtime": 8355.0394,
      "train_tokens_per_second": 1451.51
    },
    {
      "epoch": 1.8239613526570049,
      "grad_norm": 2.4647598266601562,
      "learning_rate": 1.670347813674369e-05,
      "loss": 1.367,
      "num_input_tokens_seen": 12153248,
      "step": 2360,
      "train_runtime": 8372.6066,
      "train_tokens_per_second": 1451.549
    },
    {
      "epoch": 1.8278260869565217,
      "grad_norm": 2.415402412414551,
      "learning_rate": 1.660812020287581e-05,
      "loss": 1.6971,
      "num_input_tokens_seen": 12178144,
      "step": 2365,
      "train_runtime": 8389.5893,
      "train_tokens_per_second": 1451.578
    },
    {
      "epoch": 1.8316908212560388,
      "grad_norm": 3.4335975646972656,
      "learning_rate": 1.651289966920445e-05,
      "loss": 1.69,
      "num_input_tokens_seen": 12200768,
      "step": 2370,
      "train_runtime": 8406.0458,
      "train_tokens_per_second": 1451.428
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 2.020456552505493,
      "learning_rate": 1.6417818094774838e-05,
      "loss": 1.5234,
      "num_input_tokens_seen": 12221488,
      "step": 2375,
      "train_runtime": 8421.5756,
      "train_tokens_per_second": 1451.212
    },
    {
      "epoch": 1.8394202898550724,
      "grad_norm": 2.9931600093841553,
      "learning_rate": 1.6322877036357005e-05,
      "loss": 1.6226,
      "num_input_tokens_seen": 12251088,
      "step": 2380,
      "train_runtime": 8440.9485,
      "train_tokens_per_second": 1451.388
    },
    {
      "epoch": 1.8432850241545893,
      "grad_norm": 2.030576229095459,
      "learning_rate": 1.6228078048420328e-05,
      "loss": 1.4584,
      "num_input_tokens_seen": 12279280,
      "step": 2385,
      "train_runtime": 8459.6132,
      "train_tokens_per_second": 1451.518
    },
    {
      "epoch": 1.8471497584541061,
      "grad_norm": 2.3208799362182617,
      "learning_rate": 1.6133422683108047e-05,
      "loss": 1.2512,
      "num_input_tokens_seen": 12301504,
      "step": 2390,
      "train_runtime": 8475.5196,
      "train_tokens_per_second": 1451.416
    },
    {
      "epoch": 1.8510144927536232,
      "grad_norm": 2.711249351501465,
      "learning_rate": 1.603891249021187e-05,
      "loss": 1.3482,
      "num_input_tokens_seen": 12324896,
      "step": 2395,
      "train_runtime": 8492.1904,
      "train_tokens_per_second": 1451.321
    },
    {
      "epoch": 1.8548792270531402,
      "grad_norm": 3.3453423976898193,
      "learning_rate": 1.5944549017146627e-05,
      "loss": 1.5472,
      "num_input_tokens_seen": 12355792,
      "step": 2400,
      "train_runtime": 8512.4296,
      "train_tokens_per_second": 1451.5
    },
    {
      "epoch": 1.858743961352657,
      "grad_norm": 2.587120294570923,
      "learning_rate": 1.5850333808924877e-05,
      "loss": 1.4409,
      "num_input_tokens_seen": 12378128,
      "step": 2405,
      "train_runtime": 8528.8272,
      "train_tokens_per_second": 1451.328
    },
    {
      "epoch": 1.862608695652174,
      "grad_norm": 3.449064016342163,
      "learning_rate": 1.575626840813164e-05,
      "loss": 2.0087,
      "num_input_tokens_seen": 12397744,
      "step": 2410,
      "train_runtime": 8543.8777,
      "train_tokens_per_second": 1451.068
    },
    {
      "epoch": 1.8664734299516907,
      "grad_norm": 2.4646196365356445,
      "learning_rate": 1.566235435489915e-05,
      "loss": 1.3748,
      "num_input_tokens_seen": 12419936,
      "step": 2415,
      "train_runtime": 8559.5705,
      "train_tokens_per_second": 1451.0
    },
    {
      "epoch": 1.8703381642512076,
      "grad_norm": 1.726098895072937,
      "learning_rate": 1.556859318688163e-05,
      "loss": 0.9193,
      "num_input_tokens_seen": 12447168,
      "step": 2420,
      "train_runtime": 8577.7104,
      "train_tokens_per_second": 1451.106
    },
    {
      "epoch": 1.8742028985507246,
      "grad_norm": 2.56935715675354,
      "learning_rate": 1.5474986439230116e-05,
      "loss": 1.5019,
      "num_input_tokens_seen": 12466736,
      "step": 2425,
      "train_runtime": 8592.8438,
      "train_tokens_per_second": 1450.828
    },
    {
      "epoch": 1.8780676328502417,
      "grad_norm": 2.2024645805358887,
      "learning_rate": 1.5381535644567296e-05,
      "loss": 1.4449,
      "num_input_tokens_seen": 12497408,
      "step": 2430,
      "train_runtime": 8612.7839,
      "train_tokens_per_second": 1451.03
    },
    {
      "epoch": 1.8819323671497585,
      "grad_norm": 2.4893431663513184,
      "learning_rate": 1.528824233296247e-05,
      "loss": 1.5826,
      "num_input_tokens_seen": 12525200,
      "step": 2435,
      "train_runtime": 8631.551,
      "train_tokens_per_second": 1451.095
    },
    {
      "epoch": 1.8857971014492754,
      "grad_norm": 2.6009929180145264,
      "learning_rate": 1.5195108031906452e-05,
      "loss": 1.2667,
      "num_input_tokens_seen": 12548720,
      "step": 2440,
      "train_runtime": 8648.0821,
      "train_tokens_per_second": 1451.041
    },
    {
      "epoch": 1.8896618357487922,
      "grad_norm": 1.9501386880874634,
      "learning_rate": 1.51021342662866e-05,
      "loss": 1.194,
      "num_input_tokens_seen": 12575536,
      "step": 2445,
      "train_runtime": 8666.0201,
      "train_tokens_per_second": 1451.132
    },
    {
      "epoch": 1.893526570048309,
      "grad_norm": 2.094646692276001,
      "learning_rate": 1.5009322558361788e-05,
      "loss": 1.5605,
      "num_input_tokens_seen": 12599568,
      "step": 2450,
      "train_runtime": 8683.0805,
      "train_tokens_per_second": 1451.048
    },
    {
      "epoch": 1.897391304347826,
      "grad_norm": 2.363896131515503,
      "learning_rate": 1.4916674427737548e-05,
      "loss": 1.4452,
      "num_input_tokens_seen": 12625120,
      "step": 2455,
      "train_runtime": 8700.4279,
      "train_tokens_per_second": 1451.092
    },
    {
      "epoch": 1.9012560386473432,
      "grad_norm": 2.875910520553589,
      "learning_rate": 1.4824191391341163e-05,
      "loss": 1.5105,
      "num_input_tokens_seen": 12644064,
      "step": 2460,
      "train_runtime": 8715.0974,
      "train_tokens_per_second": 1450.823
    },
    {
      "epoch": 1.90512077294686,
      "grad_norm": 2.0676863193511963,
      "learning_rate": 1.4731874963396808e-05,
      "loss": 1.4046,
      "num_input_tokens_seen": 12670384,
      "step": 2465,
      "train_runtime": 8732.9761,
      "train_tokens_per_second": 1450.867
    },
    {
      "epoch": 1.9089855072463768,
      "grad_norm": 2.922102689743042,
      "learning_rate": 1.4639726655400804e-05,
      "loss": 1.374,
      "num_input_tokens_seen": 12695056,
      "step": 2470,
      "train_runtime": 8749.8541,
      "train_tokens_per_second": 1450.888
    },
    {
      "epoch": 1.9128502415458937,
      "grad_norm": 2.354790449142456,
      "learning_rate": 1.4547747976096826e-05,
      "loss": 1.6734,
      "num_input_tokens_seen": 12719632,
      "step": 2475,
      "train_runtime": 8766.8973,
      "train_tokens_per_second": 1450.87
    },
    {
      "epoch": 1.9167149758454105,
      "grad_norm": 2.462388515472412,
      "learning_rate": 1.4455940431451242e-05,
      "loss": 1.4535,
      "num_input_tokens_seen": 12743968,
      "step": 2480,
      "train_runtime": 8783.7795,
      "train_tokens_per_second": 1450.852
    },
    {
      "epoch": 1.9205797101449276,
      "grad_norm": 2.0717220306396484,
      "learning_rate": 1.4364305524628397e-05,
      "loss": 1.2525,
      "num_input_tokens_seen": 12773472,
      "step": 2485,
      "train_runtime": 8803.1989,
      "train_tokens_per_second": 1451.003
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 2.728917121887207,
      "learning_rate": 1.4272844755966064e-05,
      "loss": 1.1322,
      "num_input_tokens_seen": 12802224,
      "step": 2490,
      "train_runtime": 8822.2769,
      "train_tokens_per_second": 1451.125
    },
    {
      "epoch": 1.9283091787439615,
      "grad_norm": 2.138838768005371,
      "learning_rate": 1.4181559622950868e-05,
      "loss": 1.6324,
      "num_input_tokens_seen": 12825168,
      "step": 2495,
      "train_runtime": 8838.6462,
      "train_tokens_per_second": 1451.033
    },
    {
      "epoch": 1.9321739130434783,
      "grad_norm": 2.2380831241607666,
      "learning_rate": 1.4090451620193717e-05,
      "loss": 1.4,
      "num_input_tokens_seen": 12846464,
      "step": 2500,
      "train_runtime": 8854.4402,
      "train_tokens_per_second": 1450.85
    },
    {
      "epoch": 1.9360386473429951,
      "grad_norm": 1.9365708827972412,
      "learning_rate": 1.399952223940537e-05,
      "loss": 0.9369,
      "num_input_tokens_seen": 12872976,
      "step": 2505,
      "train_runtime": 8872.6258,
      "train_tokens_per_second": 1450.864
    },
    {
      "epoch": 1.939903381642512,
      "grad_norm": 2.151789903640747,
      "learning_rate": 1.3908772969372042e-05,
      "loss": 1.5329,
      "num_input_tokens_seen": 12898288,
      "step": 2510,
      "train_runtime": 8890.1774,
      "train_tokens_per_second": 1450.847
    },
    {
      "epoch": 1.943768115942029,
      "grad_norm": 1.6574066877365112,
      "learning_rate": 1.3818205295930963e-05,
      "loss": 1.5081,
      "num_input_tokens_seen": 12919984,
      "step": 2515,
      "train_runtime": 8905.8364,
      "train_tokens_per_second": 1450.732
    },
    {
      "epoch": 1.9476328502415459,
      "grad_norm": 2.11118483543396,
      "learning_rate": 1.3727820701946118e-05,
      "loss": 1.1339,
      "num_input_tokens_seen": 12949088,
      "step": 2520,
      "train_runtime": 8924.9919,
      "train_tokens_per_second": 1450.88
    },
    {
      "epoch": 1.951497584541063,
      "grad_norm": 2.30607008934021,
      "learning_rate": 1.363762066728388e-05,
      "loss": 1.7893,
      "num_input_tokens_seen": 12972736,
      "step": 2525,
      "train_runtime": 8941.3152,
      "train_tokens_per_second": 1450.876
    },
    {
      "epoch": 1.9553623188405798,
      "grad_norm": 2.2627642154693604,
      "learning_rate": 1.3547606668788882e-05,
      "loss": 1.3891,
      "num_input_tokens_seen": 13001568,
      "step": 2530,
      "train_runtime": 8960.5613,
      "train_tokens_per_second": 1450.977
    },
    {
      "epoch": 1.9592270531400966,
      "grad_norm": 2.4544830322265625,
      "learning_rate": 1.3457780180259772e-05,
      "loss": 1.5153,
      "num_input_tokens_seen": 13022576,
      "step": 2535,
      "train_runtime": 8976.0099,
      "train_tokens_per_second": 1450.82
    },
    {
      "epoch": 1.9630917874396134,
      "grad_norm": 2.0892975330352783,
      "learning_rate": 1.3368142672425094e-05,
      "loss": 1.3991,
      "num_input_tokens_seen": 13048416,
      "step": 2540,
      "train_runtime": 8993.3072,
      "train_tokens_per_second": 1450.903
    },
    {
      "epoch": 1.9669565217391303,
      "grad_norm": 2.832496166229248,
      "learning_rate": 1.32786956129192e-05,
      "loss": 1.3908,
      "num_input_tokens_seen": 13074512,
      "step": 2545,
      "train_runtime": 9011.2417,
      "train_tokens_per_second": 1450.911
    },
    {
      "epoch": 1.9708212560386473,
      "grad_norm": 3.5454306602478027,
      "learning_rate": 1.3189440466258263e-05,
      "loss": 1.5141,
      "num_input_tokens_seen": 13096976,
      "step": 2550,
      "train_runtime": 9027.2692,
      "train_tokens_per_second": 1450.824
    },
    {
      "epoch": 1.9746859903381644,
      "grad_norm": 2.5515685081481934,
      "learning_rate": 1.3100378693816248e-05,
      "loss": 1.3608,
      "num_input_tokens_seen": 13122480,
      "step": 2555,
      "train_runtime": 9044.6156,
      "train_tokens_per_second": 1450.861
    },
    {
      "epoch": 1.9785507246376812,
      "grad_norm": 2.419720411300659,
      "learning_rate": 1.3011511753801021e-05,
      "loss": 1.4235,
      "num_input_tokens_seen": 13152144,
      "step": 2560,
      "train_runtime": 9064.264,
      "train_tokens_per_second": 1450.989
    },
    {
      "epoch": 1.982415458937198,
      "grad_norm": 2.4829866886138916,
      "learning_rate": 1.2922841101230429e-05,
      "loss": 1.1488,
      "num_input_tokens_seen": 13181424,
      "step": 2565,
      "train_runtime": 9083.6094,
      "train_tokens_per_second": 1451.122
    },
    {
      "epoch": 1.986280193236715,
      "grad_norm": 2.3928780555725098,
      "learning_rate": 1.2834368187908541e-05,
      "loss": 1.5917,
      "num_input_tokens_seen": 13206144,
      "step": 2570,
      "train_runtime": 9100.852,
      "train_tokens_per_second": 1451.089
    },
    {
      "epoch": 1.9901449275362317,
      "grad_norm": 1.4393404722213745,
      "learning_rate": 1.2746094462401803e-05,
      "loss": 1.4297,
      "num_input_tokens_seen": 13231552,
      "step": 2575,
      "train_runtime": 9118.5629,
      "train_tokens_per_second": 1451.057
    },
    {
      "epoch": 1.9940096618357488,
      "grad_norm": 2.9384210109710693,
      "learning_rate": 1.2658021370015408e-05,
      "loss": 1.5953,
      "num_input_tokens_seen": 13257600,
      "step": 2580,
      "train_runtime": 9136.3378,
      "train_tokens_per_second": 1451.085
    },
    {
      "epoch": 1.9978743961352659,
      "grad_norm": 2.072672128677368,
      "learning_rate": 1.2570150352769533e-05,
      "loss": 1.6893,
      "num_input_tokens_seen": 13285872,
      "step": 2585,
      "train_runtime": 9154.8028,
      "train_tokens_per_second": 1451.246
    },
    {
      "epoch": 2.0015458937198067,
      "grad_norm": 3.097930669784546,
      "learning_rate": 1.248248284937582e-05,
      "loss": 1.1388,
      "num_input_tokens_seen": 13309432,
      "step": 2590,
      "train_runtime": 9171.3192,
      "train_tokens_per_second": 1451.201
    },
    {
      "epoch": 2.0054106280193236,
      "grad_norm": 1.7344545125961304,
      "learning_rate": 1.2395020295213761e-05,
      "loss": 1.6229,
      "num_input_tokens_seen": 13336088,
      "step": 2595,
      "train_runtime": 9189.666,
      "train_tokens_per_second": 1451.205
    },
    {
      "epoch": 2.0092753623188404,
      "grad_norm": 2.1624269485473633,
      "learning_rate": 1.230776412230723e-05,
      "loss": 1.4812,
      "num_input_tokens_seen": 13361336,
      "step": 2600,
      "train_runtime": 9207.0257,
      "train_tokens_per_second": 1451.211
    },
    {
      "epoch": 2.0131400966183577,
      "grad_norm": 3.2725818157196045,
      "learning_rate": 1.2220715759300996e-05,
      "loss": 1.3148,
      "num_input_tokens_seen": 13387352,
      "step": 2605,
      "train_runtime": 9225.6131,
      "train_tokens_per_second": 1451.107
    },
    {
      "epoch": 2.0170048309178745,
      "grad_norm": 2.3776824474334717,
      "learning_rate": 1.213387663143739e-05,
      "loss": 1.4556,
      "num_input_tokens_seen": 13408840,
      "step": 2610,
      "train_runtime": 9241.0156,
      "train_tokens_per_second": 1451.014
    },
    {
      "epoch": 2.0208695652173914,
      "grad_norm": 2.2199316024780273,
      "learning_rate": 1.2047248160532901e-05,
      "loss": 1.3945,
      "num_input_tokens_seen": 13433000,
      "step": 2615,
      "train_runtime": 9257.7836,
      "train_tokens_per_second": 1450.995
    },
    {
      "epoch": 2.024734299516908,
      "grad_norm": 2.480539321899414,
      "learning_rate": 1.1960831764954972e-05,
      "loss": 1.4171,
      "num_input_tokens_seen": 13454744,
      "step": 2620,
      "train_runtime": 9273.5218,
      "train_tokens_per_second": 1450.877
    },
    {
      "epoch": 2.028599033816425,
      "grad_norm": 2.712347984313965,
      "learning_rate": 1.18746288595987e-05,
      "loss": 1.2888,
      "num_input_tokens_seen": 13483128,
      "step": 2625,
      "train_runtime": 9292.3246,
      "train_tokens_per_second": 1450.996
    },
    {
      "epoch": 2.032463768115942,
      "grad_norm": 3.0577797889709473,
      "learning_rate": 1.1788640855863737e-05,
      "loss": 1.5793,
      "num_input_tokens_seen": 13508200,
      "step": 2630,
      "train_runtime": 9310.3118,
      "train_tokens_per_second": 1450.886
    },
    {
      "epoch": 2.036328502415459,
      "grad_norm": 3.1532928943634033,
      "learning_rate": 1.1702869161631138e-05,
      "loss": 1.1625,
      "num_input_tokens_seen": 13530312,
      "step": 2635,
      "train_runtime": 9326.1872,
      "train_tokens_per_second": 1450.787
    },
    {
      "epoch": 2.040193236714976,
      "grad_norm": 2.663496971130371,
      "learning_rate": 1.1617315181240344e-05,
      "loss": 1.1903,
      "num_input_tokens_seen": 13551928,
      "step": 2640,
      "train_runtime": 9342.2432,
      "train_tokens_per_second": 1450.607
    },
    {
      "epoch": 2.044057971014493,
      "grad_norm": 2.0428519248962402,
      "learning_rate": 1.1531980315466126e-05,
      "loss": 1.4336,
      "num_input_tokens_seen": 13581752,
      "step": 2645,
      "train_runtime": 9361.7475,
      "train_tokens_per_second": 1450.771
    },
    {
      "epoch": 2.0479227053140097,
      "grad_norm": 2.740919589996338,
      "learning_rate": 1.144686596149574e-05,
      "loss": 1.5459,
      "num_input_tokens_seen": 13605592,
      "step": 2650,
      "train_runtime": 9378.3675,
      "train_tokens_per_second": 1450.742
    },
    {
      "epoch": 2.0517874396135265,
      "grad_norm": 2.446817636489868,
      "learning_rate": 1.1361973512905966e-05,
      "loss": 1.3768,
      "num_input_tokens_seen": 13628600,
      "step": 2655,
      "train_runtime": 9394.7458,
      "train_tokens_per_second": 1450.662
    },
    {
      "epoch": 2.0556521739130433,
      "grad_norm": 3.7116949558258057,
      "learning_rate": 1.127730435964036e-05,
      "loss": 1.8453,
      "num_input_tokens_seen": 13649992,
      "step": 2660,
      "train_runtime": 9410.5554,
      "train_tokens_per_second": 1450.498
    },
    {
      "epoch": 2.05951690821256,
      "grad_norm": 2.283123254776001,
      "learning_rate": 1.1192859887986432e-05,
      "loss": 1.171,
      "num_input_tokens_seen": 13672344,
      "step": 2665,
      "train_runtime": 9426.4602,
      "train_tokens_per_second": 1450.422
    },
    {
      "epoch": 2.0633816425120775,
      "grad_norm": 2.1678292751312256,
      "learning_rate": 1.1108641480553012e-05,
      "loss": 1.2196,
      "num_input_tokens_seen": 13699688,
      "step": 2670,
      "train_runtime": 9444.7251,
      "train_tokens_per_second": 1450.512
    },
    {
      "epoch": 2.0672463768115943,
      "grad_norm": 2.0902230739593506,
      "learning_rate": 1.1024650516247565e-05,
      "loss": 1.444,
      "num_input_tokens_seen": 13719192,
      "step": 2675,
      "train_runtime": 9459.5598,
      "train_tokens_per_second": 1450.299
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 2.8817005157470703,
      "learning_rate": 1.0940888370253647e-05,
      "loss": 1.2429,
      "num_input_tokens_seen": 13745464,
      "step": 2680,
      "train_runtime": 9477.5139,
      "train_tokens_per_second": 1450.324
    },
    {
      "epoch": 2.074975845410628,
      "grad_norm": 2.2528932094573975,
      "learning_rate": 1.085735641400835e-05,
      "loss": 1.3682,
      "num_input_tokens_seen": 13772184,
      "step": 2685,
      "train_runtime": 9495.5165,
      "train_tokens_per_second": 1450.388
    },
    {
      "epoch": 2.078840579710145,
      "grad_norm": 2.2641050815582275,
      "learning_rate": 1.0774056015179876e-05,
      "loss": 1.7065,
      "num_input_tokens_seen": 13794424,
      "step": 2690,
      "train_runtime": 9511.5829,
      "train_tokens_per_second": 1450.276
    },
    {
      "epoch": 2.0827053140096616,
      "grad_norm": 1.9725182056427002,
      "learning_rate": 1.069098853764515e-05,
      "loss": 1.4214,
      "num_input_tokens_seen": 13819992,
      "step": 2695,
      "train_runtime": 9528.9687,
      "train_tokens_per_second": 1450.314
    },
    {
      "epoch": 2.086570048309179,
      "grad_norm": 2.8938088417053223,
      "learning_rate": 1.0608155341467485e-05,
      "loss": 1.4776,
      "num_input_tokens_seen": 13847928,
      "step": 2700,
      "train_runtime": 9547.7863,
      "train_tokens_per_second": 1450.381
    },
    {
      "epoch": 2.0904347826086958,
      "grad_norm": 1.9232890605926514,
      "learning_rate": 1.052555778287428e-05,
      "loss": 1.4688,
      "num_input_tokens_seen": 13876776,
      "step": 2705,
      "train_runtime": 9567.1196,
      "train_tokens_per_second": 1450.465
    },
    {
      "epoch": 2.0942995169082126,
      "grad_norm": 2.4024598598480225,
      "learning_rate": 1.0443197214234856e-05,
      "loss": 1.3952,
      "num_input_tokens_seen": 13906856,
      "step": 2710,
      "train_runtime": 9587.1185,
      "train_tokens_per_second": 1450.577
    },
    {
      "epoch": 2.0981642512077294,
      "grad_norm": 3.0344746112823486,
      "learning_rate": 1.0361074984038308e-05,
      "loss": 1.2378,
      "num_input_tokens_seen": 13938824,
      "step": 2715,
      "train_runtime": 9607.825,
      "train_tokens_per_second": 1450.778
    },
    {
      "epoch": 2.1020289855072463,
      "grad_norm": 2.803091287612915,
      "learning_rate": 1.027919243687141e-05,
      "loss": 1.2043,
      "num_input_tokens_seen": 13965832,
      "step": 2720,
      "train_runtime": 9625.9316,
      "train_tokens_per_second": 1450.855
    },
    {
      "epoch": 2.105893719806763,
      "grad_norm": 2.2950737476348877,
      "learning_rate": 1.0197550913396603e-05,
      "loss": 0.9927,
      "num_input_tokens_seen": 13993192,
      "step": 2725,
      "train_runtime": 9644.4165,
      "train_tokens_per_second": 1450.911
    },
    {
      "epoch": 2.1097584541062804,
      "grad_norm": 3.218421459197998,
      "learning_rate": 1.0116151750330036e-05,
      "loss": 1.559,
      "num_input_tokens_seen": 14015368,
      "step": 2730,
      "train_runtime": 9660.3073,
      "train_tokens_per_second": 1450.82
    },
    {
      "epoch": 2.113623188405797,
      "grad_norm": 1.7129322290420532,
      "learning_rate": 1.003499628041972e-05,
      "loss": 1.2637,
      "num_input_tokens_seen": 14039784,
      "step": 2735,
      "train_runtime": 9677.2298,
      "train_tokens_per_second": 1450.806
    },
    {
      "epoch": 2.117487922705314,
      "grad_norm": 1.9507458209991455,
      "learning_rate": 9.954085832423676e-06,
      "loss": 1.4644,
      "num_input_tokens_seen": 14070264,
      "step": 2740,
      "train_runtime": 9697.0989,
      "train_tokens_per_second": 1450.977
    },
    {
      "epoch": 2.121352657004831,
      "grad_norm": 2.5997424125671387,
      "learning_rate": 9.873421731088154e-06,
      "loss": 1.3687,
      "num_input_tokens_seen": 14095624,
      "step": 2745,
      "train_runtime": 9714.7032,
      "train_tokens_per_second": 1450.958
    },
    {
      "epoch": 2.1252173913043477,
      "grad_norm": 2.490507125854492,
      "learning_rate": 9.793005297126001e-06,
      "loss": 1.2839,
      "num_input_tokens_seen": 14124280,
      "step": 2750,
      "train_runtime": 9733.7475,
      "train_tokens_per_second": 1451.063
    },
    {
      "epoch": 2.1290821256038646,
      "grad_norm": 2.792820453643799,
      "learning_rate": 9.712837847194997e-06,
      "loss": 1.4339,
      "num_input_tokens_seen": 14147848,
      "step": 2755,
      "train_runtime": 9750.1689,
      "train_tokens_per_second": 1451.036
    },
    {
      "epoch": 2.132946859903382,
      "grad_norm": 2.651247501373291,
      "learning_rate": 9.632920693876318e-06,
      "loss": 1.5834,
      "num_input_tokens_seen": 14174200,
      "step": 2760,
      "train_runtime": 9767.958,
      "train_tokens_per_second": 1451.091
    },
    {
      "epoch": 2.1368115942028987,
      "grad_norm": 2.327025890350342,
      "learning_rate": 9.553255145653017e-06,
      "loss": 1.6419,
      "num_input_tokens_seen": 14199832,
      "step": 2765,
      "train_runtime": 9785.6554,
      "train_tokens_per_second": 1451.086
    },
    {
      "epoch": 2.1406763285024155,
      "grad_norm": 2.195932388305664,
      "learning_rate": 9.47384250688862e-06,
      "loss": 1.2678,
      "num_input_tokens_seen": 14223176,
      "step": 2770,
      "train_runtime": 9802.1325,
      "train_tokens_per_second": 1451.029
    },
    {
      "epoch": 2.1445410628019324,
      "grad_norm": 2.68076491355896,
      "learning_rate": 9.39468407780579e-06,
      "loss": 1.5028,
      "num_input_tokens_seen": 14246232,
      "step": 2775,
      "train_runtime": 9818.3265,
      "train_tokens_per_second": 1450.984
    },
    {
      "epoch": 2.148405797101449,
      "grad_norm": 2.8381950855255127,
      "learning_rate": 9.315781154464987e-06,
      "loss": 1.3484,
      "num_input_tokens_seen": 14272360,
      "step": 2780,
      "train_runtime": 9836.4064,
      "train_tokens_per_second": 1450.973
    },
    {
      "epoch": 2.152270531400966,
      "grad_norm": 2.1312451362609863,
      "learning_rate": 9.237135028743296e-06,
      "loss": 1.2797,
      "num_input_tokens_seen": 14295560,
      "step": 2785,
      "train_runtime": 9852.771,
      "train_tokens_per_second": 1450.918
    },
    {
      "epoch": 2.156135265700483,
      "grad_norm": 2.715522289276123,
      "learning_rate": 9.158746988313257e-06,
      "loss": 1.3725,
      "num_input_tokens_seen": 14317320,
      "step": 2790,
      "train_runtime": 9868.4968,
      "train_tokens_per_second": 1450.811
    },
    {
      "epoch": 2.16,
      "grad_norm": 3.3898065090179443,
      "learning_rate": 9.080618316621783e-06,
      "loss": 1.1987,
      "num_input_tokens_seen": 14344984,
      "step": 2795,
      "train_runtime": 9886.7877,
      "train_tokens_per_second": 1450.925
    },
    {
      "epoch": 2.163864734299517,
      "grad_norm": 2.1439273357391357,
      "learning_rate": 9.002750292869119e-06,
      "loss": 1.3865,
      "num_input_tokens_seen": 14369896,
      "step": 2800,
      "train_runtime": 9903.8965,
      "train_tokens_per_second": 1450.934
    },
    {
      "epoch": 2.167729468599034,
      "grad_norm": 2.2262637615203857,
      "learning_rate": 8.925144191987962e-06,
      "loss": 1.426,
      "num_input_tokens_seen": 14391736,
      "step": 2805,
      "train_runtime": 9920.0686,
      "train_tokens_per_second": 1450.77
    },
    {
      "epoch": 2.1715942028985507,
      "grad_norm": 5.128806114196777,
      "learning_rate": 8.8478012846225e-06,
      "loss": 1.2725,
      "num_input_tokens_seen": 14416264,
      "step": 2810,
      "train_runtime": 9937.2391,
      "train_tokens_per_second": 1450.731
    },
    {
      "epoch": 2.1754589371980675,
      "grad_norm": 2.1847891807556152,
      "learning_rate": 8.770722837107709e-06,
      "loss": 1.2224,
      "num_input_tokens_seen": 14441560,
      "step": 2815,
      "train_runtime": 9954.259,
      "train_tokens_per_second": 1450.792
    },
    {
      "epoch": 2.1793236714975848,
      "grad_norm": 2.236989974975586,
      "learning_rate": 8.693910111448516e-06,
      "loss": 1.6314,
      "num_input_tokens_seen": 14463224,
      "step": 2820,
      "train_runtime": 9970.2112,
      "train_tokens_per_second": 1450.644
    },
    {
      "epoch": 2.1831884057971016,
      "grad_norm": 3.134799003601074,
      "learning_rate": 8.617364365299216e-06,
      "loss": 1.3563,
      "num_input_tokens_seen": 14487720,
      "step": 2825,
      "train_runtime": 9987.0837,
      "train_tokens_per_second": 1450.646
    },
    {
      "epoch": 2.1870531400966184,
      "grad_norm": 1.6674866676330566,
      "learning_rate": 8.541086851942845e-06,
      "loss": 1.3378,
      "num_input_tokens_seen": 14512136,
      "step": 2830,
      "train_runtime": 10004.0882,
      "train_tokens_per_second": 1450.621
    },
    {
      "epoch": 2.1909178743961353,
      "grad_norm": 2.6955440044403076,
      "learning_rate": 8.46507882027067e-06,
      "loss": 1.362,
      "num_input_tokens_seen": 14535688,
      "step": 2835,
      "train_runtime": 10020.8562,
      "train_tokens_per_second": 1450.544
    },
    {
      "epoch": 2.194782608695652,
      "grad_norm": 2.089470624923706,
      "learning_rate": 8.389341514761714e-06,
      "loss": 1.4238,
      "num_input_tokens_seen": 14559768,
      "step": 2840,
      "train_runtime": 10037.6082,
      "train_tokens_per_second": 1450.522
    },
    {
      "epoch": 2.198647342995169,
      "grad_norm": 2.21226167678833,
      "learning_rate": 8.313876175462415e-06,
      "loss": 1.0313,
      "num_input_tokens_seen": 14583032,
      "step": 2845,
      "train_runtime": 10053.8883,
      "train_tokens_per_second": 1450.487
    },
    {
      "epoch": 2.202512077294686,
      "grad_norm": 3.2812576293945312,
      "learning_rate": 8.238684037966318e-06,
      "loss": 1.5056,
      "num_input_tokens_seen": 14607640,
      "step": 2850,
      "train_runtime": 10071.1094,
      "train_tokens_per_second": 1450.45
    },
    {
      "epoch": 2.206376811594203,
      "grad_norm": 2.733278751373291,
      "learning_rate": 8.163766333393835e-06,
      "loss": 1.3706,
      "num_input_tokens_seen": 14632920,
      "step": 2855,
      "train_runtime": 10088.4289,
      "train_tokens_per_second": 1450.466
    },
    {
      "epoch": 2.21024154589372,
      "grad_norm": 2.3558080196380615,
      "learning_rate": 8.089124288372075e-06,
      "loss": 1.4897,
      "num_input_tokens_seen": 14658536,
      "step": 2860,
      "train_runtime": 10105.7772,
      "train_tokens_per_second": 1450.511
    },
    {
      "epoch": 2.2141062801932367,
      "grad_norm": 2.5602381229400635,
      "learning_rate": 8.014759125014781e-06,
      "loss": 1.3088,
      "num_input_tokens_seen": 14688264,
      "step": 2865,
      "train_runtime": 10125.8017,
      "train_tokens_per_second": 1450.578
    },
    {
      "epoch": 2.2179710144927536,
      "grad_norm": 2.1594455242156982,
      "learning_rate": 7.940672060902326e-06,
      "loss": 1.2333,
      "num_input_tokens_seen": 14711208,
      "step": 2870,
      "train_runtime": 10141.9704,
      "train_tokens_per_second": 1450.528
    },
    {
      "epoch": 2.2218357487922704,
      "grad_norm": 2.259369373321533,
      "learning_rate": 7.86686430906176e-06,
      "loss": 1.4086,
      "num_input_tokens_seen": 14737544,
      "step": 2875,
      "train_runtime": 10159.8486,
      "train_tokens_per_second": 1450.567
    },
    {
      "epoch": 2.2257004830917873,
      "grad_norm": 2.797555923461914,
      "learning_rate": 7.79333707794695e-06,
      "loss": 1.4677,
      "num_input_tokens_seen": 14763960,
      "step": 2880,
      "train_runtime": 10177.9635,
      "train_tokens_per_second": 1450.581
    },
    {
      "epoch": 2.2295652173913045,
      "grad_norm": 2.2797727584838867,
      "learning_rate": 7.720091571418788e-06,
      "loss": 1.5107,
      "num_input_tokens_seen": 14788648,
      "step": 2885,
      "train_runtime": 10194.9021,
      "train_tokens_per_second": 1450.592
    },
    {
      "epoch": 2.2334299516908214,
      "grad_norm": 2.777510166168213,
      "learning_rate": 7.647128988725513e-06,
      "loss": 1.4752,
      "num_input_tokens_seen": 14813416,
      "step": 2890,
      "train_runtime": 10212.6004,
      "train_tokens_per_second": 1450.504
    },
    {
      "epoch": 2.237294685990338,
      "grad_norm": 3.41514253616333,
      "learning_rate": 7.574450524483051e-06,
      "loss": 1.8468,
      "num_input_tokens_seen": 14839448,
      "step": 2895,
      "train_runtime": 10230.626,
      "train_tokens_per_second": 1450.493
    },
    {
      "epoch": 2.241159420289855,
      "grad_norm": 2.4097578525543213,
      "learning_rate": 7.502057368655441e-06,
      "loss": 1.2234,
      "num_input_tokens_seen": 14863960,
      "step": 2900,
      "train_runtime": 10247.819,
      "train_tokens_per_second": 1450.451
    },
    {
      "epoch": 2.245024154589372,
      "grad_norm": 3.3852410316467285,
      "learning_rate": 7.42995070653538e-06,
      "loss": 1.3489,
      "num_input_tokens_seen": 14889448,
      "step": 2905,
      "train_runtime": 10265.837,
      "train_tokens_per_second": 1450.388
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 4.057831287384033,
      "learning_rate": 7.35813171872482e-06,
      "loss": 1.417,
      "num_input_tokens_seen": 14916392,
      "step": 2910,
      "train_runtime": 10284.0242,
      "train_tokens_per_second": 1450.443
    },
    {
      "epoch": 2.252753623188406,
      "grad_norm": 2.260335683822632,
      "learning_rate": 7.286601581115593e-06,
      "loss": 1.3985,
      "num_input_tokens_seen": 14940152,
      "step": 2915,
      "train_runtime": 10301.0725,
      "train_tokens_per_second": 1450.349
    },
    {
      "epoch": 2.256618357487923,
      "grad_norm": 2.1120450496673584,
      "learning_rate": 7.215361464870216e-06,
      "loss": 1.1116,
      "num_input_tokens_seen": 14964520,
      "step": 2920,
      "train_runtime": 10317.8302,
      "train_tokens_per_second": 1450.355
    },
    {
      "epoch": 2.2604830917874397,
      "grad_norm": 2.651413917541504,
      "learning_rate": 7.144412536402661e-06,
      "loss": 1.3013,
      "num_input_tokens_seen": 14989224,
      "step": 2925,
      "train_runtime": 10334.8859,
      "train_tokens_per_second": 1450.352
    },
    {
      "epoch": 2.2643478260869565,
      "grad_norm": 2.5742790699005127,
      "learning_rate": 7.07375595735931e-06,
      "loss": 1.2343,
      "num_input_tokens_seen": 15015944,
      "step": 2930,
      "train_runtime": 10352.8744,
      "train_tokens_per_second": 1450.413
    },
    {
      "epoch": 2.2682125603864733,
      "grad_norm": 2.2991483211517334,
      "learning_rate": 7.003392884599905e-06,
      "loss": 1.3041,
      "num_input_tokens_seen": 15037784,
      "step": 2935,
      "train_runtime": 10368.571,
      "train_tokens_per_second": 1450.324
    },
    {
      "epoch": 2.27207729468599,
      "grad_norm": 3.060936212539673,
      "learning_rate": 6.933324470178587e-06,
      "loss": 1.4163,
      "num_input_tokens_seen": 15064056,
      "step": 2940,
      "train_runtime": 10386.6415,
      "train_tokens_per_second": 1450.33
    },
    {
      "epoch": 2.275942028985507,
      "grad_norm": 2.515965700149536,
      "learning_rate": 6.8635518613250874e-06,
      "loss": 1.2169,
      "num_input_tokens_seen": 15090824,
      "step": 2945,
      "train_runtime": 10404.5452,
      "train_tokens_per_second": 1450.407
    },
    {
      "epoch": 2.2798067632850243,
      "grad_norm": 2.867422342300415,
      "learning_rate": 6.794076200425914e-06,
      "loss": 1.1988,
      "num_input_tokens_seen": 15115336,
      "step": 2950,
      "train_runtime": 10421.9735,
      "train_tokens_per_second": 1450.333
    },
    {
      "epoch": 2.283671497584541,
      "grad_norm": 3.2752599716186523,
      "learning_rate": 6.724898625005624e-06,
      "loss": 1.2325,
      "num_input_tokens_seen": 15141704,
      "step": 2955,
      "train_runtime": 10439.8292,
      "train_tokens_per_second": 1450.379
    },
    {
      "epoch": 2.287536231884058,
      "grad_norm": 2.6499555110931396,
      "learning_rate": 6.6560202677082324e-06,
      "loss": 1.6341,
      "num_input_tokens_seen": 15164632,
      "step": 2960,
      "train_runtime": 10456.0914,
      "train_tokens_per_second": 1450.316
    },
    {
      "epoch": 2.291400966183575,
      "grad_norm": 2.3134872913360596,
      "learning_rate": 6.587442256278664e-06,
      "loss": 1.2126,
      "num_input_tokens_seen": 15191288,
      "step": 2965,
      "train_runtime": 10474.4986,
      "train_tokens_per_second": 1450.312
    },
    {
      "epoch": 2.2952657004830916,
      "grad_norm": 2.354712724685669,
      "learning_rate": 6.519165713544287e-06,
      "loss": 1.7384,
      "num_input_tokens_seen": 15221208,
      "step": 2970,
      "train_runtime": 10494.2221,
      "train_tokens_per_second": 1450.437
    },
    {
      "epoch": 2.299130434782609,
      "grad_norm": 2.705087184906006,
      "learning_rate": 6.451191757396521e-06,
      "loss": 1.3859,
      "num_input_tokens_seen": 15249576,
      "step": 2975,
      "train_runtime": 10513.4699,
      "train_tokens_per_second": 1450.48
    },
    {
      "epoch": 2.3029951690821258,
      "grad_norm": 2.864617109298706,
      "learning_rate": 6.383521500772518e-06,
      "loss": 1.3683,
      "num_input_tokens_seen": 15276600,
      "step": 2980,
      "train_runtime": 10531.5508,
      "train_tokens_per_second": 1450.556
    },
    {
      "epoch": 2.3068599033816426,
      "grad_norm": 2.5728988647460938,
      "learning_rate": 6.3161560516369846e-06,
      "loss": 1.3281,
      "num_input_tokens_seen": 15302600,
      "step": 2985,
      "train_runtime": 10549.5275,
      "train_tokens_per_second": 1450.548
    },
    {
      "epoch": 2.3107246376811594,
      "grad_norm": 1.7290546894073486,
      "learning_rate": 6.249096512964018e-06,
      "loss": 1.3734,
      "num_input_tokens_seen": 15329544,
      "step": 2990,
      "train_runtime": 10567.5104,
      "train_tokens_per_second": 1450.63
    },
    {
      "epoch": 2.3145893719806763,
      "grad_norm": 2.0755560398101807,
      "learning_rate": 6.182343982719016e-06,
      "loss": 1.2859,
      "num_input_tokens_seen": 15353256,
      "step": 2995,
      "train_runtime": 10584.0529,
      "train_tokens_per_second": 1450.603
    },
    {
      "epoch": 2.318454106280193,
      "grad_norm": 3.2161171436309814,
      "learning_rate": 6.115899553840751e-06,
      "loss": 1.6578,
      "num_input_tokens_seen": 15375320,
      "step": 3000,
      "train_runtime": 10600.2768,
      "train_tokens_per_second": 1450.464
    },
    {
      "epoch": 2.32231884057971,
      "grad_norm": 2.7646710872650146,
      "learning_rate": 6.049764314223452e-06,
      "loss": 1.3501,
      "num_input_tokens_seen": 15404072,
      "step": 3005,
      "train_runtime": 10619.3544,
      "train_tokens_per_second": 1450.566
    },
    {
      "epoch": 2.3261835748792272,
      "grad_norm": 2.47788667678833,
      "learning_rate": 5.983939346698997e-06,
      "loss": 1.2446,
      "num_input_tokens_seen": 15428312,
      "step": 3010,
      "train_runtime": 10636.1909,
      "train_tokens_per_second": 1450.549
    },
    {
      "epoch": 2.330048309178744,
      "grad_norm": 2.3190972805023193,
      "learning_rate": 5.9184257290191756e-06,
      "loss": 1.2179,
      "num_input_tokens_seen": 15454104,
      "step": 3015,
      "train_runtime": 10653.5788,
      "train_tokens_per_second": 1450.602
    },
    {
      "epoch": 2.333913043478261,
      "grad_norm": 2.2724997997283936,
      "learning_rate": 5.853224533838042e-06,
      "loss": 1.1941,
      "num_input_tokens_seen": 15478840,
      "step": 3020,
      "train_runtime": 10670.6949,
      "train_tokens_per_second": 1450.593
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 1.6835904121398926,
      "learning_rate": 5.788336828694377e-06,
      "loss": 1.4422,
      "num_input_tokens_seen": 15503608,
      "step": 3025,
      "train_runtime": 10687.5082,
      "train_tokens_per_second": 1450.629
    },
    {
      "epoch": 2.3416425120772946,
      "grad_norm": 2.4096786975860596,
      "learning_rate": 5.723763675994165e-06,
      "loss": 1.5136,
      "num_input_tokens_seen": 15529064,
      "step": 3030,
      "train_runtime": 10705.0752,
      "train_tokens_per_second": 1450.626
    },
    {
      "epoch": 2.3455072463768114,
      "grad_norm": 3.0440025329589844,
      "learning_rate": 5.659506132993253e-06,
      "loss": 1.3902,
      "num_input_tokens_seen": 15557112,
      "step": 3035,
      "train_runtime": 10723.9425,
      "train_tokens_per_second": 1450.69
    },
    {
      "epoch": 2.3493719806763287,
      "grad_norm": 3.19283390045166,
      "learning_rate": 5.595565251779986e-06,
      "loss": 1.3712,
      "num_input_tokens_seen": 15581400,
      "step": 3040,
      "train_runtime": 10741.0012,
      "train_tokens_per_second": 1450.647
    },
    {
      "epoch": 2.3532367149758455,
      "grad_norm": 2.449741840362549,
      "learning_rate": 5.531942079258029e-06,
      "loss": 1.4231,
      "num_input_tokens_seen": 15611080,
      "step": 3045,
      "train_runtime": 10760.4431,
      "train_tokens_per_second": 1450.784
    },
    {
      "epoch": 2.3571014492753624,
      "grad_norm": 3.3325231075286865,
      "learning_rate": 5.468637657129194e-06,
      "loss": 1.5838,
      "num_input_tokens_seen": 15632808,
      "step": 3050,
      "train_runtime": 10776.4963,
      "train_tokens_per_second": 1450.639
    },
    {
      "epoch": 2.360966183574879,
      "grad_norm": 3.2640671730041504,
      "learning_rate": 5.405653021876408e-06,
      "loss": 1.3735,
      "num_input_tokens_seen": 15661704,
      "step": 3055,
      "train_runtime": 10795.5545,
      "train_tokens_per_second": 1450.755
    },
    {
      "epoch": 2.364830917874396,
      "grad_norm": 2.7873332500457764,
      "learning_rate": 5.342989204746698e-06,
      "loss": 1.317,
      "num_input_tokens_seen": 15685000,
      "step": 3060,
      "train_runtime": 10812.0079,
      "train_tokens_per_second": 1450.702
    },
    {
      "epoch": 2.368695652173913,
      "grad_norm": 2.2768454551696777,
      "learning_rate": 5.280647231734376e-06,
      "loss": 1.7597,
      "num_input_tokens_seen": 15705144,
      "step": 3065,
      "train_runtime": 10827.4545,
      "train_tokens_per_second": 1450.493
    },
    {
      "epoch": 2.37256038647343,
      "grad_norm": 2.0219576358795166,
      "learning_rate": 5.2186281235641756e-06,
      "loss": 1.1406,
      "num_input_tokens_seen": 15733736,
      "step": 3070,
      "train_runtime": 10845.9563,
      "train_tokens_per_second": 1450.655
    },
    {
      "epoch": 2.376425120772947,
      "grad_norm": 2.2368266582489014,
      "learning_rate": 5.156932895674571e-06,
      "loss": 1.3128,
      "num_input_tokens_seen": 15758488,
      "step": 3075,
      "train_runtime": 10863.2341,
      "train_tokens_per_second": 1450.626
    },
    {
      "epoch": 2.380289855072464,
      "grad_norm": 2.397644281387329,
      "learning_rate": 5.095562558201156e-06,
      "loss": 1.3296,
      "num_input_tokens_seen": 15784328,
      "step": 3080,
      "train_runtime": 10880.3272,
      "train_tokens_per_second": 1450.722
    },
    {
      "epoch": 2.3841545893719807,
      "grad_norm": 1.620989441871643,
      "learning_rate": 5.034518115960088e-06,
      "loss": 1.5386,
      "num_input_tokens_seen": 15807352,
      "step": 3085,
      "train_runtime": 10896.6262,
      "train_tokens_per_second": 1450.665
    },
    {
      "epoch": 2.3880193236714975,
      "grad_norm": 1.583410382270813,
      "learning_rate": 4.973800568431655e-06,
      "loss": 1.3997,
      "num_input_tokens_seen": 15835608,
      "step": 3090,
      "train_runtime": 10915.9466,
      "train_tokens_per_second": 1450.686
    },
    {
      "epoch": 2.3918840579710143,
      "grad_norm": 2.5888922214508057,
      "learning_rate": 4.9134109097438795e-06,
      "loss": 1.5016,
      "num_input_tokens_seen": 15861064,
      "step": 3095,
      "train_runtime": 10933.6657,
      "train_tokens_per_second": 1450.663
    },
    {
      "epoch": 2.395748792270531,
      "grad_norm": 3.169487476348877,
      "learning_rate": 4.8533501286562755e-06,
      "loss": 1.336,
      "num_input_tokens_seen": 15888216,
      "step": 3100,
      "train_runtime": 10952.0298,
      "train_tokens_per_second": 1450.71
    },
    {
      "epoch": 2.3996135265700484,
      "grad_norm": 2.586806058883667,
      "learning_rate": 4.793619208543654e-06,
      "loss": 1.3855,
      "num_input_tokens_seen": 15909064,
      "step": 3105,
      "train_runtime": 10967.6979,
      "train_tokens_per_second": 1450.538
    },
    {
      "epoch": 2.4034782608695653,
      "grad_norm": 2.268033504486084,
      "learning_rate": 4.734219127379996e-06,
      "loss": 1.5093,
      "num_input_tokens_seen": 15931528,
      "step": 3110,
      "train_runtime": 10983.9966,
      "train_tokens_per_second": 1450.431
    },
    {
      "epoch": 2.407342995169082,
      "grad_norm": 2.932910442352295,
      "learning_rate": 4.67515085772246e-06,
      "loss": 1.3957,
      "num_input_tokens_seen": 15955912,
      "step": 3115,
      "train_runtime": 11000.6562,
      "train_tokens_per_second": 1450.451
    },
    {
      "epoch": 2.411207729468599,
      "grad_norm": 2.1814191341400146,
      "learning_rate": 4.616415366695473e-06,
      "loss": 1.2769,
      "num_input_tokens_seen": 15980792,
      "step": 3120,
      "train_runtime": 11017.865,
      "train_tokens_per_second": 1450.444
    },
    {
      "epoch": 2.415072463768116,
      "grad_norm": 2.524282217025757,
      "learning_rate": 4.558013615974866e-06,
      "loss": 1.6381,
      "num_input_tokens_seen": 16002408,
      "step": 3125,
      "train_runtime": 11033.798,
      "train_tokens_per_second": 1450.308
    },
    {
      "epoch": 2.418937198067633,
      "grad_norm": 2.2553436756134033,
      "learning_rate": 4.499946561772153e-06,
      "loss": 0.9411,
      "num_input_tokens_seen": 16025464,
      "step": 3130,
      "train_runtime": 11049.8214,
      "train_tokens_per_second": 1450.292
    },
    {
      "epoch": 2.42280193236715,
      "grad_norm": 3.4351143836975098,
      "learning_rate": 4.442215154818846e-06,
      "loss": 1.3216,
      "num_input_tokens_seen": 16046392,
      "step": 3135,
      "train_runtime": 11065.3923,
      "train_tokens_per_second": 1450.142
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 2.517277240753174,
      "learning_rate": 4.38482034035092e-06,
      "loss": 1.3965,
      "num_input_tokens_seen": 16067096,
      "step": 3140,
      "train_runtime": 11080.7921,
      "train_tokens_per_second": 1449.995
    },
    {
      "epoch": 2.4305314009661836,
      "grad_norm": 2.6277031898498535,
      "learning_rate": 4.3277630580933345e-06,
      "loss": 1.3469,
      "num_input_tokens_seen": 16088056,
      "step": 3145,
      "train_runtime": 11096.3768,
      "train_tokens_per_second": 1449.848
    },
    {
      "epoch": 2.4343961352657004,
      "grad_norm": 2.620940685272217,
      "learning_rate": 4.271044242244609e-06,
      "loss": 1.6105,
      "num_input_tokens_seen": 16110936,
      "step": 3150,
      "train_runtime": 11112.3357,
      "train_tokens_per_second": 1449.824
    },
    {
      "epoch": 2.4382608695652173,
      "grad_norm": 2.2616100311279297,
      "learning_rate": 4.214664821461564e-06,
      "loss": 1.4006,
      "num_input_tokens_seen": 16138408,
      "step": 3155,
      "train_runtime": 11130.6144,
      "train_tokens_per_second": 1449.912
    },
    {
      "epoch": 2.442125603864734,
      "grad_norm": 2.983591079711914,
      "learning_rate": 4.15862571884412e-06,
      "loss": 1.3891,
      "num_input_tokens_seen": 16165592,
      "step": 3160,
      "train_runtime": 11148.5205,
      "train_tokens_per_second": 1450.021
    },
    {
      "epoch": 2.4459903381642514,
      "grad_norm": 1.9234771728515625,
      "learning_rate": 4.102927851920154e-06,
      "loss": 1.2644,
      "num_input_tokens_seen": 16193720,
      "step": 3165,
      "train_runtime": 11167.3085,
      "train_tokens_per_second": 1450.101
    },
    {
      "epoch": 2.449855072463768,
      "grad_norm": 3.534618377685547,
      "learning_rate": 4.0475721326305145e-06,
      "loss": 1.4823,
      "num_input_tokens_seen": 16217544,
      "step": 3170,
      "train_runtime": 11184.2587,
      "train_tokens_per_second": 1450.033
    },
    {
      "epoch": 2.453719806763285,
      "grad_norm": 2.4104623794555664,
      "learning_rate": 3.992559467314036e-06,
      "loss": 1.4095,
      "num_input_tokens_seen": 16243640,
      "step": 3175,
      "train_runtime": 11202.2485,
      "train_tokens_per_second": 1450.034
    },
    {
      "epoch": 2.457584541062802,
      "grad_norm": 2.7404890060424805,
      "learning_rate": 3.937890756692766e-06,
      "loss": 1.2955,
      "num_input_tokens_seen": 16270808,
      "step": 3180,
      "train_runtime": 11220.305,
      "train_tokens_per_second": 1450.122
    },
    {
      "epoch": 2.4614492753623187,
      "grad_norm": 2.538505792617798,
      "learning_rate": 3.883566895857158e-06,
      "loss": 1.2694,
      "num_input_tokens_seen": 16295000,
      "step": 3185,
      "train_runtime": 11237.1563,
      "train_tokens_per_second": 1450.1
    },
    {
      "epoch": 2.4653140096618356,
      "grad_norm": 2.247056722640991,
      "learning_rate": 3.829588774251464e-06,
      "loss": 1.4534,
      "num_input_tokens_seen": 16322776,
      "step": 3190,
      "train_runtime": 11255.6596,
      "train_tokens_per_second": 1450.184
    },
    {
      "epoch": 2.469178743961353,
      "grad_norm": 2.3332536220550537,
      "learning_rate": 3.77595727565912e-06,
      "loss": 1.259,
      "num_input_tokens_seen": 16350776,
      "step": 3195,
      "train_runtime": 11274.3436,
      "train_tokens_per_second": 1450.264
    },
    {
      "epoch": 2.4730434782608697,
      "grad_norm": 3.007197380065918,
      "learning_rate": 3.7226732781883405e-06,
      "loss": 1.3829,
      "num_input_tokens_seen": 16377576,
      "step": 3200,
      "train_runtime": 11292.2175,
      "train_tokens_per_second": 1450.342
    },
    {
      "epoch": 2.4769082125603865,
      "grad_norm": 1.7737833261489868,
      "learning_rate": 3.669737654257682e-06,
      "loss": 1.2338,
      "num_input_tokens_seen": 16403352,
      "step": 3205,
      "train_runtime": 11309.5923,
      "train_tokens_per_second": 1450.393
    },
    {
      "epoch": 2.4807729468599033,
      "grad_norm": 2.4879539012908936,
      "learning_rate": 3.6171512705818006e-06,
      "loss": 1.5446,
      "num_input_tokens_seen": 16424328,
      "step": 3210,
      "train_runtime": 11325.1593,
      "train_tokens_per_second": 1450.251
    },
    {
      "epoch": 2.48463768115942,
      "grad_norm": 2.5729501247406006,
      "learning_rate": 3.5649149881572196e-06,
      "loss": 1.4049,
      "num_input_tokens_seen": 16450280,
      "step": 3215,
      "train_runtime": 11342.9562,
      "train_tokens_per_second": 1450.264
    },
    {
      "epoch": 2.488502415458937,
      "grad_norm": 2.474797010421753,
      "learning_rate": 3.5130296622482877e-06,
      "loss": 1.2086,
      "num_input_tokens_seen": 16474040,
      "step": 3220,
      "train_runtime": 11359.5102,
      "train_tokens_per_second": 1450.242
    },
    {
      "epoch": 2.4923671497584543,
      "grad_norm": 2.681774616241455,
      "learning_rate": 3.461496142373111e-06,
      "loss": 1.374,
      "num_input_tokens_seen": 16496456,
      "step": 3225,
      "train_runtime": 11375.6237,
      "train_tokens_per_second": 1450.158
    },
    {
      "epoch": 2.496231884057971,
      "grad_norm": 2.4257328510284424,
      "learning_rate": 3.4103152722897104e-06,
      "loss": 1.5025,
      "num_input_tokens_seen": 16525784,
      "step": 3230,
      "train_runtime": 11395.0783,
      "train_tokens_per_second": 1450.256
    },
    {
      "epoch": 2.500096618357488,
      "grad_norm": 2.9531447887420654,
      "learning_rate": 3.3594878899821463e-06,
      "loss": 1.429,
      "num_input_tokens_seen": 16551816,
      "step": 3235,
      "train_runtime": 11413.0405,
      "train_tokens_per_second": 1450.255
    },
    {
      "epoch": 2.503961352657005,
      "grad_norm": 1.8222386837005615,
      "learning_rate": 3.309014827646842e-06,
      "loss": 1.3197,
      "num_input_tokens_seen": 16578488,
      "step": 3240,
      "train_runtime": 11431.05,
      "train_tokens_per_second": 1450.303
    },
    {
      "epoch": 2.5078260869565216,
      "grad_norm": 2.5277507305145264,
      "learning_rate": 3.2588969116789407e-06,
      "loss": 1.5693,
      "num_input_tokens_seen": 16602056,
      "step": 3245,
      "train_runtime": 11447.5797,
      "train_tokens_per_second": 1450.268
    },
    {
      "epoch": 2.5116908212560385,
      "grad_norm": 2.448629379272461,
      "learning_rate": 3.2091349626587747e-06,
      "loss": 1.2085,
      "num_input_tokens_seen": 16633016,
      "step": 3250,
      "train_runtime": 11467.8313,
      "train_tokens_per_second": 1450.406
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 3.093090534210205,
      "learning_rate": 3.159729795338426e-06,
      "loss": 1.3924,
      "num_input_tokens_seen": 16658952,
      "step": 3255,
      "train_runtime": 11485.3575,
      "train_tokens_per_second": 1450.451
    },
    {
      "epoch": 2.5194202898550726,
      "grad_norm": 2.653327703475952,
      "learning_rate": 3.1106822186284025e-06,
      "loss": 1.374,
      "num_input_tokens_seen": 16684840,
      "step": 3260,
      "train_runtime": 11503.0116,
      "train_tokens_per_second": 1450.476
    },
    {
      "epoch": 2.5232850241545894,
      "grad_norm": 2.2899911403656006,
      "learning_rate": 3.061993035584365e-06,
      "loss": 1.1856,
      "num_input_tokens_seen": 16711192,
      "step": 3265,
      "train_runtime": 11520.6149,
      "train_tokens_per_second": 1450.547
    },
    {
      "epoch": 2.5271497584541063,
      "grad_norm": 2.3795180320739746,
      "learning_rate": 3.0136630433940274e-06,
      "loss": 1.4148,
      "num_input_tokens_seen": 16744456,
      "step": 3270,
      "train_runtime": 11542.3235,
      "train_tokens_per_second": 1450.701
    },
    {
      "epoch": 2.531014492753623,
      "grad_norm": 3.229681968688965,
      "learning_rate": 2.9656930333640394e-06,
      "loss": 1.2612,
      "num_input_tokens_seen": 16770792,
      "step": 3275,
      "train_runtime": 11560.3484,
      "train_tokens_per_second": 1450.717
    },
    {
      "epoch": 2.53487922705314,
      "grad_norm": 2.833111047744751,
      "learning_rate": 2.9180837909070862e-06,
      "loss": 1.5658,
      "num_input_tokens_seen": 16796632,
      "step": 3280,
      "train_runtime": 11578.0392,
      "train_tokens_per_second": 1450.732
    },
    {
      "epoch": 2.5387439613526572,
      "grad_norm": 2.962053060531616,
      "learning_rate": 2.8708360955290088e-06,
      "loss": 1.3706,
      "num_input_tokens_seen": 16818232,
      "step": 3285,
      "train_runtime": 11593.7643,
      "train_tokens_per_second": 1450.627
    },
    {
      "epoch": 2.542608695652174,
      "grad_norm": 2.1567091941833496,
      "learning_rate": 2.823950720816032e-06,
      "loss": 1.5309,
      "num_input_tokens_seen": 16840760,
      "step": 3290,
      "train_runtime": 11610.0121,
      "train_tokens_per_second": 1450.538
    },
    {
      "epoch": 2.546473429951691,
      "grad_norm": 2.519718885421753,
      "learning_rate": 2.7774284344221107e-06,
      "loss": 1.2257,
      "num_input_tokens_seen": 16866152,
      "step": 3295,
      "train_runtime": 11627.2976,
      "train_tokens_per_second": 1450.565
    },
    {
      "epoch": 2.5503381642512077,
      "grad_norm": 2.1789255142211914,
      "learning_rate": 2.731269998056346e-06,
      "loss": 1.657,
      "num_input_tokens_seen": 16891992,
      "step": 3300,
      "train_runtime": 11645.1218,
      "train_tokens_per_second": 1450.564
    },
    {
      "epoch": 2.5542028985507246,
      "grad_norm": 2.1031405925750732,
      "learning_rate": 2.6854761674705382e-06,
      "loss": 1.287,
      "num_input_tokens_seen": 16919992,
      "step": 3305,
      "train_runtime": 11664.3651,
      "train_tokens_per_second": 1450.571
    },
    {
      "epoch": 2.5580676328502414,
      "grad_norm": 2.696218967437744,
      "learning_rate": 2.640047692446801e-06,
      "loss": 1.2005,
      "num_input_tokens_seen": 16951736,
      "step": 3310,
      "train_runtime": 11684.6805,
      "train_tokens_per_second": 1450.766
    },
    {
      "epoch": 2.5619323671497582,
      "grad_norm": 2.2959115505218506,
      "learning_rate": 2.59498531678527e-06,
      "loss": 1.2639,
      "num_input_tokens_seen": 16978120,
      "step": 3315,
      "train_runtime": 11702.6626,
      "train_tokens_per_second": 1450.791
    },
    {
      "epoch": 2.5657971014492755,
      "grad_norm": 2.5113024711608887,
      "learning_rate": 2.5502897782919506e-06,
      "loss": 1.6735,
      "num_input_tokens_seen": 17000248,
      "step": 3320,
      "train_runtime": 11718.2374,
      "train_tokens_per_second": 1450.751
    },
    {
      "epoch": 2.5696618357487924,
      "grad_norm": 2.3618886470794678,
      "learning_rate": 2.5059618087666188e-06,
      "loss": 1.4608,
      "num_input_tokens_seen": 17027832,
      "step": 3325,
      "train_runtime": 11736.4132,
      "train_tokens_per_second": 1450.855
    },
    {
      "epoch": 2.573526570048309,
      "grad_norm": 2.8175032138824463,
      "learning_rate": 2.462002133990851e-06,
      "loss": 1.8394,
      "num_input_tokens_seen": 17051528,
      "step": 3330,
      "train_runtime": 11753.2367,
      "train_tokens_per_second": 1450.794
    },
    {
      "epoch": 2.577391304347826,
      "grad_norm": 2.4128775596618652,
      "learning_rate": 2.4184114737161338e-06,
      "loss": 1.0936,
      "num_input_tokens_seen": 17083784,
      "step": 3335,
      "train_runtime": 11774.2277,
      "train_tokens_per_second": 1450.947
    },
    {
      "epoch": 2.581256038647343,
      "grad_norm": 2.240699291229248,
      "learning_rate": 2.3751905416520727e-06,
      "loss": 1.3537,
      "num_input_tokens_seen": 17106600,
      "step": 3340,
      "train_runtime": 11790.3327,
      "train_tokens_per_second": 1450.901
    },
    {
      "epoch": 2.58512077294686,
      "grad_norm": 3.217423915863037,
      "learning_rate": 2.3323400454547313e-06,
      "loss": 1.4152,
      "num_input_tokens_seen": 17134072,
      "step": 3345,
      "train_runtime": 11808.6051,
      "train_tokens_per_second": 1450.982
    },
    {
      "epoch": 2.5889855072463765,
      "grad_norm": 2.6917059421539307,
      "learning_rate": 2.28986068671502e-06,
      "loss": 1.463,
      "num_input_tokens_seen": 17158712,
      "step": 3350,
      "train_runtime": 11825.8985,
      "train_tokens_per_second": 1450.944
    },
    {
      "epoch": 2.592850241545894,
      "grad_norm": 2.944549798965454,
      "learning_rate": 2.2477531609472164e-06,
      "loss": 0.9474,
      "num_input_tokens_seen": 17183768,
      "step": 3355,
      "train_runtime": 11842.963,
      "train_tokens_per_second": 1450.969
    },
    {
      "epoch": 2.5967149758454107,
      "grad_norm": 2.7184066772460938,
      "learning_rate": 2.2060181575775857e-06,
      "loss": 1.5592,
      "num_input_tokens_seen": 17211736,
      "step": 3360,
      "train_runtime": 11861.8275,
      "train_tokens_per_second": 1451.019
    },
    {
      "epoch": 2.6005797101449275,
      "grad_norm": 2.024616003036499,
      "learning_rate": 2.1646563599330788e-06,
      "loss": 1.4286,
      "num_input_tokens_seen": 17240968,
      "step": 3365,
      "train_runtime": 11881.2945,
      "train_tokens_per_second": 1451.102
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 4.87252950668335,
      "learning_rate": 2.1236684452301646e-06,
      "loss": 1.5159,
      "num_input_tokens_seen": 17265992,
      "step": 3370,
      "train_runtime": 11898.8514,
      "train_tokens_per_second": 1451.064
    },
    {
      "epoch": 2.608309178743961,
      "grad_norm": 3.3447911739349365,
      "learning_rate": 2.083055084563712e-06,
      "loss": 1.306,
      "num_input_tokens_seen": 17292936,
      "step": 3375,
      "train_runtime": 11917.1639,
      "train_tokens_per_second": 1451.095
    },
    {
      "epoch": 2.6121739130434785,
      "grad_norm": 2.7851040363311768,
      "learning_rate": 2.0428169428960224e-06,
      "loss": 1.2576,
      "num_input_tokens_seen": 17322024,
      "step": 3380,
      "train_runtime": 11936.4481,
      "train_tokens_per_second": 1451.187
    },
    {
      "epoch": 2.6160386473429953,
      "grad_norm": 2.2016077041625977,
      "learning_rate": 2.0029546790459513e-06,
      "loss": 1.3776,
      "num_input_tokens_seen": 17346904,
      "step": 3385,
      "train_runtime": 11953.3403,
      "train_tokens_per_second": 1451.218
    },
    {
      "epoch": 2.619903381642512,
      "grad_norm": 2.511050224304199,
      "learning_rate": 1.963468945678093e-06,
      "loss": 1.321,
      "num_input_tokens_seen": 17371032,
      "step": 3390,
      "train_runtime": 11970.0582,
      "train_tokens_per_second": 1451.207
    },
    {
      "epoch": 2.623768115942029,
      "grad_norm": 2.378110885620117,
      "learning_rate": 1.924360389292121e-06,
      "loss": 1.2342,
      "num_input_tokens_seen": 17401352,
      "step": 3395,
      "train_runtime": 11989.6389,
      "train_tokens_per_second": 1451.366
    },
    {
      "epoch": 2.627632850241546,
      "grad_norm": 2.113057851791382,
      "learning_rate": 1.8856296502121923e-06,
      "loss": 1.292,
      "num_input_tokens_seen": 17429144,
      "step": 3400,
      "train_runtime": 12008.295,
      "train_tokens_per_second": 1451.425
    },
    {
      "epoch": 2.6314975845410626,
      "grad_norm": 2.0939254760742188,
      "learning_rate": 1.8472773625764605e-06,
      "loss": 1.3223,
      "num_input_tokens_seen": 17454552,
      "step": 3405,
      "train_runtime": 12026.1927,
      "train_tokens_per_second": 1451.378
    },
    {
      "epoch": 2.6353623188405795,
      "grad_norm": 3.058915853500366,
      "learning_rate": 1.8093041543266964e-06,
      "loss": 1.4513,
      "num_input_tokens_seen": 17482152,
      "step": 3410,
      "train_runtime": 12045.2786,
      "train_tokens_per_second": 1451.37
    },
    {
      "epoch": 2.6392270531400968,
      "grad_norm": 2.533996343612671,
      "learning_rate": 1.771710647198002e-06,
      "loss": 1.4779,
      "num_input_tokens_seen": 17509576,
      "step": 3415,
      "train_runtime": 12063.514,
      "train_tokens_per_second": 1451.449
    },
    {
      "epoch": 2.6430917874396136,
      "grad_norm": 2.25878643989563,
      "learning_rate": 1.7344974567086392e-06,
      "loss": 1.2233,
      "num_input_tokens_seen": 17538152,
      "step": 3420,
      "train_runtime": 12082.503,
      "train_tokens_per_second": 1451.533
    },
    {
      "epoch": 2.6469565217391304,
      "grad_norm": 2.014681100845337,
      "learning_rate": 1.6976651921499553e-06,
      "loss": 1.3067,
      "num_input_tokens_seen": 17567640,
      "step": 3425,
      "train_runtime": 12102.3451,
      "train_tokens_per_second": 1451.59
    },
    {
      "epoch": 2.6508212560386473,
      "grad_norm": 1.6418187618255615,
      "learning_rate": 1.661214456576385e-06,
      "loss": 1.2737,
      "num_input_tokens_seen": 17595592,
      "step": 3430,
      "train_runtime": 12120.5635,
      "train_tokens_per_second": 1451.714
    },
    {
      "epoch": 2.654685990338164,
      "grad_norm": 2.8128552436828613,
      "learning_rate": 1.6251458467956005e-06,
      "loss": 1.2401,
      "num_input_tokens_seen": 17619464,
      "step": 3435,
      "train_runtime": 12137.9284,
      "train_tokens_per_second": 1451.604
    },
    {
      "epoch": 2.6585507246376814,
      "grad_norm": 2.4099855422973633,
      "learning_rate": 1.5894599533587296e-06,
      "loss": 1.6317,
      "num_input_tokens_seen": 17639000,
      "step": 3440,
      "train_runtime": 12152.6237,
      "train_tokens_per_second": 1451.456
    },
    {
      "epoch": 2.662415458937198,
      "grad_norm": 2.239229917526245,
      "learning_rate": 1.5541573605506898e-06,
      "loss": 1.3659,
      "num_input_tokens_seen": 17660312,
      "step": 3445,
      "train_runtime": 12167.9148,
      "train_tokens_per_second": 1451.384
    },
    {
      "epoch": 2.666280193236715,
      "grad_norm": 3.1600759029388428,
      "learning_rate": 1.5192386463806164e-06,
      "loss": 1.4904,
      "num_input_tokens_seen": 17685672,
      "step": 3450,
      "train_runtime": 12185.1538,
      "train_tokens_per_second": 1451.411
    },
    {
      "epoch": 2.670144927536232,
      "grad_norm": 2.554892063140869,
      "learning_rate": 1.484704382572394e-06,
      "loss": 1.1928,
      "num_input_tokens_seen": 17710664,
      "step": 3455,
      "train_runtime": 12202.2549,
      "train_tokens_per_second": 1451.426
    },
    {
      "epoch": 2.6740096618357487,
      "grad_norm": 2.9820547103881836,
      "learning_rate": 1.4505551345553203e-06,
      "loss": 1.3748,
      "num_input_tokens_seen": 17738760,
      "step": 3460,
      "train_runtime": 12221.2972,
      "train_tokens_per_second": 1451.463
    },
    {
      "epoch": 2.6778743961352656,
      "grad_norm": 3.6175293922424316,
      "learning_rate": 1.4167914614548279e-06,
      "loss": 1.1893,
      "num_input_tokens_seen": 17765384,
      "step": 3465,
      "train_runtime": 12239.3876,
      "train_tokens_per_second": 1451.493
    },
    {
      "epoch": 2.6817391304347824,
      "grad_norm": 2.3961167335510254,
      "learning_rate": 1.3834139160833176e-06,
      "loss": 1.539,
      "num_input_tokens_seen": 17791816,
      "step": 3470,
      "train_runtime": 12257.281,
      "train_tokens_per_second": 1451.53
    },
    {
      "epoch": 2.6856038647342997,
      "grad_norm": 2.6484243869781494,
      "learning_rate": 1.3504230449311456e-06,
      "loss": 1.3702,
      "num_input_tokens_seen": 17816920,
      "step": 3475,
      "train_runtime": 12274.5245,
      "train_tokens_per_second": 1451.536
    },
    {
      "epoch": 2.6894685990338165,
      "grad_norm": 2.867887258529663,
      "learning_rate": 1.3178193881576372e-06,
      "loss": 1.1663,
      "num_input_tokens_seen": 17841128,
      "step": 3480,
      "train_runtime": 12291.7764,
      "train_tokens_per_second": 1451.469
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 2.533104181289673,
      "learning_rate": 1.285603479582273e-06,
      "loss": 1.3468,
      "num_input_tokens_seen": 17870440,
      "step": 3485,
      "train_runtime": 12311.1686,
      "train_tokens_per_second": 1451.563
    },
    {
      "epoch": 2.69719806763285,
      "grad_norm": 3.1103198528289795,
      "learning_rate": 1.2537758466759198e-06,
      "loss": 1.2286,
      "num_input_tokens_seen": 17898472,
      "step": 3490,
      "train_runtime": 12329.8491,
      "train_tokens_per_second": 1451.638
    },
    {
      "epoch": 2.701062801932367,
      "grad_norm": 3.4970688819885254,
      "learning_rate": 1.2223370105522159e-06,
      "loss": 1.6877,
      "num_input_tokens_seen": 17919928,
      "step": 3495,
      "train_runtime": 12345.6715,
      "train_tokens_per_second": 1451.515
    },
    {
      "epoch": 2.7049275362318843,
      "grad_norm": 2.277045965194702,
      "learning_rate": 1.191287485959036e-06,
      "loss": 1.2804,
      "num_input_tokens_seen": 17946184,
      "step": 3500,
      "train_runtime": 12363.511,
      "train_tokens_per_second": 1451.544
    },
    {
      "epoch": 2.7087922705314007,
      "grad_norm": 2.926940441131592,
      "learning_rate": 1.1606277812700545e-06,
      "loss": 1.342,
      "num_input_tokens_seen": 17973992,
      "step": 3505,
      "train_runtime": 12382.5253,
      "train_tokens_per_second": 1451.561
    },
    {
      "epoch": 2.712657004830918,
      "grad_norm": 2.150505304336548,
      "learning_rate": 1.1303583984764305e-06,
      "loss": 1.4593,
      "num_input_tokens_seen": 17995768,
      "step": 3510,
      "train_runtime": 12398.2078,
      "train_tokens_per_second": 1451.481
    },
    {
      "epoch": 2.716521739130435,
      "grad_norm": 2.3581326007843018,
      "learning_rate": 1.1004798331785775e-06,
      "loss": 1.2319,
      "num_input_tokens_seen": 18018744,
      "step": 3515,
      "train_runtime": 12414.5796,
      "train_tokens_per_second": 1451.418
    },
    {
      "epoch": 2.7203864734299517,
      "grad_norm": 3.342343330383301,
      "learning_rate": 1.0709925745780724e-06,
      "loss": 1.5888,
      "num_input_tokens_seen": 18045640,
      "step": 3520,
      "train_runtime": 12432.9518,
      "train_tokens_per_second": 1451.436
    },
    {
      "epoch": 2.7242512077294685,
      "grad_norm": 2.799574136734009,
      "learning_rate": 1.0418971054696136e-06,
      "loss": 1.2687,
      "num_input_tokens_seen": 18073592,
      "step": 3525,
      "train_runtime": 12451.5508,
      "train_tokens_per_second": 1451.513
    },
    {
      "epoch": 2.7281159420289853,
      "grad_norm": 2.2150347232818604,
      "learning_rate": 1.0131939022331338e-06,
      "loss": 1.0752,
      "num_input_tokens_seen": 18097320,
      "step": 3530,
      "train_runtime": 12468.0446,
      "train_tokens_per_second": 1451.496
    },
    {
      "epoch": 2.7319806763285026,
      "grad_norm": 3.117405414581299,
      "learning_rate": 9.848834348260089e-07,
      "loss": 1.5528,
      "num_input_tokens_seen": 18124264,
      "step": 3535,
      "train_runtime": 12486.7834,
      "train_tokens_per_second": 1451.476
    },
    {
      "epoch": 2.7358454106280194,
      "grad_norm": 2.150639057159424,
      "learning_rate": 9.56966166775347e-07,
      "loss": 1.4011,
      "num_input_tokens_seen": 18152968,
      "step": 3540,
      "train_runtime": 12506.0664,
      "train_tokens_per_second": 1451.533
    },
    {
      "epoch": 2.7397101449275363,
      "grad_norm": 2.143631935119629,
      "learning_rate": 9.29442555170415e-07,
      "loss": 1.1267,
      "num_input_tokens_seen": 18180168,
      "step": 3545,
      "train_runtime": 12524.4742,
      "train_tokens_per_second": 1451.571
    },
    {
      "epoch": 2.743574879227053,
      "grad_norm": 2.2704734802246094,
      "learning_rate": 9.023130506551236e-07,
      "loss": 1.304,
      "num_input_tokens_seen": 18204024,
      "step": 3550,
      "train_runtime": 12540.9732,
      "train_tokens_per_second": 1451.564
    },
    {
      "epoch": 2.74743961352657,
      "grad_norm": 2.3676252365112305,
      "learning_rate": 8.755780974206929e-07,
      "loss": 1.3232,
      "num_input_tokens_seen": 18231400,
      "step": 3555,
      "train_runtime": 12559.6362,
      "train_tokens_per_second": 1451.587
    },
    {
      "epoch": 2.751304347826087,
      "grad_norm": 3.035327434539795,
      "learning_rate": 8.49238133198349e-07,
      "loss": 1.2118,
      "num_input_tokens_seen": 18261352,
      "step": 3560,
      "train_runtime": 12579.3093,
      "train_tokens_per_second": 1451.698
    },
    {
      "epoch": 2.7551690821256036,
      "grad_norm": 2.670114040374756,
      "learning_rate": 8.232935892521609e-07,
      "loss": 1.3933,
      "num_input_tokens_seen": 18288168,
      "step": 3565,
      "train_runtime": 12597.6655,
      "train_tokens_per_second": 1451.711
    },
    {
      "epoch": 2.759033816425121,
      "grad_norm": 2.1197826862335205,
      "learning_rate": 7.977448903719875e-07,
      "loss": 1.2571,
      "num_input_tokens_seen": 18312104,
      "step": 3570,
      "train_runtime": 12614.308,
      "train_tokens_per_second": 1451.693
    },
    {
      "epoch": 2.7628985507246377,
      "grad_norm": 1.912513256072998,
      "learning_rate": 7.725924548665192e-07,
      "loss": 1.439,
      "num_input_tokens_seen": 18342200,
      "step": 3575,
      "train_runtime": 12633.9588,
      "train_tokens_per_second": 1451.817
    },
    {
      "epoch": 2.7667632850241546,
      "grad_norm": 3.0686371326446533,
      "learning_rate": 7.478366945564307e-07,
      "loss": 1.4528,
      "num_input_tokens_seen": 18368168,
      "step": 3580,
      "train_runtime": 12651.9212,
      "train_tokens_per_second": 1451.809
    },
    {
      "epoch": 2.7706280193236714,
      "grad_norm": 2.809797525405884,
      "learning_rate": 7.23478014767634e-07,
      "loss": 1.6051,
      "num_input_tokens_seen": 18389000,
      "step": 3585,
      "train_runtime": 12667.3535,
      "train_tokens_per_second": 1451.684
    },
    {
      "epoch": 2.7744927536231883,
      "grad_norm": 2.85255765914917,
      "learning_rate": 6.995168143246438e-07,
      "loss": 1.4844,
      "num_input_tokens_seen": 18410424,
      "step": 3590,
      "train_runtime": 12682.8605,
      "train_tokens_per_second": 1451.599
    },
    {
      "epoch": 2.7783574879227055,
      "grad_norm": 2.30138897895813,
      "learning_rate": 6.75953485544048e-07,
      "loss": 1.563,
      "num_input_tokens_seen": 18435240,
      "step": 3595,
      "train_runtime": 12700.1607,
      "train_tokens_per_second": 1451.575
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 2.5577585697174072,
      "learning_rate": 6.527884142280838e-07,
      "loss": 1.2962,
      "num_input_tokens_seen": 18465416,
      "step": 3600,
      "train_runtime": 12719.769,
      "train_tokens_per_second": 1451.71
    },
    {
      "epoch": 2.786086956521739,
      "grad_norm": 2.166849136352539,
      "learning_rate": 6.30021979658324e-07,
      "loss": 1.2689,
      "num_input_tokens_seen": 18495752,
      "step": 3605,
      "train_runtime": 12739.7655,
      "train_tokens_per_second": 1451.813
    },
    {
      "epoch": 2.789951690821256,
      "grad_norm": 2.148970603942871,
      "learning_rate": 6.076545545894597e-07,
      "loss": 1.4287,
      "num_input_tokens_seen": 18520520,
      "step": 3610,
      "train_runtime": 12756.7264,
      "train_tokens_per_second": 1451.824
    },
    {
      "epoch": 2.793816425120773,
      "grad_norm": 2.099205493927002,
      "learning_rate": 5.85686505243202e-07,
      "loss": 1.1175,
      "num_input_tokens_seen": 18546456,
      "step": 3615,
      "train_runtime": 12774.1778,
      "train_tokens_per_second": 1451.871
    },
    {
      "epoch": 2.7976811594202897,
      "grad_norm": 2.100558042526245,
      "learning_rate": 5.641181913022958e-07,
      "loss": 1.2425,
      "num_input_tokens_seen": 18575784,
      "step": 3620,
      "train_runtime": 12793.4415,
      "train_tokens_per_second": 1451.977
    },
    {
      "epoch": 2.8015458937198066,
      "grad_norm": 2.430997610092163,
      "learning_rate": 5.4294996590461e-07,
      "loss": 1.5314,
      "num_input_tokens_seen": 18600216,
      "step": 3625,
      "train_runtime": 12810.4414,
      "train_tokens_per_second": 1451.957
    },
    {
      "epoch": 2.805410628019324,
      "grad_norm": 2.852595090866089,
      "learning_rate": 5.221821756373674e-07,
      "loss": 1.6834,
      "num_input_tokens_seen": 18625240,
      "step": 3630,
      "train_runtime": 12827.5436,
      "train_tokens_per_second": 1451.972
    },
    {
      "epoch": 2.8092753623188407,
      "grad_norm": 2.040116548538208,
      "learning_rate": 5.018151605314714e-07,
      "loss": 1.4675,
      "num_input_tokens_seen": 18652280,
      "step": 3635,
      "train_runtime": 12845.988,
      "train_tokens_per_second": 1451.993
    },
    {
      "epoch": 2.8131400966183575,
      "grad_norm": 1.8428449630737305,
      "learning_rate": 4.818492540559383e-07,
      "loss": 1.3854,
      "num_input_tokens_seen": 18680488,
      "step": 3640,
      "train_runtime": 12864.4368,
      "train_tokens_per_second": 1452.103
    },
    {
      "epoch": 2.8170048309178743,
      "grad_norm": 2.358722448348999,
      "learning_rate": 4.6228478311243216e-07,
      "loss": 1.5857,
      "num_input_tokens_seen": 18705752,
      "step": 3645,
      "train_runtime": 12882.0164,
      "train_tokens_per_second": 1452.083
    },
    {
      "epoch": 2.820869565217391,
      "grad_norm": 3.7408406734466553,
      "learning_rate": 4.4312206802991085e-07,
      "loss": 1.3894,
      "num_input_tokens_seen": 18728664,
      "step": 3650,
      "train_runtime": 12898.3741,
      "train_tokens_per_second": 1452.017
    },
    {
      "epoch": 2.8247342995169085,
      "grad_norm": 2.5261011123657227,
      "learning_rate": 4.2436142255939394e-07,
      "loss": 1.067,
      "num_input_tokens_seen": 18754968,
      "step": 3655,
      "train_runtime": 12916.1202,
      "train_tokens_per_second": 1452.059
    },
    {
      "epoch": 2.828599033816425,
      "grad_norm": 2.1759066581726074,
      "learning_rate": 4.0600315386881695e-07,
      "loss": 1.3396,
      "num_input_tokens_seen": 18775496,
      "step": 3660,
      "train_runtime": 12931.3824,
      "train_tokens_per_second": 1451.933
    },
    {
      "epoch": 2.832463768115942,
      "grad_norm": 2.1762192249298096,
      "learning_rate": 3.88047562538002e-07,
      "loss": 1.5705,
      "num_input_tokens_seen": 18798376,
      "step": 3665,
      "train_runtime": 12947.4539,
      "train_tokens_per_second": 1451.898
    },
    {
      "epoch": 2.836328502415459,
      "grad_norm": 2.0028481483459473,
      "learning_rate": 3.704949425537313e-07,
      "loss": 1.6831,
      "num_input_tokens_seen": 18825704,
      "step": 3670,
      "train_runtime": 12966.1242,
      "train_tokens_per_second": 1451.915
    },
    {
      "epoch": 2.840193236714976,
      "grad_norm": 2.506361961364746,
      "learning_rate": 3.53345581304948e-07,
      "loss": 1.2997,
      "num_input_tokens_seen": 18848472,
      "step": 3675,
      "train_runtime": 12982.2734,
      "train_tokens_per_second": 1451.862
    },
    {
      "epoch": 2.8440579710144926,
      "grad_norm": 2.537550210952759,
      "learning_rate": 3.3659975957803514e-07,
      "loss": 1.2354,
      "num_input_tokens_seen": 18875704,
      "step": 3680,
      "train_runtime": 13000.7995,
      "train_tokens_per_second": 1451.888
    },
    {
      "epoch": 2.8479227053140095,
      "grad_norm": 2.3962631225585938,
      "learning_rate": 3.202577515522359e-07,
      "loss": 1.3309,
      "num_input_tokens_seen": 18900136,
      "step": 3685,
      "train_runtime": 13017.8723,
      "train_tokens_per_second": 1451.861
    },
    {
      "epoch": 2.8517874396135268,
      "grad_norm": 2.482518196105957,
      "learning_rate": 3.043198247951379e-07,
      "loss": 1.3476,
      "num_input_tokens_seen": 18934232,
      "step": 3690,
      "train_runtime": 13039.799,
      "train_tokens_per_second": 1452.034
    },
    {
      "epoch": 2.8556521739130436,
      "grad_norm": 2.515747308731079,
      "learning_rate": 2.8878624025832636e-07,
      "loss": 1.567,
      "num_input_tokens_seen": 18958424,
      "step": 3695,
      "train_runtime": 13056.6956,
      "train_tokens_per_second": 1452.008
    },
    {
      "epoch": 2.8595169082125604,
      "grad_norm": 4.13999080657959,
      "learning_rate": 2.736572522730824e-07,
      "loss": 1.1867,
      "num_input_tokens_seen": 18985928,
      "step": 3700,
      "train_runtime": 13075.3578,
      "train_tokens_per_second": 1452.039
    },
    {
      "epoch": 2.8633816425120773,
      "grad_norm": 1.879888892173767,
      "learning_rate": 2.589331085462332e-07,
      "loss": 1.4012,
      "num_input_tokens_seen": 19011976,
      "step": 3705,
      "train_runtime": 13093.4567,
      "train_tokens_per_second": 1452.021
    },
    {
      "epoch": 2.867246376811594,
      "grad_norm": 1.826075792312622,
      "learning_rate": 2.446140501560945e-07,
      "loss": 1.6049,
      "num_input_tokens_seen": 19036536,
      "step": 3710,
      "train_runtime": 13110.3396,
      "train_tokens_per_second": 1452.025
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 1.7962868213653564,
      "learning_rate": 2.3070031154852624e-07,
      "loss": 1.1797,
      "num_input_tokens_seen": 19065176,
      "step": 3715,
      "train_runtime": 13129.3552,
      "train_tokens_per_second": 1452.103
    },
    {
      "epoch": 2.874975845410628,
      "grad_norm": 2.513338804244995,
      "learning_rate": 2.1719212053307737e-07,
      "loss": 1.3042,
      "num_input_tokens_seen": 19095208,
      "step": 3720,
      "train_runtime": 13149.4112,
      "train_tokens_per_second": 1452.172
    },
    {
      "epoch": 2.878840579710145,
      "grad_norm": 2.6411380767822266,
      "learning_rate": 2.0408969827928337e-07,
      "loss": 1.4252,
      "num_input_tokens_seen": 19119096,
      "step": 3725,
      "train_runtime": 13165.9706,
      "train_tokens_per_second": 1452.16
    },
    {
      "epoch": 2.882705314009662,
      "grad_norm": 1.9952774047851562,
      "learning_rate": 1.9139325931301633e-07,
      "loss": 1.1936,
      "num_input_tokens_seen": 19146824,
      "step": 3730,
      "train_runtime": 13184.5792,
      "train_tokens_per_second": 1452.214
    },
    {
      "epoch": 2.8865700483091787,
      "grad_norm": 2.5280423164367676,
      "learning_rate": 1.7910301151300156e-07,
      "loss": 1.3319,
      "num_input_tokens_seen": 19172008,
      "step": 3735,
      "train_runtime": 13202.2826,
      "train_tokens_per_second": 1452.174
    },
    {
      "epoch": 2.8904347826086956,
      "grad_norm": 1.818530559539795,
      "learning_rate": 1.6721915610738715e-07,
      "loss": 1.3326,
      "num_input_tokens_seen": 19197816,
      "step": 3740,
      "train_runtime": 13219.6361,
      "train_tokens_per_second": 1452.22
    },
    {
      "epoch": 2.8942995169082124,
      "grad_norm": 3.1974174976348877,
      "learning_rate": 1.557418876704658e-07,
      "loss": 1.6555,
      "num_input_tokens_seen": 19219512,
      "step": 3745,
      "train_runtime": 13235.6899,
      "train_tokens_per_second": 1452.097
    },
    {
      "epoch": 2.8981642512077297,
      "grad_norm": 2.914487838745117,
      "learning_rate": 1.4467139411948327e-07,
      "loss": 1.409,
      "num_input_tokens_seen": 19242648,
      "step": 3750,
      "train_runtime": 13252.122,
      "train_tokens_per_second": 1452.043
    },
    {
      "epoch": 2.9020289855072465,
      "grad_norm": 2.424593687057495,
      "learning_rate": 1.340078567115599e-07,
      "loss": 1.2189,
      "num_input_tokens_seen": 19272104,
      "step": 3755,
      "train_runtime": 13271.6744,
      "train_tokens_per_second": 1452.123
    },
    {
      "epoch": 2.9058937198067634,
      "grad_norm": 2.5261104106903076,
      "learning_rate": 1.237514500407322e-07,
      "loss": 1.4556,
      "num_input_tokens_seen": 19300728,
      "step": 3760,
      "train_runtime": 13291.1168,
      "train_tokens_per_second": 1452.152
    },
    {
      "epoch": 2.90975845410628,
      "grad_norm": 2.505793809890747,
      "learning_rate": 1.1390234203508266e-07,
      "loss": 1.4568,
      "num_input_tokens_seen": 19327256,
      "step": 3765,
      "train_runtime": 13309.0293,
      "train_tokens_per_second": 1452.191
    },
    {
      "epoch": 2.913623188405797,
      "grad_norm": 3.0230326652526855,
      "learning_rate": 1.0446069395399494e-07,
      "loss": 1.5172,
      "num_input_tokens_seen": 19355192,
      "step": 3770,
      "train_runtime": 13327.6303,
      "train_tokens_per_second": 1452.261
    },
    {
      "epoch": 2.917487922705314,
      "grad_norm": 2.2581028938293457,
      "learning_rate": 9.542666038551696e-08,
      "loss": 1.3625,
      "num_input_tokens_seen": 19379992,
      "step": 3775,
      "train_runtime": 13344.7655,
      "train_tokens_per_second": 1452.254
    },
    {
      "epoch": 2.9213526570048307,
      "grad_norm": 2.2835681438446045,
      "learning_rate": 8.680038924382683e-08,
      "loss": 1.1008,
      "num_input_tokens_seen": 19407416,
      "step": 3780,
      "train_runtime": 13363.2099,
      "train_tokens_per_second": 1452.302
    },
    {
      "epoch": 2.925217391304348,
      "grad_norm": 2.674126148223877,
      "learning_rate": 7.85820217668043e-08,
      "loss": 1.304,
      "num_input_tokens_seen": 19438616,
      "step": 3785,
      "train_runtime": 13383.7308,
      "train_tokens_per_second": 1452.406
    },
    {
      "epoch": 2.929082125603865,
      "grad_norm": 3.532975673675537,
      "learning_rate": 7.07716925137325e-08,
      "loss": 1.5054,
      "num_input_tokens_seen": 19466840,
      "step": 3790,
      "train_runtime": 13402.7818,
      "train_tokens_per_second": 1452.448
    },
    {
      "epoch": 2.9329468599033817,
      "grad_norm": 2.9549336433410645,
      "learning_rate": 6.336952936308316e-08,
      "loss": 1.1181,
      "num_input_tokens_seen": 19501192,
      "step": 3795,
      "train_runtime": 13424.7495,
      "train_tokens_per_second": 1452.63
    },
    {
      "epoch": 2.9368115942028985,
      "grad_norm": 2.462742805480957,
      "learning_rate": 5.637565351042928e-08,
      "loss": 1.1867,
      "num_input_tokens_seen": 19527800,
      "step": 3800,
      "train_runtime": 13442.7569,
      "train_tokens_per_second": 1452.663
    },
    {
      "epoch": 2.9406763285024153,
      "grad_norm": 2.4948434829711914,
      "learning_rate": 4.979017946645792e-08,
      "loss": 1.5721,
      "num_input_tokens_seen": 19553640,
      "step": 3805,
      "train_runtime": 13460.9108,
      "train_tokens_per_second": 1452.624
    },
    {
      "epoch": 2.9445410628019326,
      "grad_norm": 2.256643772125244,
      "learning_rate": 4.3613215055096635e-08,
      "loss": 1.569,
      "num_input_tokens_seen": 19580232,
      "step": 3810,
      "train_runtime": 13478.8976,
      "train_tokens_per_second": 1452.658
    },
    {
      "epoch": 2.948405797101449,
      "grad_norm": 2.022615909576416,
      "learning_rate": 3.784486141173993e-08,
      "loss": 1.4547,
      "num_input_tokens_seen": 19600840,
      "step": 3815,
      "train_runtime": 13493.9501,
      "train_tokens_per_second": 1452.565
    },
    {
      "epoch": 2.9522705314009663,
      "grad_norm": 2.328152656555176,
      "learning_rate": 3.248521298161444e-08,
      "loss": 1.1088,
      "num_input_tokens_seen": 19630824,
      "step": 3820,
      "train_runtime": 13513.8414,
      "train_tokens_per_second": 1452.646
    },
    {
      "epoch": 2.956135265700483,
      "grad_norm": 2.658125162124634,
      "learning_rate": 2.7534357518205213e-08,
      "loss": 1.0245,
      "num_input_tokens_seen": 19664296,
      "step": 3825,
      "train_runtime": 13535.1635,
      "train_tokens_per_second": 1452.83
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.9906312227249146,
      "learning_rate": 2.2992376081840127e-08,
      "loss": 1.3907,
      "num_input_tokens_seen": 19689320,
      "step": 3830,
      "train_runtime": 13552.3267,
      "train_tokens_per_second": 1452.837
    },
    {
      "epoch": 2.963864734299517,
      "grad_norm": 2.3060877323150635,
      "learning_rate": 1.8859343038354903e-08,
      "loss": 1.2328,
      "num_input_tokens_seen": 19714264,
      "step": 3835,
      "train_runtime": 13569.8211,
      "train_tokens_per_second": 1452.802
    },
    {
      "epoch": 2.9677294685990336,
      "grad_norm": 2.5033187866210938,
      "learning_rate": 1.51353260578746e-08,
      "loss": 1.1883,
      "num_input_tokens_seen": 19742824,
      "step": 3840,
      "train_runtime": 13588.6578,
      "train_tokens_per_second": 1452.89
    },
    {
      "epoch": 2.971594202898551,
      "grad_norm": 2.8101601600646973,
      "learning_rate": 1.182038611370062e-08,
      "loss": 1.2534,
      "num_input_tokens_seen": 19769272,
      "step": 3845,
      "train_runtime": 13606.755,
      "train_tokens_per_second": 1452.901
    },
    {
      "epoch": 2.9754589371980678,
      "grad_norm": 2.9489309787750244,
      "learning_rate": 8.91457748133373e-09,
      "loss": 1.4825,
      "num_input_tokens_seen": 19795848,
      "step": 3850,
      "train_runtime": 13624.8273,
      "train_tokens_per_second": 1452.925
    },
    {
      "epoch": 2.9793236714975846,
      "grad_norm": 2.4836785793304443,
      "learning_rate": 6.4179477375525546e-09,
      "loss": 1.2913,
      "num_input_tokens_seen": 19824040,
      "step": 3855,
      "train_runtime": 13643.6416,
      "train_tokens_per_second": 1452.987
    },
    {
      "epoch": 2.9831884057971014,
      "grad_norm": 1.731153964996338,
      "learning_rate": 4.3305377596641884e-09,
      "loss": 1.4655,
      "num_input_tokens_seen": 19855880,
      "step": 3860,
      "train_runtime": 13664.4385,
      "train_tokens_per_second": 1453.106
    },
    {
      "epoch": 2.9870531400966183,
      "grad_norm": 2.166882276535034,
      "learning_rate": 2.6523817248214068e-09,
      "loss": 1.2901,
      "num_input_tokens_seen": 19885880,
      "step": 3865,
      "train_runtime": 13684.294,
      "train_tokens_per_second": 1453.19
    },
    {
      "epoch": 2.990917874396135,
      "grad_norm": 2.7909648418426514,
      "learning_rate": 1.3835071094592256e-09,
      "loss": 1.7065,
      "num_input_tokens_seen": 19910936,
      "step": 3870,
      "train_runtime": 13701.6784,
      "train_tokens_per_second": 1453.175
    },
    {
      "epoch": 2.994782608695652,
      "grad_norm": 2.380708694458008,
      "learning_rate": 5.23934688853589e-10,
      "loss": 1.2878,
      "num_input_tokens_seen": 19933128,
      "step": 3875,
      "train_runtime": 13717.7315,
      "train_tokens_per_second": 1453.092
    },
    {
      "epoch": 2.998647342995169,
      "grad_norm": 2.4190850257873535,
      "learning_rate": 7.367853678275083e-11,
      "loss": 1.4698,
      "num_input_tokens_seen": 19956024,
      "step": 3880,
      "train_runtime": 13734.0452,
      "train_tokens_per_second": 1453.033
    },
    {
      "epoch": 3.0,
      "num_input_tokens_seen": 19969352,
      "step": 3882,
      "total_flos": 3.740538625410662e+16,
      "train_loss": 1.478851812638799,
      "train_runtime": 13742.8687,
      "train_samples_per_second": 4.518,
      "train_steps_per_second": 0.282
    }
  ],
  "logging_steps": 5,
  "max_steps": 3882,
  "num_input_tokens_seen": 19969352,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.740538625410662e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
