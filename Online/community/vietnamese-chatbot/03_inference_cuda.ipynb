{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pwa5c2klebs",
   "metadata": {},
   "source": [
    "# 越南语对话模型推理（CUDA 版本）\n",
    "\n",
    "本示例展示如何在 NVIDIA GPU 上部署并运行基于 Qwen1.5-0.5B-Chat 模型的越南语对话系统。\n",
    "\n",
    "## 项目概述\n",
    "\n",
    "- **基础模型**: Qwen1.5-0.5B-Chat\n",
    "- **微调方法**: LoRA (Low-Rank Adaptation)\n",
    "- **训练数据**: 万卷丝绸数据集越南语部分\n",
    "- **推理框架**: PyTorch + transformers + PEFT\n",
    "- **硬件平台**: NVIDIA GPU (CUDA)\n",
    "\n",
    "## 功能特性\n",
    "\n",
    "- ✅ LoRA 权重加载与模型融合\n",
    "- ✅ 流式文本生成 (Streaming)\n",
    "- ✅ 对话历史管理\n",
    "- ✅ CUDA 加速推理\n",
    "\n",
    "## 快速开始\n",
    "\n",
    "1. 准备训练好的 LoRA 权重\n",
    "2. 加载基础模型和 LoRA 适配器\n",
    "3. 运行交互式对话界面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8583c4f-7947-41df-b9ec-592917cf8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Model/Lora-Tach\"\n",
    "device = \"cuda\"\n",
    "tensors_type=\"pt\"\n",
    "inf_dtype=torch.float16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jm0rvfv5ime",
   "metadata": {},
   "source": [
    "# 环境配置说明\n",
    "\n",
    "## 版本要求\n",
    "\n",
    "运行本推理示例需要以下软件版本：\n",
    "\n",
    "- **PyTorch**: 2.9.1+cu126\n",
    "- **transformers**: 4.57.1\n",
    "- **peft**: 0.17.1\n",
    "- **CUDA**: 12.6\n",
    "\n",
    "## 安装命令\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install transformers peft\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cy6vq0gb0bb",
   "metadata": {},
   "source": [
    "# 模型加载与初始化\n",
    "\n",
    "## 加载基础模型\n",
    "\n",
    "使用 transformers 库提供的 `AutoTokenizer` 和 `AutoModelForCausalLM` 类加载预训练模型：\n",
    "\n",
    "```python\n",
    "# AutoTokenizer: 自动识别并加载与模型匹配的分词器\n",
    "# AutoModelForCausalLM: 自动加载因果语言模型架构\n",
    "# torch.float16: 使用半精度浮点数，减少显存占用\n",
    "```\n",
    "\n",
    "## 路径说明\n",
    "\n",
    "- `./Model/Lora-Tach`: 合并后的 LoRA 模型路径（可直接加载）\n",
    "- `./Model/Qwen1.5-0.5B-Chat`: 基础模型路径\n",
    "- `./Model/lora-train_2026-01-07-16-29-48`: LoRA 适配器路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afb3c3e-ada5-4001-aaa6-bf6d3bcd9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "omodel = AutoModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    dtype=inf_dtype,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1349b7a7-37ca-4852-be2c-6faf96ea3be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yes\\Desktop\\gpt2-samall\\pyvenv\\Lib\\site-packages\\peft\\mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'C:\\Users\\yes\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen1___5-0___5B-Chat' to './Model/Qwen1.5-0.5B-Chat'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "adapter_path = \"./Model/lora-train_2026-01-07-16-29-48\"  # LoRA 相关文件所在路径\n",
    "lora_config = LoraConfig.from_pretrained(adapter_path)\n",
    "model = get_peft_model(omodel, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58x9o6vn8t",
   "metadata": {},
   "source": [
    "# LoRA 适配器加载\n",
    "\n",
    "## PEFT 参数高效微调\n",
    "\n",
    "使用 HuggingFace PEFT 库加载训练好的 LoRA 适配器：\n",
    "\n",
    "```python\n",
    "# LoraConfig.from_pretrained: 从指定路径加载 LoRA 配置\n",
    "# get_peft_model: 将 LoRA 适配器应用到基础模型\n",
    "# .eval(): 将模型设置为评估模式，关闭 Dropout 等训练层\n",
    "```\n",
    "\n",
    "## 注意事项\n",
    "\n",
    "- 确保 base_model_name_or_path 路径正确\n",
    "- 如果路径不匹配，可能会看到警告信息，但不影响推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61be06fe-a4c1-449f-a2df-7cc16b4e0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=omodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f09824b-5e43-437b-865f-020b14ba6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful and friendly chatbot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m8usud3c1n",
   "metadata": {},
   "source": [
    "# 对话系统构建\n",
    "\n",
    "## 系统提示与历史管理\n",
    "\n",
    "### 系统提示词 (System Prompt)\n",
    "```python\n",
    "system_prompt = \"You are a helpful and friendly chatbot\"\n",
    "```\n",
    "- 定义模型的基本身份和行为准则\n",
    "- 在每次对话开始时作为系统消息注入\n",
    "- 引导模型生成更符合预期的回复\n",
    "\n",
    "### 历史记录格式化函数\n",
    "`build_input_from_chat_history()` 函数将对话历史转换为模型所需的格式：\n",
    "\n",
    "```python\n",
    "# 输入格式:\n",
    "#   chat_history: [(用户消息1, 助手回复1), (用户消息2, 助手回复2), ...]\n",
    "#   msg: 当前用户输入\n",
    "\n",
    "# 输出格式:\n",
    "#   messages: [\n",
    "#     {'role': 'system', 'content': system_prompt},\n",
    "#     {'role': 'user', 'content': 用户消息1},\n",
    "#     {'role': 'assistant', 'content': 助手回复1},\n",
    "#     ...\n",
    "#     {'role': 'user', 'content': 当前输入}\n",
    "#   ]\n",
    "```\n",
    "\n",
    "### Qwen1.5 对话格式说明\n",
    "Qwen1.5 系列模型使用以下特殊标记：\n",
    "- `<|im_start|>`: 消息开始标记\n",
    "- `<|im_end|>`: 消息结束标记\n",
    "- `<|endoftext|>`: 文本结束标记\n",
    "\n",
    "`tokenizer.apply_chat_template()` 会自动将消息列表转换为这种格式的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669839ff-a0db-4af5-ad35-f4be42b4adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_from_chat_history(chat_history, msg: str):\n",
    "    messages = [{'role': 'system', 'content': system_prompt}]\n",
    "    for user_msg, ai_msg in chat_history:\n",
    "        messages.append({'role': 'user', 'content': user_msg})\n",
    "        messages.append({'role': 'assistant', 'content': ai_msg})\n",
    "    messages.append({'role': 'user', 'content': msg})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d76ab3-0594-4d1e-99b6-4612481bf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate model predictions.\n",
    "def predict(message, history):\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "\n",
    "    # Formatting the input for the model.\n",
    "    messages = build_input_from_chat_history(history, message)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=tensors_type,\n",
    "            tokenize=True\n",
    "        ).to(device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=300, skip_prompt=True, skip_special_tokens=False)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        top_p=0.7,\n",
    "        temperature=0.95\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()  # Starting the generation in a separate thread.\n",
    "    for new_token in streamer:\n",
    "        yield new_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4nn7zwtua",
   "metadata": {},
   "source": [
    "# 流式推理函数\n",
    "\n",
    "## 功能说明\n",
    "实现模型的流式文本生成，将用户输入和对话历史转换为模型可处理的格式，并支持实时 token 输出。\n",
    "\n",
    "## 处理流程\n",
    "1. **输入格式化**：调用 `build_input_from_chat_history` 将对话历史转换为模型所需的聊天模板格式\n",
    "2. **Tokenization**：使用分词器的 `apply_chat_template` 方法将消息列表转换为模型输入张量\n",
    "3. **流式配置**：创建 `TextIteratorStreamer` 对象，设置超时和过滤参数\n",
    "4. **生成参数**：配置采样策略（top-p=0.7，temperature=0.95）\n",
    "5. **异步生成**：在新线程中启动模型生成过程，实现实时 token 流式输出\n",
    "\n",
    "## 关键参数\n",
    "- **max_new_tokens**: 控制生成文本的最大长度（1024）\n",
    "- **top_p**: 使用核采样（0.7）平衡生成质量与多样性\n",
    "- **temperature**: 较高温度值（0.95）增加生成多样性\n",
    "\n",
    "## 技术特点\n",
    "- 支持实时交互体验，逐个 token 输出响应\n",
    "- 采用异步生成避免界面阻塞\n",
    "- 适合对话场景的快速响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9111b98-23e4-46f8-9f8a-54dfe0c182a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You>  Nguồn gốc tục thờ cúng tổ tiên của người Việt?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngoài ra từ 1950 đến 1960, có một số công dân ở Đã Nha và Quảng Bình đã bị làn để từ bỏ vỡ các tôn giáo xã hội. Với sự quan tâm đến sự sống động và hòa bình, những người có khả năng thích nghi với việc tham gia vào việc tôn giáo mới hơn. Họ thường tìm kiếm những nền tảng khác nhau như tôn giáo tự nhiên, tôn giáo tôn giáo truyền thống và tôn giáo truyền thống. Từ đó, ngày càng phát triển của một số nền tảng, theo đó trở thành các nguồn tin của những người dân được tôn giáo như: tôn giáo của người dân, tôn giáo của nhà nước, tôn giáo xã hội.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "while True:\n",
    "    message = input(\"You> \").strip()\n",
    "    if message in (\"exit\", \"quit\"):\n",
    "        break\n",
    "\n",
    "    ans = \"\"\n",
    "    print(\"Bot> \", end=\"\", flush=True)\n",
    "    for tok in predict(message, history):\n",
    "        print(tok, end=\"\", flush=True)\n",
    "        ans += tok\n",
    "    print()\n",
    "\n",
    "    history.append([message, ans.strip()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbscolt1s04",
   "metadata": {},
   "source": [
    "# 交互对话\n",
    "\n",
    "启动命令行对话界面，支持多轮交互和流式响应。输入 `exit` 或 `quit` 退出程序。\n",
    "\n",
    "## 使用示例\n",
    "\n",
    "```\n",
    "You> Xin chào!\n",
    "Bot> Xin chào! Tôi có thể giúp gì cho bạn?\n",
    "\n",
    "You> Nguồn gốc tục thờ cúng tổ tiên của người Việt?\n",
    "Bot> Tục thờ cúng tổ tiên có nguồn gốc từ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cfe94-b7a0-48cc-9575-ec0ba94c4ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
