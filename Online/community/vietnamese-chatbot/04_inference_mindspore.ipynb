{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb541d08-b96f-4701-8e59-9c4509aacc3e",
   "metadata": {},
   "source": [
    "# 越南语对话模型推理示例\n",
    "\n",
    "本示例展示如何在香橙派AIPRO上部署并运行基于Qwen1.5-0.5B-Chat模型的越南语对话系统。\n",
    "\n",
    "## 项目概述\n",
    "\n",
    "- **基础模型**: Qwen1.5-0.5B-Chat\n",
    "- **微调方法**: LoRA (Low-Rank Adaptation)\n",
    "- **训练数据**: 万卷丝绸数据集越南语部分\n",
    "- **部署平台**: 香橙派AIPRO\n",
    "- **推理框架**: MindSpore + MindNLP\n",
    "\n",
    "## 功能特性\n",
    "\n",
    "- ✅ LoRA权重加载与模型融合\n",
    "- ✅ 流式文本生成 (Streaming)\n",
    "- ✅ 对话历史管理\n",
    "- ✅ LoRA层激活验证\n",
    "- ✅ 边缘设备推理优化\n",
    "\n",
    "## 快速开始\n",
    "\n",
    "1. 准备训练好的LoRA权重\n",
    "2. 加载基础模型和LoRA适配器\n",
    "3. 运行交互式对话界面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d87bb0-0f54-4467-9c76-a089b7a6e771",
   "metadata": {},
   "source": [
    "# 环境配置说明\n",
    "\n",
    "## 版本要求\n",
    "\n",
    "运行本推理示例需要以下软件版本：\n",
    "\n",
    "- **MindSpore**: 2.6.0\n",
    "- **MindNLP**: 0.4.1\n",
    "- **CANN**: 8.1.RC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4caac11f-7ccf-4caa-88dc-f0f244b93856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindnlp                   0.4.1\n",
      "mindpet                   1.0.4\n",
      "mindspore                 2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip list |grep mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf582f",
   "metadata": {},
   "source": [
    "# NPU 配置\n",
    "\n",
    "使用 `npu-smi info` 命令查看昇腾 NPU 的配置信息，包括 NPU 型号、健康状态、功耗、温度、内存使用情况等。\n",
    "\n",
    "## 硬件信息\n",
    "\n",
    "- **NPU 型号**: Ascend 310B4\n",
    "- **内存**: 15.6GB\n",
    "- **Hugepages**: 15/15 页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48pgx050lj9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
"+--------------------------------------------------------------------------------------------------------+\n",
"| npu-smi 25.2.0                                   Version: 25.2.0                                       |\n",
"+-------------------------------+-----------------+------------------------------------------------------+\n",
"| NPU     Name                  | Health          | Power(W)     Temp(C)           Hugepages-Usage(page) |\n",
"| Chip    Device                | Bus-Id          | AICore(%)    Memory-Usage(MB)                        |\n",
"+===============================+=================+======================================================+\n",
"| 0       310B4                 | OK/Alarm        | 0.0          51                15    / 15            |\n",
"| 0       0                     | NA              | 0            3681 / 15610                            |\n",
"+===============================+=================+======================================================+\n"

     ]
    }],
   "source": [
    "!npu-smi info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2a248-e43a-4e17-a8c9-882cd5d869c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cannops/CANN_Env/8_1_RC1_py10/pyvenv/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/cannops/CANN_Env/8_1_RC1_py10/pyvenv/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/cannops/CANN_Env/8_1_RC1_py10/pyvenv/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/cannops/CANN_Env/8_1_RC1_py10/pyvenv/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "[WARNING] ME(14370:246290596995104,MainProcess):2026-01-08-16:09:53.210.000 [mindspore/context.py:1402] For 'context.set_context', the parameter 'ascend_config' will be deprecated and removed in a future version. Please use the api mindspore.device_context.ascend.op_precision.precision_mode(),\n",
      "                                                       mindspore.device_context.ascend.op_precision.op_precision_mode(),\n",
      "                                                       mindspore.device_context.ascend.op_precision.matmul_allow_hf32(),\n",
      "                                                       mindspore.device_context.ascend.op_precision.conv_allow_hf32(),\n",
      "                                                       mindspore.device_context.ascend.op_tuning.op_compile() instead.\n",
      "/home/cannops/CANN_Env/8_1_RC1_py10/pyvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 2.246 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from mindnlp.transformers import TextIteratorStreamer\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18ec92-fb19-466e-a22d-84aa012826f6",
   "metadata": {},
   "source": [
    "# 模型加载与初始化\n",
    "\n",
    "## 加载基础模型\n",
    "\n",
    "使用MindNLP提供的`AutoTokenizer`和`AutoModelForCausalLM`类加载预训练模型：\n",
    "\n",
    "```python\n",
    "# AutoTokenizer: 自动识别并加载与模型匹配的分词器\n",
    "# AutoModelForCausalLM: 自动加载因果语言模型架构\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6bf3bdd-9cd2-4efe-8923-afb6f14bb52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "path = \"./Model/Qwen1.5-0.5B-Chat/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path,ms_dtype=mindspore.float16)\n",
    "omodel = AutoModelForCausalLM.from_pretrained(path,ms_dtype=mindspore.float16)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f30eceb-ad62-4976-9733-2f8cb52ac4d0",
   "metadata": {},
   "source": [
    "# LoRA适配器加载\n",
    "\n",
    "## PEFT参数高效微调\n",
    "\n",
    "使用MindNLP的PEFT（Parameter-Efficient Fine-Tuning）模块加载训练好的LoRA适配器：\n",
    "\n",
    "```python\n",
    "# PeftModel: 专门用于加载和管理PEFT微调权重\n",
    "# from_pretrained: 从指定路径加载LoRA适配器配置和权重\n",
    "# .eval(): 将模型设置为评估模式，关闭Dropout等训练层\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cdf57a-3152-42cb-a2d6-44403d7b3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.peft import get_peft_model, PeftModel, LoHaConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4996f6-edf5-4950-a2cf-663216aa3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path = \"./Model/lora-train_2026-01-07-16-29-48\"\n",
    "loramodel = PeftModel.from_pretrained(omodel, lora_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4448d8a-41f4-422b-abee-8903513b6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=loramodel.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951896e-18d7-40e5-9753-4b1482a7186d",
   "metadata": {},
   "source": [
    "# LoRA层激活验证\n",
    "\n",
    "## 技术目的\n",
    "验证PEFT微调权重在推理过程中的有效集成与激活状态，确保低秩适配器在边缘部署中正确运作，为模型性能调优提供量化依据。\n",
    "\n",
    "## 技术原理\n",
    "基于`register_forward_hook`机制，在模型计算图中动态监控LoRA模块的执行状态。\n",
    "通过`PeftModel`的模块遍历与条件筛选，对包含`lora_A`和`lora_B`属性的低秩适配层注册回调函数。\n",
    "前向传播时，`AutoModelForCausalLM`的推理路径触发钩子函数，实时记录`LoRALinear`或`LoRAEmbedding`等适配层的调用信息，通过`accessed_params`集合统计有效激活的微调参数规模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd164cda-6fbf-48aa-b60e-18aac6ad716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessed_params = []\n",
    "\n",
    "def hook(module, input, output):\n",
    "    if hasattr(module, 'lora_A') or hasattr(module, 'lora_B'):\n",
    "        accessed_params.append(module.__class__.__name__)\n",
    "\n",
    "# 注册hook到LoRA层\n",
    "hooks = []\n",
    "for module in model.modules():\n",
    "    if hasattr(module, 'lora_A'):\n",
    "        hook_handle = module.register_forward_hook(hook)\n",
    "        hooks.append(hook_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c629c6-15d7-43bd-802e-bcfb97c68f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated. Please use an appropriate `Cache` class\n",
      "2026-01-08 16:11:13.233289: E external/org_tensorflow/tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute is_closed which is not in the op definition: Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node Range2}}\n"
     ]
    }
   ],
   "source": [
    "test_input = tokenizer(\"测试\", return_tensors=\"ms\")\n",
    "\n",
    "_ = model(**test_input)\n",
    "\n",
    "# 移除hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b457f9-d9d9-4c9c-b45b-018e1cfb2e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在前向传播中访问了 168 个LoRA层\n",
      "示例访问的层: {'Linear'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"在前向传播中访问了 {len(accessed_params)} 个LoRA层\")\n",
    "print(f\"示例访问的层: {set(accessed_params[:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1008fb-2f8c-4301-bb4c-9ff5385e3c41",
   "metadata": {},
   "source": [
    "# 对话系统构建\n",
    "\n",
    "## 系统提示与历史管理\n",
    "\n",
    "### 系统提示词 (System Prompt)\n",
    "```python\n",
    "system_prompt = \"You are a helpful and friendly chatbot\"\n",
    "```\n",
    "- 定义模型的基本身份和行为准则\n",
    "- 在每次对话开始时作为系统消息注入\n",
    "- 引导模型生成更符合预期的回复\n",
    "\n",
    "### 历史记录格式化函数\n",
    "`build_input_from_chat_history()` 函数将对话历史转换为模型所需的格式：\n",
    "\n",
    "```python\n",
    "# 输入格式:\n",
    "#   chat_history: [(用户消息1, 助手回复1), (用户消息2, 助手回复2), ...]\n",
    "#   msg: 当前用户输入\n",
    "\n",
    "# 输出格式:\n",
    "#   messages: [\n",
    "#     {'role': 'system', 'content': system_prompt},\n",
    "#     {'role': 'user', 'content': 用户消息1},\n",
    "#     {'role': 'assistant', 'content': 助手回复1},\n",
    "#     ...\n",
    "#     {'role': 'user', 'content': 当前输入}\n",
    "#   ]\n",
    "```\n",
    "\n",
    "### Qwen1.5对话格式说明\n",
    "Qwen1.5系列模型使用以下特殊标记：\n",
    "- `<|im_start|>`: 消息开始标记\n",
    "- `<|im_end|>`: 消息结束标记\n",
    "- `<|endoftext|>`: 文本结束标记\n",
    "\n",
    "`tokenizer.apply_chat_template()` 会自动将消息列表转换为这种格式的文本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657fd243-7ead-4eb9-a5f8-fa7662bf2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"You are a helpful and friendly chatbot\"\n",
    "def build_input_from_chat_history(chat_history, msg: str):\n",
    "    messages = [{'role': 'system', 'content': system_prompt}]\n",
    "    for user_msg, ai_msg in chat_history:\n",
    "        messages.append({'role': 'user', 'content': user_msg})\n",
    "        messages.append({'role': 'assistant', 'content': ai_msg})\n",
    "    messages.append({'role': 'user', 'content': msg})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b9cee-043c-448c-a4a9-f9a7a5ef607c",
   "metadata": {},
   "source": [
    "# 流式推理函数\n",
    "\n",
    "## 功能说明\n",
    "实现模型的流式文本生成，将用户输入和对话历史转换为模型可处理的格式，并支持实时token输出。\n",
    "\n",
    "## 处理流程\n",
    "1. **输入格式化**：调用`build_input_from_chat_history`将对话历史转换为模型所需的聊天模板格式\n",
    "2. **Tokenization**：使用分词器的`apply_chat_template`方法将消息列表转换为模型输入张量\n",
    "3. **流式配置**：创建`TextIteratorStreamer`对象，设置超时和过滤参数\n",
    "4. **生成参数**：配置采样策略（top-p=0.9，temperature=0.1）和重复惩罚（1.2）\n",
    "5. **异步生成**：在新线程中启动模型生成过程，实现实时token流式输出\n",
    "\n",
    "## 关键参数\n",
    "- **max_new_tokens**: 控制生成文本的最大长度（1024）\n",
    "- **do_sample**: 启用采样策略以增加生成多样性\n",
    "- **top_p**: 使用核采样（0.9）平衡生成质量与多样性\n",
    "- **temperature**: 较低温度值（0.1）确保生成内容更确定\n",
    "- **repetition_penalty**: 惩罚重复内容（1.2）避免循环回复\n",
    "\n",
    "## 技术特点\n",
    "- 支持实时交互体验，逐个token输出响应\n",
    "- 采用异步生成避免界面阻塞\n",
    "- 平衡生成质量与响应速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb39031-e753-42cf-b909-9fac6b4ea493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate model predictions.\n",
    "def predict(message, history):\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    # Formatting the input for the model.\n",
    "    messages = build_input_from_chat_history(history, message)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"ms\",\n",
    "            tokenize=True\n",
    "        )\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=300, skip_prompt=True, skip_special_tokens=False)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.1,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()  # Starting the generation in a separate thread.\n",
    "    for new_token in streamer:\n",
    "        yield new_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29d19d-3454-416c-bbb6-78c688d6e74c",
   "metadata": {},
   "source": [
    "# 交互对话\n",
    "\n",
    "启动命令行对话界面，支持多轮交互和流式响应。输入\"exit\"或\"quit\"退出程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea3dfe-b811-4f7c-8686-42a360091be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You>  Nguồn gốc tục thờ cúng tổ tiên của người Việt?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Trong lịch sử, Cúng là một trong những công việc quan trọng nhất cho dân tộc Việt Nam. Nó được xây dựng từ năm 1902 đến năm 1934 và đã trở thành sự kiện lớn trong cuộc chiến chống đế quốc Mỹ. Trung tâm Cúng là nơi các hoạt động văn hóa truyền thống như phán hùng vĩ, trang phục nguy hiểm, và các hội thảo xã hội.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = []\n",
    "\n",
    "while True:\n",
    "    message = input(\"You> \").strip()\n",
    "    if message in (\"exit\", \"quit\"):\n",
    "        break\n",
    "    ans = \"\"\n",
    "    print(\"Bot> \", end=\"\", flush=True)\n",
    "    for tok in predict(message, history):\n",
    "        print(tok, end=\"\", flush=True)\n",
    "        ans += tok\n",
    "    print()\n",
    "    history.append([message, ans.strip()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ab547-3109-4290-9c98-32eb9e7d2cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CANN8_1_RC1",
   "language": "python",
   "name": "cann8_1_rc1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
